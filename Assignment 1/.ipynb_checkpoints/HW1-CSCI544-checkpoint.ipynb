{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_4104\\4217004950.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv('./amazon_reviews_us_Beauty_v1_00.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n",
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_4104\\4217004950.py:1: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv('./amazon_reviews_us_Beauty_v1_00.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n",
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_4104\\4217004950.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('./amazon_reviews_us_Beauty_v1_00.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./amazon_reviews_us_Beauty_v1_00.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['star_rating', 'review_body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let ratings with the values of 1 and 2 form class 1, ratings with the value of 3 form class 2, and ratings with the values of 4 and 5 form class 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_4104\\3935828091.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_one.loc[:, \"label\"] =1\n",
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_4104\\3935828091.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_two.loc[:, \"label\"] =2\n",
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_4104\\3935828091.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_three.loc[:, \"label\"] =3\n"
     ]
    }
   ],
   "source": [
    "class_one = df[(df['star_rating']==1) | (df['star_rating']==2)]\n",
    "class_two = df[df['star_rating']==3]\n",
    "class_three = df[(df['star_rating']==4) | (df['star_rating']==5)]\n",
    "\n",
    "class_one.loc[:, \"label\"] =1\n",
    "class_two.loc[:, \"label\"] =2\n",
    "class_three.loc[:, \"label\"] =3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 12000\n"
     ]
    }
   ],
   "source": [
    "class_one = class_one.sample(n=20000, random_state=100)\n",
    "class_two = class_two.sample(n=20000, random_state=100)\n",
    "class_three = class_three.sample(n=20000, random_state=100)\n",
    "dataset = pd.concat([class_one, class_two, class_three])\n",
    "dataset.reset_index(drop=True)\n",
    "train = dataset.sample(frac=0.8, random_state=100)\n",
    "test = dataset.drop(train.index)\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: For DATA CLEANING:\n",
    "- First we calculate average length of a review before the cleaning\n",
    "- We then convert each review string to lowercase by using str.lower()\n",
    "- From the review body, to remove HTML elements we used BeautifulSoup and for the removal of the URLs in the review we used regular expression which eliminates the URLs from a review, for the process to remove the HTML elements and URLs, we clean each review by using lambda function and the processed review is then reassigned to the particular position in the dataframe by using the '.apply' function\n",
    "- The BeautifulSoup(review, 'html.parser') returns an object which has parsed html and remaining text and we then extract the remaining text\n",
    "- Non alphabetical characters and extra spaces are substituted by single space ' ' by the help of re.sub() , '[^a-zA-Z]+' means the characters not involving alphabets.\n",
    "- Contractions are removed by the use of Contractions library which takes each review from the dataframe and expands contraction with the help of contractions.fix() function\n",
    "- At the end, calculate average length of a review after the cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert all reviews into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len_before = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "\n",
    "#Covert all reviews to lower case\n",
    "train['review_body'] = train['review_body'].str.lower()\n",
    "test['review_body'] = test['review_body'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# URL Remover code\n",
    "train['review_body'] = train['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "test['review_body'] = test['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "def html_tag_remover(review):\n",
    "    soup = BeautifulSoup(review, 'html.parser')\n",
    "    review = soup.get_text()\n",
    "    return review  \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: html_tag_remover(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: html_tag_remover(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub(' +', ' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub(' +', ' ', review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform contractions on the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of the reviews in terms of character length before cleaning:  287.3779333333333\n",
      "Average length of the reviews in terms of character length after cleaning:  165.6902\n"
     ]
    }
   ],
   "source": [
    "def expand_contractions(review):\n",
    "    review = contractions.fix(review)\n",
    "    return review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: expand_contractions(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: expand_contractions(review))\n",
    "avg_len_after = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "\n",
    "print('Average length of the reviews in terms of character length before cleaning: ', avg_len_before)\n",
    "print('Average length of the reviews in terms of character length after cleaning: ', avg_len_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: For DATA PREPROCESSING:\n",
    "- First we calculate average length of a review before the preprocessing\n",
    "- I used NLTk library for stopward removal and to perform lemmatization\n",
    "- In the stop removal snippet, for each review of the dataframe, we first get all the stopwords in englush and then we generate tokens of a review with the word_tokenize() function and then we create the list of words which doesnot contain the stopwords and then apply the filtered review back to the dataframe in the form of list.\n",
    "- In the lemmatization step, for each review of the dataframe, we first lemmatize each and every word of the review and  then create the string of the final lemmatized words of the review and apply back to the dataframe\n",
    "- At the end, calculate average length of a review after the preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len_before_prepro = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    stop_words_english = set(stopwords.words('english'))\n",
    "    review_word_tokens = word_tokenize(review)\n",
    "    filtered_review = [word for word in review_word_tokens if not word in stop_words_english]\n",
    "    return filtered_review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: remove_stopwords(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: remove_stopwords(review))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def review_lemmatize(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_review = [lemmatizer.lemmatize(word) for word in review]\n",
    "    return ' '.join(lemmatized_review)    \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "\n",
    "avg_len_after_prepro = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "\n",
    "print('Average length of the reviews in terms of character length before preprocessing: ', avg_len_before_prepro)\n",
    "print('Average length of the reviews in terms of character length after preprocessing: ', avg_len_after_prepro)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: For TF-IDF Feature Extraction\n",
    "- we used TfidfVectorizer library to convert each review into feature vectors\n",
    "- we used min_df to When building the vocabulary ignore terms that have a document frequency strictly lower than the given 0.01\n",
    "- Here by using fit_transform() it learns vocabulary and idf and return document-term matrix which we then convert to get a new dataframe of vectors. We do the same procedure to get the vectors for training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(train['review_body'])\n",
    "test_corpus = list(test['review_body'])\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 0.001)\n",
    "X_train = tfidf_vectorizer.fit_transform(train_corpus)\n",
    "X_train = pd.DataFrame(X_train.toarray())\n",
    "X_test = tfidf_vectorizer.transform(test_corpus)\n",
    "X_test = pd.DataFrame(X_test.toarray())\n",
    "Y_train = train['label']\n",
    "Y_test = test['label']\n",
    "Y_train = Y_train.astype('int')\n",
    "Y_test = Y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perceptron model was used and imported from sklearn library, we set random_state to 100 and eta0 to 0.1. We trained the training data using .fit() method and predicted the output for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 0.696101, 0.594079, 0.641056\n",
      "Class 2: 0.565405, 0.461210, 0.508020\n",
      "Class 3: 0.596618, 0.798982, 0.683128\n",
      "Average: 0.619375, 0.618090, 0.610735\n"
     ]
    }
   ],
   "source": [
    "perceptr = Perceptron(random_state = 100, eta0=0.1)\n",
    "perceptr.fit(X_train, Y_train)\n",
    "Y_pred_test = perceptr.predict(X_test)\n",
    "\n",
    "percep_scores = precision_recall_fscore_support(Y_test, Y_pred_test, average=None)\n",
    "average_percep_scores = precision_recall_fscore_support(Y_test, Y_pred_test, average='macro')\n",
    "\n",
    "print('Class 1: %f, %f, %f' % (percep_scores[0][0], percep_scores[1][0], percep_scores[2][0]))\n",
    "print('Class 2: %f, %f, %f' % (percep_scores[0][1],  percep_scores[1][1],  percep_scores[2][1]))\n",
    "print('Class 3: %f, %f, %f' % (percep_scores[0][2],  percep_scores[1][2],  percep_scores[2][2]))\n",
    "print('Average: %f, %f, %f' % (average_percep_scores[0], average_percep_scores[1], average_percep_scores[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM model was used and imported from sklearn library, we set random_state to 100, max_iter to 1000. We trained the training data using .fit() method and predicted the output for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 0.689001, 0.701982, 0.695431\n",
      "Class 2: 0.602248, 0.551594, 0.575809\n",
      "Class 3: 0.705826, 0.752163, 0.728258\n",
      "Average: 0.665692, 0.668580, 0.666499\n"
     ]
    }
   ],
   "source": [
    "clf_svm = LinearSVC(random_state=100, max_iter=1000)\n",
    "clf_svm.fit(X_train, Y_train)\n",
    "Y_pred_test_svm = clf_svm.predict(X_test)\n",
    "\n",
    "svm_scores = precision_recall_fscore_support(Y_test, Y_pred_test_svm, average=None)\n",
    "average_svm_scores = precision_recall_fscore_support(Y_test, Y_pred_test_svm, average='macro')\n",
    "\n",
    "print('Class 1: %f, %f, %f' % (svm_scores[0][0], svm_scores[1][0], svm_scores[2][0]))\n",
    "print('Class 2: %f, %f, %f' % (svm_scores[0][1],  svm_scores[1][1],  svm_scores[2][1]))\n",
    "print('Class 3: %f, %f, %f' % (svm_scores[0][2],  svm_scores[1][2],  svm_scores[2][2]))\n",
    "print('Average: %f, %f, %f' % (average_svm_scores[0], average_svm_scores[1], average_svm_scores[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression model was used and imported from sklearn library, we set random_state to 100, max_iter to 1000. We trained the training data using .fit() method and predicted the output for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 0.694044, 0.707120, 0.700521\n",
      "Class 2: 0.600364, 0.580467, 0.590248\n",
      "Class 3: 0.730489, 0.740712, 0.735565\n",
      "Average: 0.674966, 0.676100, 0.675445\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(random_state = 100, max_iter=1000)\n",
    "logistic.fit(X_train, Y_train)\n",
    "Y_pred_test_logis = logistic.predict(X_test)\n",
    "\n",
    "logistic_scores = precision_recall_fscore_support(Y_test, Y_pred_test_logis, average=None)\n",
    "average_logistic_scores = precision_recall_fscore_support(Y_test, Y_pred_test_logis, average='macro')\n",
    "\n",
    "print('Class 1: %f, %f, %f' % (logistic_scores[0][0], logistic_scores[1][0], logistic_scores[2][0]))\n",
    "print('Class 2: %f, %f, %f' % (logistic_scores[0][1],  logistic_scores[1][1],  logistic_scores[2][1]))\n",
    "print('Class 3: %f, %f, %f' % (logistic_scores[0][2],  logistic_scores[1][2],  logistic_scores[2][2]))\n",
    "print('Average: %f, %f, %f' % (average_logistic_scores[0], average_logistic_scores[1], average_logistic_scores[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes model was used and imported from sklearn library, we set random_state to 100, max_iter to 1000. We trained the training data using .fit() method and predicted the output for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 0.694044, 0.707120, 0.700521\n",
      "Class 2: 0.600364, 0.580467, 0.590248\n",
      "Class 3: 0.730489, 0.740712, 0.735565\n",
      "Average: 0.674966, 0.676100, 0.675445\n"
     ]
    }
   ],
   "source": [
    "naive_bay = MultinomialNB(force_alpha=True)\n",
    "naive_bay.fit(X_train, Y_train)\n",
    "Y_pred_test_naive = logistic.predict(X_test)\n",
    "\n",
    "naive_scores = precision_recall_fscore_support(Y_test, Y_pred_test_naive, average=None)\n",
    "average_naive_scores = precision_recall_fscore_support(Y_test, Y_pred_test_naive, average='macro')\n",
    "\n",
    "\n",
    "print('Class 1: %f, %f, %f' % (naive_scores[0][0], naive_scores[1][0], naive_scores[2][0]))\n",
    "print('Class 2: %f, %f, %f' % (naive_scores[0][1],  naive_scores[1][1],  naive_scores[2][1]))\n",
    "print('Class 3: %f, %f, %f' % (naive_scores[0][2],  naive_scores[1][2],  naive_scores[2][2]))\n",
    "print('Average: %f, %f, %f' % (average_naive_scores[0], average_naive_scores[1], average_naive_scores[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
