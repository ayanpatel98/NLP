{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_17392\\4217004950.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv('./amazon_reviews_us_Beauty_v1_00.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n",
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_17392\\4217004950.py:1: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv('./amazon_reviews_us_Beauty_v1_00.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n",
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_17392\\4217004950.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('./amazon_reviews_us_Beauty_v1_00.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./amazon_reviews_us_Beauty_v1_00.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['star_rating', 'review_body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let ratings with the values of 1 and 2 form class 1, ratings with the value of 3 form class 2, and ratings with the values of 4 and 5 form class 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_17392\\3935828091.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_one.loc[:, \"label\"] =1\n",
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_17392\\3935828091.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_two.loc[:, \"label\"] =2\n",
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_17392\\3935828091.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_three.loc[:, \"label\"] =3\n"
     ]
    }
   ],
   "source": [
    "class_one = df[(df['star_rating']==1) | (df['star_rating']==2)]\n",
    "class_two = df[df['star_rating']==3]\n",
    "class_three = df[(df['star_rating']==4) | (df['star_rating']==5)]\n",
    "\n",
    "class_one.loc[:, \"label\"] =1\n",
    "class_two.loc[:, \"label\"] =2\n",
    "class_three.loc[:, \"label\"] =3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 12000\n"
     ]
    }
   ],
   "source": [
    "class_one = class_one.sample(n=20000, random_state=100)\n",
    "class_two = class_two.sample(n=20000, random_state=100)\n",
    "class_three = class_three.sample(n=20000, random_state=100)\n",
    "dataset = pd.concat([class_one, class_two, class_three])\n",
    "dataset.reset_index(drop=True)\n",
    "train = dataset.sample(frac=0.8, random_state=100)\n",
    "test = dataset.drop(train.index)\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: For DATA CLEANING:\n",
    "- First we calculate average length of a review before the cleaning\n",
    "- We then convert each review string to lowercase by using str.lower()\n",
    "- From the review body, to remove HTML elements we used BeautifulSoup and for the removal of the URLs in the review we used regular expression which eliminates the URLs from a review, for the process to remove the HTML elements and URLs, we clean each review by using lambda function and the processed review is then reassigned to the particular position in the dataframe by using the '.apply' function\n",
    "- The BeautifulSoup(review, 'html.parser') returns an object which has parsed html and remaining text and we then extract the remaining text\n",
    "- Non alphabetical characters and extra spaces are substituted by single space ' ' by the help of re.sub() , '[^a-zA-Z]+' means the characters not involving alphabets.\n",
    "- Contractions are removed by the use of Contractions library which takes each review from the dataframe and expands contraction with the help of contractions.fix() function\n",
    "- At the end, calculate average length of a review after the cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert all reviews into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len_before = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "\n",
    "#Covert all reviews to lower case\n",
    "train['review_body'] = train['review_body'].str.lower()\n",
    "test['review_body'] = test['review_body'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# URL Remover code\n",
    "train['review_body'] = train['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "test['review_body'] = test['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "def html_tag_remover(review):\n",
    "    soup = BeautifulSoup(review, 'html.parser')\n",
    "    review = soup.get_text()\n",
    "    return review  \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: html_tag_remover(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: html_tag_remover(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub(' +', ' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub(' +', ' ', review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform contractions on the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of the reviews in terms of character length before cleaning:  287.3779333333333\n",
      "Average length of the reviews in terms of character length after cleaning:  276.02595\n"
     ]
    }
   ],
   "source": [
    "def expand_contractions(review):\n",
    "    review = contractions.fix(review)\n",
    "    return review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: expand_contractions(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: expand_contractions(review))\n",
    "avg_len_after = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "\n",
    "print('Average length of the reviews in terms of character length before cleaning: ', avg_len_before)\n",
    "print('Average length of the reviews in terms of character length after cleaning: ', avg_len_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len_before_prepro = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    stop_words_english = set(stopwords.words('english'))\n",
    "    review_word_tokens = word_tokenize(review)\n",
    "    filtered_review = [word for word in review_word_tokens if not word in stop_words_english]\n",
    "    return filtered_review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: remove_stopwords(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: remove_stopwords(review))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of the reviews in terms of character length before preprocessing:  276.02595\n",
      "Average length of the reviews in terms of character length after preprocessing:  165.65626666666665\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def review_lemmatize(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_review = [lemmatizer.lemmatize(word) for word in review]\n",
    "    return ' '.join(lemmatized_review)    \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "\n",
    "avg_len_after_prepro = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "\n",
    "print('Average length of the reviews in terms of character length before preprocessing: ', avg_len_before_prepro)\n",
    "print('Average length of the reviews in terms of character length after preprocessing: ', avg_len_after_prepro)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(train['review_body'])\n",
    "test_corpus = list(test['review_body'])\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 0.001)\n",
    "X_train = tfidf_vectorizer.fit_transform(train_corpus)\n",
    "X_train = pd.DataFrame(X_train.toarray())\n",
    "X_test = tfidf_vectorizer.transform(test_corpus)\n",
    "X_test = pd.DataFrame(X_test.toarray())\n",
    "Y_train = train['label']\n",
    "Y_test = test['label']\n",
    "Y_train = Y_train.astype('int')\n",
    "Y_test = Y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision\t Recall\t\t F1Score\n",
      "0.652927400468384 \t 0.682162955713237 \t 0.6672250807706114\n",
      "0.6447118429385688 \t 0.25558624152648757 \t 0.3660553757641136\n",
      "0.5522679239148106 \t 0.8643765903307888 \t 0.6739410772740799\n",
      "AvgPrecision\t\t AvgRecall\t\t AvgF1Score\n",
      "0.6166357224405878 \t 0.6007085958568378 \t 0.569073844602935\n"
     ]
    }
   ],
   "source": [
    "perceptr = Perceptron(random_state = 100)\n",
    "perceptr.fit(X_train, Y_train)\n",
    "Y_pred_test = perceptr.predict(X_test)\n",
    "\n",
    "# target_names = ['class 1', 'class 2', 'class 3']\n",
    "# print(classification_report(Y_test, Y_pred_test, target_names=target_names))\n",
    "\n",
    "percep_scores = precision_recall_fscore_support(Y_test, Y_pred_test, average=None)\n",
    "average_percep_scores = precision_recall_fscore_support(Y_test, Y_pred_test, average='macro')\n",
    "print('Precision\\t', 'Recall\\t\\t', 'F1Score')\n",
    "print(percep_scores[0][0], '\\t', percep_scores[1][0], '\\t', percep_scores[2][0])\n",
    "print(percep_scores[0][1], '\\t', percep_scores[1][1], '\\t', percep_scores[2][1])\n",
    "print(percep_scores[0][2], '\\t', percep_scores[1][2], '\\t', percep_scores[2][2])\n",
    "\n",
    "print('AvgPrecision\\t\\t', 'AvgRecall\\t\\t', 'AvgF1Score')\n",
    "print(average_percep_scores[0],'\\t', average_percep_scores[1],'\\t', average_percep_scores[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision\t Recall\t\t F1Score\n",
      "0.6890009606147934 \t 0.7019818938096404 \t 0.6954308568658345\n",
      "0.6022478070175439 \t 0.5515942756716044 \t 0.575809199318569\n",
      "0.7058261700095511 \t 0.7521628498727735 \t 0.7282581916728257\n",
      "AvgPrecision\t\t AvgRecall\t\t AvgF1Score\n",
      "0.6656916458806293 \t 0.668579673118006 \t 0.6664994159524097\n"
     ]
    }
   ],
   "source": [
    "clf_svm = LinearSVC(random_state=100, max_iter=1000)\n",
    "clf_svm.fit(X_train, Y_train)\n",
    "Y_pred_test_svm = clf_svm.predict(X_test)\n",
    "\n",
    "svm_scores = precision_recall_fscore_support(Y_test, Y_pred_test_svm, average=None)\n",
    "average_svm_scores = precision_recall_fscore_support(Y_test, Y_pred_test_svm, average='macro')\n",
    "print('Precision\\t', 'Recall\\t\\t', 'F1Score')\n",
    "print(svm_scores[0][0], '\\t', svm_scores[1][0], '\\t', svm_scores[2][0])\n",
    "print(svm_scores[0][1], '\\t', svm_scores[1][1], '\\t', svm_scores[2][1])\n",
    "print(svm_scores[0][2], '\\t', svm_scores[1][2], '\\t', svm_scores[2][2])\n",
    "\n",
    "print('AvgPrecision\\t\\t', 'AvgRecall\\t\\t', 'AvgF1Score')\n",
    "print(average_svm_scores[0],'\\t', average_svm_scores[1],'\\t', average_svm_scores[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(random_state = 100, max_iter=1000)\n",
    "logistic.fit(X_train, Y_train)\n",
    "Y_pred_test_logis = logistic.predict(X_test)\n",
    "\n",
    "logistic_scores = precision_recall_fscore_support(Y_test, Y_pred_test_logis, average=None)\n",
    "average_logistic_scores = precision_recall_fscore_support(Y_test, Y_pred_test_logis, average='macro')\n",
    "print('Precision\\t', 'Recall\\t\\t', 'F1Score')\n",
    "print(logistic_scores[0][0], '\\t', logistic_scores[1][0], '\\t', logistic_scores[2][0])\n",
    "print(logistic_scores[0][1], '\\t', logistic_scores[1][1], '\\t', logistic_scores[2][1])\n",
    "print(logistic_scores[0][2], '\\t', logistic_scores[1][2], '\\t', logistic_scores[2][2])\n",
    "\n",
    "print('AvgPrecision\\t\\t', 'AvgRecall\\t\\t', 'AvgF1Score')\n",
    "print(average_logistic_scores[0],'\\t', average_logistic_scores[1],'\\t', average_logistic_scores[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bay = MultinomialNB(force_alpha=True)\n",
    "naive_bay.fit(X_train, Y_train)\n",
    "Y_pred_test_naive = logistic.predict(X_test)\n",
    "\n",
    "naive_scores = precision_recall_fscore_support(Y_test, Y_pred_test_naive, average=None)\n",
    "average_naive_scores = precision_recall_fscore_support(Y_test, Y_pred_test_naive, average='macro')\n",
    "print('Precision\\t', 'Recall\\t\\t', 'F1Score')\n",
    "print(naive_scores[0][0], '\\t', naive_scores[1][0], '\\t', naive_scores[2][0])\n",
    "print(naive_scores[0][1], '\\t', naive_scores[1][1], '\\t', naive_scores[2][1])\n",
    "print(naive_scores[0][2], '\\t', naive_scores[1][2], '\\t', naive_scores[2][2])\n",
    "\n",
    "print('AvgPrecision\\t\\t', 'AvgRecall\\t\\t', 'AvgF1Score')\n",
    "print(average_naive_scores[0],'\\t', average_naive_scores[1],'\\t', average_naive_scores[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
