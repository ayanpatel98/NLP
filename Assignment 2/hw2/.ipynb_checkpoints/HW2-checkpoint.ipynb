{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64318c2c",
   "metadata": {},
   "source": [
    "Task 1: Vocabulary Creation \n",
    "\n",
    "- For the vocabulary creation task we first read the train dataset file and create a dataframe of it and replace all words with unknown tag for which the frequency of the particular word is below the threshold\n",
    "- Secondly we use the frequency as the parameter to sort the words in the dataframe and then we extract all the unique words in tha ascending order of the dataframe and then export it to the csv file having columns 'name', 'position of the word', 'frequency'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9af09273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the selected threshold for unknown words replacement? 2\n",
      "What is the total size of your vocabulary? 16920\n",
      "What is the total occurrences of the special token '<unk>' after replacement? 32537\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv('./data/train', sep='\\t', names=['idx', 'name', 'tag'])\n",
    "df['frequency'] = df['name'].map(df['name'].value_counts())\n",
    "def replace_unk(entry):\n",
    "    if entry['frequency']<=2:\n",
    "        return '<unk>'\n",
    "    else:\n",
    "        return entry['name']\n",
    "\n",
    "df['name'] = df.apply(lambda entry: replace_unk(entry), axis=1)\n",
    "\n",
    "# Sort by descending freq\n",
    "df = df.sort_values(by=['frequency'], ascending=False)\n",
    "# Return a Series containing counts of unique values.\n",
    "df_counted = df['name'].value_counts().reset_index()\n",
    "df_counted.columns = [''] * len(df_counted.columns)\n",
    "\n",
    "df_counted.columns = ['name', 'frequency']\n",
    "df_unknown = df_counted[df_counted['name']=='<unk>']\n",
    "\n",
    "unk_idx = df_counted[df_counted['name']=='<unk>'].index\n",
    "\n",
    "df_counted = df_counted.drop(index=unk_idx)\n",
    "df_counted = pd.concat([df_unknown, df_counted])\n",
    "df_counted = df_counted.reset_index()\n",
    "df_counted['index'] = df_counted.index+1\n",
    "columns_titles = [\"name\",\"index\", \"frequency\"]\n",
    "df_counted=df_counted.reindex(columns=columns_titles)\n",
    "df_counted.to_csv(\"vocab.txt\", sep=\"\\t\", header=None, index=False)\n",
    "\n",
    "print(\"What is the selected threshold for unknown words replacement?\", 2)\n",
    "print(\"What is the total size of your vocabulary?\", len(df_counted))\n",
    "print(\"What is the total occurrences of the special token '<unk>' after replacement?\", int(df_unknown['frequency']\n",
    "                                                                                           [df_unknown['name']=='<unk>']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ffbc29",
   "metadata": {},
   "source": [
    "Task 2: Model Learning\n",
    "\n",
    "- In this task we create transition matrix and emmission matrix and after creating both the matrices we then convert the matrix to a python dictionary for faster retrieval of data.\n",
    "- transition matrix is of size: length of all_tags * length of all_tags\n",
    "- row represents the tag at the previous state and the column represents the tag at the current state for which we want to calculate the transition probability\n",
    "- Emmission matrix is of size:  length of all_tags * length of vocabualry\n",
    "- row represents the current tag at the previous state and the column represents the vocab word for which we want to calculate the emmission probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a21974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train', sep='\\t', names=['idx', 'name', 'tag'])\n",
    "df['frequency'] = df['name'].map(df['name'].value_counts())\n",
    "df['name'] = df.apply(lambda entry: replace_unk(entry), axis=1)\n",
    "df_pos = pd.DataFrame(df['tag'].value_counts()).reset_index()\n",
    "df_pos.columns = [''] * len(df_pos.columns)\n",
    "df_pos.columns = ['tag', 'count']\n",
    "all_tags = list(df_pos['tag'])\n",
    "\n",
    "all_sentences = []\n",
    "temp_sentence = []\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i]['idx']==1 and i!=0:\n",
    "        all_sentences.append(temp_sentence)\n",
    "        temp_sentence =[]\n",
    "    temp_sentence.append((df.loc[i]['name'], df.loc[i]['tag']))\n",
    "    \n",
    "\n",
    "\n",
    "# Transition Matrix code\n",
    "transition_matrix = [[0 for j in range(len(all_tags))] for i in range(len(all_tags))]\n",
    "tag_freq = {} # format: key = <TAG>, value = <tag_freq>\n",
    "def generate_transition_matrix():\n",
    "    # Calculate tag frequency\n",
    "    for sentence in all_sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            curr_tag = sentence[i][1]\n",
    "            if curr_tag not in tag_freq:\n",
    "                tag_freq[curr_tag]=1\n",
    "            else:\n",
    "                tag_freq[curr_tag]+=1\n",
    "    \n",
    "    # 1. Calculate the number of transitions from one tag to another for each sentence\n",
    "    for sentence in all_sentences:\n",
    "        for i in range(1, len(sentence)):\n",
    "            curr_tag_index = all_tags.index(sentence[i][1])\n",
    "            prev_tag_index = all_tags.index(sentence[i-1][1])\n",
    "            transition_matrix[prev_tag_index][curr_tag_index]+=1\n",
    "    \n",
    "    # 2. Calculate the transition probabilities for each transition\n",
    "    for i in range(len(transition_matrix)):\n",
    "        for j in range(len(transition_matrix[0])):\n",
    "            prev_tag_index = i\n",
    "            prev_tag_count = tag_freq[all_tags[i]]\n",
    "            transition_matrix[i][j]/=prev_tag_count\n",
    "            \n",
    "# Emmission Matrix code\n",
    "vocabulary = list(df_counted['name'])\n",
    "emmission_matrix = [[0 for j in range(len(vocabulary))] for i in range(len(all_tags))]\n",
    "def generate_emmission_matrix():    \n",
    "    # 1. Calculate the number of transitions from one tag to another for each sentence\n",
    "    for sentence in all_sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            curr_word_index = vocabulary.index(sentence[i][0])\n",
    "            prev_tag_index = all_tags.index(sentence[i][1])\n",
    "            emmission_matrix[prev_tag_index][curr_word_index]+=1\n",
    "    \n",
    "    # 2. Calculate the transition probabilities for each transition\n",
    "    for i in range(len(emmission_matrix)):\n",
    "        for j in range(len(emmission_matrix[0])):\n",
    "            prev_tag_index = i\n",
    "            prev_tag_count = tag_freq[all_tags[i]]\n",
    "            emmission_matrix[i][j]/=prev_tag_count\n",
    "\n",
    "\n",
    "generate_transition_matrix()\n",
    "generate_emmission_matrix()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542ef4bb",
   "metadata": {},
   "source": [
    "- Below is the Code for conversion of transition and emmission matrix to respective dictionary\n",
    "- For the transition dictionary each key represents a tuple in which  first value is the previous tag and the second value of the tuple is the current tag and the value of the key represents the transition probability from the previous tag to the current tag\n",
    "- Likewise for the emmission dictionary each key represents a tuple in which  first value is the current tag and the second value of the tuple is the vocab word from which the current tag is pointed to, and the value of the key represents the emmission probability from the current tag to the current vocab word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eed24bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transition and emission parameters in the HMM model:  2070 , 761400\n"
     ]
    }
   ],
   "source": [
    "# Code for conversion of transition and emmission matrix to respective dictionary\n",
    "\n",
    "\n",
    "start_tags = {} # maintains the initial tag frequency of all tag.\n",
    "def starting_transition_prob():\n",
    "    \n",
    "    start_tags_total = 0\n",
    "    start_tags_prob = {}\n",
    "    \n",
    "    for i in range(len(all_tags)):\n",
    "        start_tags[all_tags[i]]=0\n",
    "        \n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i]['idx']==1:\n",
    "            start_tags_total+=1\n",
    "            start_tags[df.loc[i]['tag']]+=1\n",
    "    \n",
    "    for tag in start_tags:\n",
    "        start_tags_prob[tag] = start_tags[tag]/start_tags_total\n",
    "    \n",
    "    return start_tags_prob\n",
    "\n",
    "# transition matrix is of dimension: len of all_tags * len of all_tags\n",
    "def calculate_trans_prob():\n",
    "    trans_prob_dict = {}\n",
    "    for i in range(len(transition_matrix)):\n",
    "        for j in range(len(transition_matrix[0])):\n",
    "            tag_at_i = all_tags[i]\n",
    "            tag_at_j = all_tags[j]\n",
    "            trans_prob_dict['(' + tag_at_i + ', ' + tag_at_j + ')'] = transition_matrix[i][j]\n",
    "    return trans_prob_dict\n",
    "\n",
    "# emmission matrix is of dimension: len of all_tags * len of vocabulary\n",
    "def calculate_emmission_prob():\n",
    "    emmission_prob_dict = {}\n",
    "    for i in range(len(emmission_matrix)):\n",
    "        for j in range(len(vocabulary)):\n",
    "            tag_at_i = all_tags[i]\n",
    "            vocab_at_j = vocabulary[j]\n",
    "            emmission_prob_dict['(' + tag_at_i + ', ' + vocab_at_j + ')'] = emmission_matrix[i][j]\n",
    "    \n",
    "    return emmission_prob_dict\n",
    "\n",
    "# Probability Matrices\n",
    "trans_prob_dict = calculate_trans_prob()\n",
    "emmission_prob_dict = calculate_emmission_prob()\n",
    "start_tags_prob = starting_transition_prob()\n",
    "\n",
    "total_transition_prob = {}\n",
    "# Add both transition probs in to the final transition dictionary\n",
    "for key in start_tags_prob:\n",
    "    total_transition_prob['(' + '<s>' + ', ' + key + ')'] = start_tags_prob[key]\n",
    "    \n",
    "for key in trans_prob_dict:\n",
    "    total_transition_prob[key] = trans_prob_dict[key]\n",
    "\n",
    "print(\"Total transition and emission parameters in the HMM model: \", len(total_transition_prob), ',', len(emmission_prob_dict))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e98630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the dictionaries to the json file\n",
    "with open('hmm.json', 'w') as f:\n",
    "    json.dump({\"transition\": total_transition_prob, \"emission\": emmission_prob_dict}, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57411543",
   "metadata": {},
   "source": [
    "Task 3: Greedy Decoding with HMM\n",
    "\n",
    "- In greedy decoding for each transition in the states we calculate the net probability  score till the current tag every time and store the tag which is giving maximum score and the stored tag will help us get the final sequence and accuracy of the greedy decoding algorithm.\n",
    "\n",
    "- To handle a situation where a vocab word is not present in the emmission matrix then we use the probability for the unknown words considering the not found word as unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ecd32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the dev data for Greedy Decoding 0.9298904752146093\n"
     ]
    }
   ],
   "source": [
    "df_dev = pd.read_csv('./data/dev', sep='\\t', names=['idx', 'name', 'tag'])\n",
    "df_dev['frequency'] = df_dev['name'].map(df_dev['name'].value_counts())\n",
    "\n",
    "\n",
    "all_sentences_dev = []\n",
    "temp_sentence_dev = []\n",
    "for i in range(len(df_dev)):\n",
    "    if df_dev.loc[i]['idx']==1 and i!=0:\n",
    "        all_sentences_dev.append(temp_sentence_dev)\n",
    "        temp_sentence_dev =[]\n",
    "    temp_sentence_dev.append((df_dev.loc[i]['name'], df_dev.loc[i]['tag']))\n",
    "    \n",
    "from_tag = None\n",
    "tag_sequence = []\n",
    "sentence_scores = [] # score for each tag to word score for each sentence\n",
    "\n",
    "for sentence in all_sentences_dev:\n",
    "    curr_sentence_scores = []\n",
    "    curr_sequence = []\n",
    "    for i in range(len(sentence)):\n",
    "        max_score = float('-inf') # initialize max score\n",
    "        for j in range(len(all_tags)):\n",
    "            curr_score = 1\n",
    "            if i==0:\n",
    "                curr_score *= start_tags_prob[all_tags[j]]\n",
    "            else:\n",
    "                curr_score *= trans_prob_dict['(' + from_tag + ', ' + all_tags[j] + ')']\n",
    "            \n",
    "            if str('(' + all_tags[j] + ', ' + sentence[i][0] + ')') not in emmission_prob_dict:\n",
    "                curr_score *= emmission_prob_dict['(' + all_tags[j] + ', ' + '<unk>' + ')']\n",
    "            else:\n",
    "                curr_score *= emmission_prob_dict['(' + all_tags[j] + ', ' + sentence[i][0] + ')']\n",
    "            \n",
    "            if curr_score>max_score:\n",
    "                max_score = curr_score\n",
    "                highest_score_tag = all_tags[j]\n",
    "        \n",
    "        from_tag = highest_score_tag\n",
    "        curr_sequence.append(highest_score_tag)\n",
    "        curr_sentence_scores.append(max_score)\n",
    "    sentence_scores.append(curr_sentence_scores)\n",
    "    tag_sequence.append(curr_sequence)\n",
    "\n",
    "def accuracy_finder():\n",
    "    frequency = 0\n",
    "    cur_tag_freq = 0\n",
    "    \n",
    "    for i in range(len(all_sentences_dev)):\n",
    "        for j in range(len(all_sentences_dev[i])):\n",
    "            if tag_sequence[i][j]==all_sentences_dev[i][j][1]:\n",
    "                cur_tag_freq+=1\n",
    "            frequency+=1\n",
    "    return cur_tag_freq/frequency\n",
    "\n",
    "print(\"Accuracy on the dev data for Greedy Decoding in percent: \", accuracy_finder()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28279aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Decoding Test Data\n",
    "df_test = pd.read_csv('./data/test', sep='\\t', names=['idx', 'name', 'tag'])\n",
    "df_test['frequency'] = df_test['name'].map(df_test['name'].value_counts())\n",
    "\n",
    "\n",
    "all_sentences_test = []\n",
    "temp_sentence_test = []\n",
    "for i in range(len(df_test)):\n",
    "    if df_test.loc[i]['idx']==1 and i!=0:\n",
    "        all_sentences_test.append(temp_sentence_test)\n",
    "        temp_sentence_test =[]\n",
    "    temp_sentence_test.append(df_test.loc[i]['name'])\n",
    "    \n",
    "from_tag_test = None\n",
    "tag_sequence_test = []\n",
    "sentence_scores_test = [] # score for each tag to word score for each sentence\n",
    "\n",
    "for sentence in all_sentences_test:\n",
    "    curr_sentence_scores = []\n",
    "    curr_sequence = []\n",
    "    for i in range(len(sentence)):\n",
    "        max_score = float('-inf') # initialize max score\n",
    "        for j in range(len(all_tags)):\n",
    "            curr_score = 1\n",
    "            if i==0:\n",
    "                curr_score *= start_tags_prob[all_tags[j]]\n",
    "            else:\n",
    "                curr_score *= trans_prob_dict['(' + from_tag_test + ', ' + all_tags[j] + ')']\n",
    "            \n",
    "            if str('(' + all_tags[j] + ', ' + sentence[i] + ')') not in emmission_prob_dict:\n",
    "                curr_score *= emmission_prob_dict['(' + all_tags[j] + ', ' + '<unk>' + ')']\n",
    "            else:\n",
    "                curr_score *= emmission_prob_dict['(' + all_tags[j] + ', ' + sentence[i] + ')']\n",
    "            \n",
    "            if curr_score>max_score:\n",
    "                max_score = curr_score\n",
    "                highest_score_tag = all_tags[j]\n",
    "        \n",
    "        from_tag_test = highest_score_tag\n",
    "        curr_sequence.append(highest_score_tag)\n",
    "        curr_sentence_scores.append(max_score)\n",
    "    sentence_scores_test.append(curr_sentence_scores)\n",
    "    tag_sequence_test.append(curr_sequence)\n",
    "\n",
    "# Creating the final output format\n",
    "final_output = []\n",
    "\n",
    "for i in range(len(all_sentences_test)):\n",
    "    idx = 1\n",
    "    curr_sentence = []\n",
    "    for j in range(len(all_sentences_test[i])):\n",
    "        curr_sentence.append([idx, all_sentences_test[i][j], tag_sequence_test[i][j]])\n",
    "        idx+=1\n",
    "    final_output.append(curr_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff8d6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the final output: greedy.out\n",
    "with open(\"greedy.out\", 'w') as f:\n",
    "    for item in final_output:\n",
    "        for element in item:\n",
    "            f.write(\"\\n\".join([str(element[0])+\"\\t\"+element[1]+\"\\t\"+element[2]]))\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b7c00",
   "metadata": {},
   "source": [
    "Task 4: Viterbi Decoding with HMM\n",
    "\n",
    "- For Viterbi Decoding , we maintain a list/array representing all the scores for the previous state/pos tags.\n",
    "- We maintain a Memo or cache memory as a dictionary for storing all indices and pos tags as the key and value as the score or the probability, the cache will continue updating until we find a better transition mapping from one tag and to a vocab word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6432d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viterbi decoding for each sentence:\n",
    "\n",
    "def viterbi_decoding_algo(sentence):\n",
    "    tag_score_probs = []\n",
    "    memo={}\n",
    "    for i in range(len(all_tags)):\n",
    "        if str('(' + all_tags[i] + ', ' + sentence[0][0] + ')') not in emmission_prob_dict:\n",
    "            tag_score_probs.append(start_tags_prob[all_tags[i]]*\n",
    "                                   emmission_prob_dict['(' + all_tags[i] + ', ' + '<unk>' + ')'])\n",
    "        else:\n",
    "            tag_score_probs.append(start_tags_prob[all_tags[i]]*\n",
    "                                   emmission_prob_dict['(' + all_tags[i] + ', ' + sentence[0][0] + ')'])\n",
    "            \n",
    "    for i in range(1, len(sentence)):\n",
    "        temp_tag_score=[float('-inf')]*len(all_tags)\n",
    "        for j in range(len(all_tags)): # CURRENT\n",
    "            best_score = float('-inf')\n",
    "            curr_score = 1\n",
    "            for k in range(len(tag_score_probs)): # PREVIOUS\n",
    "                if str('(' + all_tags[j] + ', ' + sentence[i][0] + ')') not in emmission_prob_dict:\n",
    "                    curr_score = tag_score_probs[k] * trans_prob_dict['(' + all_tags[k] + ', ' + all_tags[j] + ')']* emmission_prob_dict['(' + all_tags[j] + ', ' + '<unk>' + ')']\n",
    "                \n",
    "                else:\n",
    "                    curr_score = tag_score_probs[k] * trans_prob_dict['(' + all_tags[k] + ', ' + all_tags[j] + ')'] * emmission_prob_dict['(' + all_tags[j] + ', ' + sentence[i][0] + ')']\n",
    "                    \n",
    "                if best_score<curr_score:\n",
    "                    best_score=curr_score\n",
    "                    memo[i, all_tags[j]] = (all_tags[k], curr_score)\n",
    "            temp_tag_score[j] = best_score\n",
    "        tag_score_probs = temp_tag_score\n",
    "    \n",
    "    return tag_score_probs, memo\n",
    "                    \n",
    "                        \n",
    "                        \n",
    "each_sentence_cache = []\n",
    "each_sentence_scores = []\n",
    "\n",
    "for sentence in all_sentences_dev:\n",
    "    op = viterbi_decoding_algo(sentence)\n",
    "    each_sentence_cache.append(op[1])\n",
    "    each_sentence_scores.append(op[0])\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c8a9a",
   "metadata": {},
   "source": [
    "- We calculated all the best scores for each sentence and stored it to a memo/cache using viterbi decoding, now we propagate backward to find the best pos tag sequence for the corresponding sentence.\n",
    "- While propagating backwords we maintain the tag which gives the max score until now and find the next best tag (using the cache/memo) at the previous position, thats how we find the final sequence for that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65bfdf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the dev data for Viterbi Decoding in percent:  94.37120021859417\n"
     ]
    }
   ],
   "source": [
    "#  Back propagate to find the best sequence\n",
    "\n",
    "def propagate_bacwards(i):\n",
    "    final_seq = []\n",
    "    final_seq_scores = []\n",
    "    sent_scores = each_sentence_scores[i]\n",
    "    max_tag = all_tags[sent_scores.index(max(sent_scores))]\n",
    "    final_seq.append(max_tag)\n",
    "    sent_memo = each_sentence_cache[i]\n",
    "    \n",
    "    for j in range(int(len(each_sentence_cache[i])/len(all_tags)), 0, -1):\n",
    "        score, max_tag = sent_memo[(j, max_tag)][1], sent_memo[(j, max_tag)][0]\n",
    "        temp_max = [max_tag]\n",
    "        temp_max.extend(final_seq)\n",
    "        final_seq = temp_max\n",
    "        temp_score = [score]\n",
    "        temp_score.extend(final_seq_scores)\n",
    "        final_seq_scores = temp_score\n",
    "    \n",
    "    return final_seq, final_seq_scores\n",
    "\n",
    "final_seq = []\n",
    "final_seq_scores = []\n",
    "for i in range(len(each_sentence_scores)):\n",
    "    op_dev = propagate_bacwards(i)\n",
    "    final_seq.append(op_dev[0])\n",
    "    final_seq_scores.append(op_dev[1])\n",
    "\n",
    "# Accuracy finder\n",
    "def accuracy_finder_viterbi():\n",
    "    frequency = 0\n",
    "    cur_tag_freq = 0\n",
    "\n",
    "    for i in range(len(all_sentences_dev)):\n",
    "        for j in range(len(all_sentences_dev[i])):\n",
    "            if final_seq[i][j]==all_sentences_dev[i][j][1]:\n",
    "                cur_tag_freq+=1\n",
    "            frequency+=1\n",
    "    return cur_tag_freq/frequency\n",
    "\n",
    "print(\"Accuracy on the dev data for Viterbi Decoding in percent: \", accuracy_finder_viterbi()*100)\n",
    "\n",
    "# Testing data Viterbi Algorithm                        \n",
    "each_sentence_cache_test = []\n",
    "each_sentence_scores_test = []\n",
    "\n",
    "for sentence in all_sentences_test:\n",
    "    op_test = viterbi_decoding_algo(sentence)\n",
    "    each_sentence_cache_test.append(op_test[1])\n",
    "    each_sentence_scores_test.append(op_test[0])\n",
    "    \n",
    "def propagate_bacwards_testing(i):\n",
    "    final_seq = []\n",
    "    final_seq_scores = []\n",
    "    sent_scores = each_sentence_scores_test[i]\n",
    "    sent_memo = each_sentence_cache_test[i]\n",
    "    max_tag = all_tags[sent_scores.index(max(sent_scores))]\n",
    "    final_seq.append(max_tag)\n",
    "    \n",
    "    for j in range(int(len(each_sentence_cache_test[i])/len(all_tags)), 0, -1):\n",
    "        score, max_tag = sent_memo[(j, max_tag)][1], sent_memo[(j, max_tag)][0]\n",
    "        temp_max = [max_tag]\n",
    "        temp_max.extend(final_seq)\n",
    "        final_seq = temp_max\n",
    "        temp_score = [score]\n",
    "        temp_score.extend(final_seq_scores)\n",
    "        final_seq_scores = temp_score\n",
    "    \n",
    "    return final_seq, final_seq_scores\n",
    "\n",
    "final_seq_test = []\n",
    "final_seq_scores_test = []\n",
    "for i in range(len(each_sentence_scores_test)):\n",
    "    op_test = propagate_bacwards_testing(i)\n",
    "    final_seq_test.append(op_test[0])\n",
    "    final_seq_scores_test.append(op_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76cf9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the final output: viterbi.out\n",
    "# Creating the final output format\n",
    "final_output_test = []\n",
    "for i in range(len(all_sentences_test)):\n",
    "    idx = 1\n",
    "    curr_sentence = []\n",
    "    for j in range(len(all_sentences_test[i])):\n",
    "        curr_sentence.append([idx, all_sentences_test[i][j], final_seq_test[i][j]])\n",
    "        idx+=1\n",
    "    final_output_test.append(curr_sentence)\n",
    "    \n",
    "with open(\"viterbi.out\", 'w') as f:\n",
    "    for item in final_output_test:\n",
    "        for element in item:\n",
    "            f.write(\"\\n\".join([str(element[0])+\"\\t\"+element[1]+\"\\t\"+element[2]]))\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b79a53a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
