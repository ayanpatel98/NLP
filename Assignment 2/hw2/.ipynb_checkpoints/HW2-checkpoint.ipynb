{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a0abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64318c2c",
   "metadata": {},
   "source": [
    "Task 1: Vocabulary Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9af09273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the selected threshold for unknown words replacement? 2\n",
      "What is the total size of your vocabulary 23183\n",
      "What is the total occurrences of the special token '<unk>' after replacement? 20011\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/train', sep='\\t', names=['idx', 'name', 'tag'])\n",
    "df['frequency'] = df['name'].map(df['name'].value_counts())\n",
    "def replace_unk(entry):\n",
    "    if entry['frequency']<=1:\n",
    "        return '<unk>'\n",
    "    else:\n",
    "        return entry['name']\n",
    "\n",
    "df['name'] = df.apply(lambda entry: replace_unk(entry), axis=1)\n",
    "\n",
    "# Sort by descending freq\n",
    "df = df.sort_values(by=['frequency'], ascending=False)\n",
    "# Return a Series containing counts of unique values.\n",
    "df_counted = df['name'].value_counts().reset_index()\n",
    "df_counted.columns = [''] * len(df_counted.columns)\n",
    "\n",
    "df_counted.columns = ['name', 'frequency']\n",
    "df_unknown = df_counted[df_counted['name']=='<unk>']\n",
    "\n",
    "unk_idx = df_counted[df_counted['name']=='<unk>'].index\n",
    "\n",
    "df_counted = df_counted.drop(index=unk_idx)\n",
    "df_counted = pd.concat([df_unknown, df_counted])\n",
    "df_counted = df_counted.reset_index()\n",
    "df_counted['index'] = df_counted.index+1\n",
    "columns_titles = [\"name\",\"index\", \"frequency\"]\n",
    "df_counted=df_counted.reindex(columns=columns_titles)\n",
    "df_counted.to_csv(\"vocab.txt\", sep=\"\\t\", header=None)\n",
    "\n",
    "print(\"What is the selected threshold for unknown words replacement?\", 2)\n",
    "print(\"What is the total size of your vocabulary\", len(df_counted))\n",
    "print(\"What is the total occurrences of the special token '<unk>' after replacement?\", int(df_unknown['frequency']\n",
    "                                                                                           [df_unknown['name']=='<unk>']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a21974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train', sep='\\t', names=['idx', 'name', 'tag'])\n",
    "df['frequency'] = df['name'].map(df['name'].value_counts())\n",
    "df['name'] = df.apply(lambda entry: replace_unk(entry), axis=1)\n",
    "df_pos = pd.DataFrame(df['tag'].value_counts()).reset_index()\n",
    "df_pos.columns = [''] * len(df_pos.columns)\n",
    "df_pos.columns = ['tag', 'count']\n",
    "all_tags = list(df_pos['tag'])\n",
    "\n",
    "all_sentences = []\n",
    "temp_sentence = []\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i]['idx']==1 and i!=0:\n",
    "        all_sentences.append(temp_sentence)\n",
    "        temp_sentence =[]\n",
    "    temp_sentence.append((df.loc[i]['name'], df.loc[i]['tag']))\n",
    "    \n",
    "\n",
    "\n",
    "# Transition Matrix code\n",
    "transition_matrix = [[0 for j in range(len(all_tags))] for i in range(len(all_tags))]\n",
    "tag_freq = {} # format: key = <TAG>, value = <tag_freq>\n",
    "def generate_transition_matrix():\n",
    "    # Calculate tag frequency\n",
    "    for sentence in all_sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            curr_tag = sentence[i][1]\n",
    "            if curr_tag not in tag_freq:\n",
    "                tag_freq[curr_tag]=1\n",
    "            else:\n",
    "                tag_freq[curr_tag]+=1\n",
    "    \n",
    "    # 1. Calculate the number of transitions from one tag to another for each sentence\n",
    "    for sentence in all_sentences:\n",
    "        for i in range(1, len(sentence)):\n",
    "            curr_tag_index = all_tags.index(sentence[i][1])\n",
    "            prev_tag_index = all_tags.index(sentence[i-1][1])\n",
    "            transition_matrix[prev_tag_index][curr_tag_index]+=1\n",
    "    \n",
    "    # 2. Calculate the transition probabilities for each transition\n",
    "    for i in range(len(transition_matrix)):\n",
    "        for j in range(len(transition_matrix[0])):\n",
    "            prev_tag_index = i\n",
    "            prev_tag_count = tag_freq[all_tags[i]]\n",
    "            if(transition_matrix[i][j] == 0) : transition_matrix[i][j] = 1e-10\n",
    "            else: transition_matrix[i][j]/=prev_tag_count\n",
    "            \n",
    "# Emmission Matrix code\n",
    "vocabulary = list(df_counted['name'])\n",
    "emmission_matrix = [[0 for j in range(len(vocabulary))] for i in range(len(all_tags))]\n",
    "def generate_emmission_matrix():    \n",
    "    # 1. Calculate the number of transitions from one tag to another for each sentence\n",
    "    for sentence in all_sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            curr_word_index = vocabulary.index(sentence[i][0])\n",
    "            prev_tag_index = all_tags.index(sentence[i][1])\n",
    "            emmission_matrix[prev_tag_index][curr_word_index]+=1\n",
    "    \n",
    "    # 2. Calculate the transition probabilities for each transition\n",
    "    for i in range(len(emmission_matrix)):\n",
    "        for j in range(len(emmission_matrix[0])):\n",
    "            prev_tag_index = i\n",
    "            prev_tag_count = tag_freq[all_tags[i]]\n",
    "            if(emmission_matrix[i][j] == 0) : emmission_matrix[i][j] = 1e-10\n",
    "            else: emmission_matrix[i][j]/=prev_tag_count\n",
    "\n",
    "\n",
    "generate_transition_matrix()\n",
    "generate_emmission_matrix()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c243672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(emmission_matrix)\n",
    "# np.array(transition_matrix)\n",
    "# all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed24bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transition and emission parameters in the HMM model:  2070 , 1043235\n"
     ]
    }
   ],
   "source": [
    "# Code for conversion of transition and emmission matrix to respective dictionary\n",
    "\n",
    "def calculate_trans_prob():\n",
    "    trans_prob_dict = {}\n",
    "    for i in range(len(transition_matrix)):\n",
    "        for j in range(len(transition_matrix[0])):\n",
    "            tag_at_i = all_tags[i]\n",
    "            tag_at_j = all_tags[j]\n",
    "            trans_prob_dict['(' + tag_at_i + ', ' + tag_at_j + ')'] = transition_matrix[i][j]\n",
    "    return trans_prob_dict\n",
    "            \n",
    "def calculate_emmission_prob():\n",
    "    emmission_prob_dict = {}\n",
    "    for i in range(len(emmission_matrix)):\n",
    "        for j in range(len(vocabulary)):\n",
    "            tag_at_i = all_tags[i]\n",
    "            vocab_at_j = vocabulary[j]\n",
    "            emmission_prob_dict['(' + tag_at_i + ', ' + vocab_at_j + ')'] = emmission_matrix[i][j]\n",
    "    \n",
    "    return emmission_prob_dict\n",
    "\n",
    "start_tags = {} # maintains the initial tag frequency of all tag.\n",
    "def starting_transition_prob():\n",
    "    \n",
    "    start_tags_total = 0\n",
    "    start_tags_prob = {}\n",
    "    \n",
    "    for i in range(len(all_tags)):\n",
    "        start_tags[all_tags[i]]=0\n",
    "        \n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i]['idx']==1:\n",
    "            start_tags_total+=1\n",
    "            start_tags[df.loc[i]['tag']]+=1\n",
    "    \n",
    "    for tag in start_tags:\n",
    "        start_tags_prob[tag] = start_tags[tag]/start_tags_total\n",
    "    \n",
    "    return start_tags_prob\n",
    "\n",
    "# Probability Matrices\n",
    "trans_prob_dict = calculate_trans_prob()\n",
    "emmission_prob_dict = calculate_emmission_prob()\n",
    "start_tags_prob = starting_transition_prob()\n",
    "\n",
    "total_transition_prob = {}\n",
    "# Add both transition probs in to the final transition dictionary\n",
    "for key in start_tags_prob:\n",
    "    total_transition_prob['(' + '<s>' + ', ' + key + ')'] = start_tags_prob[key]\n",
    "    \n",
    "for key in trans_prob_dict:\n",
    "    total_transition_prob[key] = trans_prob_dict[key]\n",
    "\n",
    "print(\"Total transition and emission parameters in the HMM model: \", len(total_transition_prob), ',', len(emmission_prob_dict))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e98630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the dictionaries to the json file\n",
    "with open('hmm.json', 'w') as f:\n",
    "    json.dump({\"transition\": total_transition_prob, \"emission\": emmission_prob_dict}, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57411543",
   "metadata": {},
   "source": [
    "Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ecd32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the dev data for Greedy Decoding 0.9351048568891318\n"
     ]
    }
   ],
   "source": [
    "df_dev = pd.read_csv('./data/dev', sep='\\t', names=['idx', 'name', 'tag'])\n",
    "df_dev['frequency'] = df_dev['name'].map(df_dev['name'].value_counts())\n",
    "\n",
    "\n",
    "all_sentences_dev = []\n",
    "temp_sentence_dev = []\n",
    "for i in range(len(df_dev)):\n",
    "    if df_dev.loc[i]['idx']==1 and i!=0:\n",
    "        all_sentences_dev.append(temp_sentence_dev)\n",
    "        temp_sentence_dev =[]\n",
    "    temp_sentence_dev.append((df_dev.loc[i]['name'], df_dev.loc[i]['tag']))\n",
    "    \n",
    "from_tag = None\n",
    "tag_sequence = []\n",
    "sentence_scores = [] # score for each tag to word score for each sentence\n",
    "\n",
    "for sentence in all_sentences_dev:\n",
    "    curr_sentence_scores = []\n",
    "    curr_sequence = []\n",
    "    for i in range(len(sentence)):\n",
    "        max_score = float('-inf') # initialize max score\n",
    "        for j in range(len(all_tags)):\n",
    "            curr_score = 1\n",
    "            if i==0:\n",
    "                curr_score *= start_tags_prob[all_tags[j]]\n",
    "            else:\n",
    "#                 if str('(' + from_tag + ', ' + all_tags[j] + ')') in trans_prob_dict: #optional\n",
    "                curr_score *= trans_prob_dict['(' + from_tag + ', ' + all_tags[j] + ')']\n",
    "            \n",
    "            if str('(' + all_tags[j] + ', ' + sentence[i][0] + ')') in emmission_prob_dict:\n",
    "                curr_score *= emmission_prob_dict['(' + all_tags[j] + ', ' + sentence[i][0] + ')']\n",
    "            else:\n",
    "                curr_score *= emmission_prob_dict['(' + all_tags[j] + ', ' + '<unk>' + ')']\n",
    "            \n",
    "            if curr_score>max_score:\n",
    "                max_score = curr_score\n",
    "                highest_score_tag = all_tags[j]\n",
    "        \n",
    "        from_tag = highest_score_tag\n",
    "        curr_sequence.append(highest_score_tag)\n",
    "        curr_sentence_scores.append(max_score)\n",
    "    sentence_scores.append(curr_sentence_scores)\n",
    "    tag_sequence.append(curr_sequence)\n",
    "\n",
    "def accuracy_finder():\n",
    "    frequency = 0\n",
    "    cur_tag_freq = 0\n",
    "    \n",
    "    for i in range(len(all_sentences_dev)):\n",
    "        for j in range(len(all_sentences_dev[i])):\n",
    "            if tag_sequence[i][j]==all_sentences_dev[i][j][1]:\n",
    "                cur_tag_freq+=1\n",
    "            frequency+=1\n",
    "    return cur_tag_freq/frequency\n",
    "\n",
    "print(\"Accuracy on the dev data for Greedy Decoding\", accuracy_finder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28279aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Decoding Test Data\n",
    "df_test = pd.read_csv('./data/test', sep='\\t', names=['idx', 'name', 'tag'])\n",
    "df_test['frequency'] = df_test['name'].map(df_test['name'].value_counts())\n",
    "\n",
    "\n",
    "all_sentences_test = []\n",
    "temp_sentence_test = []\n",
    "for i in range(len(df_test)):\n",
    "    if df_test.loc[i]['idx']==1 and i!=0:\n",
    "        all_sentences_test.append(temp_sentence_test)\n",
    "        temp_sentence_test =[]\n",
    "    temp_sentence_test.append(df_test.loc[i]['name'])\n",
    "    \n",
    "from_tag_test = None\n",
    "tag_sequence_test = []\n",
    "sentence_scores_test = [] # score for each tag to word score for each sentence\n",
    "\n",
    "for sentence in all_sentences_test:\n",
    "    curr_sentence_scores = []\n",
    "    curr_sequence = []\n",
    "    for i in range(len(sentence)):\n",
    "        max_score = float('-inf') # initialize max score\n",
    "        for j in range(len(all_tags)):\n",
    "            curr_score = 1\n",
    "            if i==0:\n",
    "                curr_score *= start_tags_prob[all_tags[j]]\n",
    "            else:\n",
    "#                 if str('(' + from_tag_test + ', ' + all_tags[j] + ')') in trans_prob_dict: #optional\n",
    "                curr_score *= trans_prob_dict['(' + from_tag_test + ', ' + all_tags[j] + ')']\n",
    "            \n",
    "            if str('(' + all_tags[j] + ', ' + sentence[i] + ')') in emmission_prob_dict:\n",
    "                curr_score *= emmission_prob_dict['(' + all_tags[j] + ', ' + sentence[i] + ')']\n",
    "            else:\n",
    "                curr_score *= emmission_prob_dict['(' + all_tags[j] + ', ' + '<unk>' + ')']\n",
    "            \n",
    "            if curr_score>max_score:\n",
    "                max_score = curr_score\n",
    "                highest_score_tag = all_tags[j]\n",
    "        \n",
    "        from_tag_test = highest_score_tag\n",
    "        curr_sequence.append(highest_score_tag)\n",
    "        curr_sentence_scores.append(max_score)\n",
    "    sentence_scores_test.append(curr_sentence_scores)\n",
    "    tag_sequence_test.append(curr_sequence)\n",
    "\n",
    "# Creating the final output format\n",
    "final_output = []\n",
    "\n",
    "for i in range(len(all_sentences_test)):\n",
    "    idx = 1\n",
    "    curr_sentence = []\n",
    "    for j in range(len(all_sentences_test[i])):\n",
    "        curr_sentence.append([idx, all_sentences_test[i][j], tag_sequence_test[i][j]])\n",
    "        idx+=1\n",
    "    final_output.append(curr_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff8d6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the final output: greedy.out\n",
    "with open(\"greedy.out\", 'w') as f:\n",
    "    for item in final_output:\n",
    "        for element in item:\n",
    "            f.write(\"\\n\".join([str(element[0])+\"\\t\"+element[1]+\"\\t\"+element[2]]))\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b7c00",
   "metadata": {},
   "source": [
    "Viterbi Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6432d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viterbi decoding for each sentence:\n",
    "\n",
    "def viterbi_decoding_algo(sentence):\n",
    "    tag_score_probs = []\n",
    "    memo={}\n",
    "    for i in range(len(all_tags)):\n",
    "        if str('(' + all_tags[i] + ', ' + sentence[0][0] + ')') in emmission_prob_dict:\n",
    "            tag_score_probs.append(start_tags_prob[all_tags[i]]*\n",
    "                                   emmission_prob_dict['(' + all_tags[i] + ', ' + sentence[0][0] + ')'])\n",
    "        else:\n",
    "            tag_score_probs.append(start_tags_prob[all_tags[i]]*\n",
    "                                   emmission_prob_dict['(' + all_tags[i] + ', ' + '<unk>' + ')'])\n",
    "            \n",
    "    for i in range(1, len(sentence)):\n",
    "        temp_tag_score=[float('-inf')]*len(all_tags)\n",
    "        for j in range(len(all_tags)): # CURRENT\n",
    "            best_score = float('-inf')\n",
    "            curr_score = 1\n",
    "            for k in range(len(tag_score_probs)): # PREVIOUS\n",
    "                if str('(' + all_tags[k] + ', ' + all_tags[j] + ')') in trans_prob_dict and str('(' + all_tags[j] + ', ' + sentence[i][0] + ')') in emmission_prob_dict:\n",
    "                    curr_score = tag_score_probs[k] * trans_prob_dict['(' + all_tags[k] + ', ' + all_tags[j] + ')'] * emmission_prob_dict['(' + all_tags[j] + ', ' + sentence[i][0] + ')']\n",
    "                \n",
    "                else:\n",
    "                    curr_score = tag_score_probs[k] * trans_prob_dict['(' + all_tags[k] + ', ' + all_tags[j] + ')']* emmission_prob_dict['(' + all_tags[j] + ', ' + '<unk>' + ')']\n",
    "                    \n",
    "                if best_score<curr_score:\n",
    "                    best_score=curr_score\n",
    "                    memo[str(i)+', '+all_tags[j]] = [all_tags[k], curr_score]\n",
    "            temp_tag_score[j] = best_score\n",
    "        tag_score_probs = temp_tag_score\n",
    "    \n",
    "    return tag_score_probs, memo\n",
    "                    \n",
    "                        \n",
    "                        \n",
    "each_sentence_cache = []\n",
    "each_sentence_scores = []\n",
    "\n",
    "for sentence in all_sentences_dev:\n",
    "    op = viterbi_decoding_algo(sentence)\n",
    "    each_sentence_cache.append(op[1])\n",
    "    each_sentence_scores.append(op[0])\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "39144e6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sen = all_sentences_dev[0]\n",
    "# for i in range(len(sen)):\n",
    "#     print(sen[i][0])\n",
    "# len(each_sentence_cache[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f88cc3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the dev data for Viterbi Decoding 0.9480914755865231\n"
     ]
    }
   ],
   "source": [
    "# Back propagate to find the best sequence\n",
    "\n",
    "def propagate_bacwards(i):\n",
    "    final_seq = []\n",
    "    final_seq_scores = []\n",
    "    sent_scores = each_sentence_scores[i]\n",
    "    sent_memo = each_sentence_cache[i]\n",
    "    max_tag = all_tags[sent_scores.index(max(sent_scores))]\n",
    "    final_seq.append(max_tag)\n",
    "    \n",
    "    for j in range(len(each_sentence_cache[i])//len(all_tags), 0, -1):\n",
    "        score = sent_memo[str(j)+', '+max_tag][1]\n",
    "        max_tag = sent_memo[str(j)+', '+max_tag][0]\n",
    "        final_seq = [max_tag]+final_seq\n",
    "        final_seq_scores = [score] + final_seq_scores\n",
    "    \n",
    "    return final_seq, final_seq_scores\n",
    "\n",
    "final_seq = []\n",
    "final_seq_scores = []\n",
    "for i in range(len(each_sentence_scores)):\n",
    "    op_dev = propagate_bacwards(i)\n",
    "    final_seq.append(op_dev[0])\n",
    "    final_seq_scores.append(op_dev[1])\n",
    "\n",
    "# Accuracy finder\n",
    "def accuracy_finder_viterbi():\n",
    "    frequency = 0\n",
    "    cur_tag_freq = 0\n",
    "\n",
    "    for i in range(len(all_sentences_dev)):\n",
    "        for j in range(len(all_sentences_dev[i])):\n",
    "            if final_seq[i][j]==all_sentences_dev[i][j][1]:\n",
    "                cur_tag_freq+=1\n",
    "            frequency+=1\n",
    "    return cur_tag_freq/frequency\n",
    "\n",
    "print(\"Accuracy on the dev data for Viterbi Decoding\", accuracy_finder_viterbi())\n",
    "\n",
    "# Testing data Viterbi Algorithm                        \n",
    "each_sentence_cache_test = []\n",
    "each_sentence_scores_test = []\n",
    "\n",
    "for sentence in all_sentences_test:\n",
    "    op_test = viterbi_decoding_algo(sentence)\n",
    "    each_sentence_cache_test.append(op_test[1])\n",
    "    each_sentence_scores_test.append(op_test[0])\n",
    "    \n",
    "def propagate_bacwards_testing(i):\n",
    "    final_seq = []\n",
    "    final_seq_scores = []\n",
    "    sent_scores = each_sentence_scores_test[i]\n",
    "    sent_memo = each_sentence_cache_test[i]\n",
    "    max_tag = all_tags[sent_scores.index(max(sent_scores))]\n",
    "    final_seq.append(max_tag)\n",
    "    \n",
    "    for j in range(len(each_sentence_cache_test[i])//len(all_tags), 0, -1):\n",
    "        score = sent_memo[str(j)+', '+max_tag][1]\n",
    "        max_tag = sent_memo[str(j)+', '+max_tag][0]\n",
    "        final_seq = [max_tag]+final_seq\n",
    "        final_seq_scores = [score] + final_seq_scores\n",
    "    \n",
    "    return final_seq, final_seq_scores\n",
    "\n",
    "final_seq_test = []\n",
    "final_seq_scores_test = []\n",
    "for i in range(len(each_sentence_scores_test)):\n",
    "    op_test = propagate_bacwards_testing(i)\n",
    "    final_seq_test.append(op_test[0])\n",
    "    final_seq_scores_test.append(op_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "905c7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the final output: viterbi.out\n",
    "# Creating the final output format\n",
    "final_output_test = []\n",
    "for i in range(len(all_sentences_test)):\n",
    "    idx = 1\n",
    "    curr_sentence = []\n",
    "    for j in range(len(all_sentences_test[i])):\n",
    "        curr_sentence.append([idx, all_sentences_test[i][j], final_seq_test[i][j]])\n",
    "        idx+=1\n",
    "    final_output_test.append(curr_sentence)\n",
    "    \n",
    "with open(\"viterbi.out\", 'w') as f:\n",
    "    for item in final_output_test:\n",
    "        for element in item:\n",
    "            f.write(\"\\n\".join([str(element[0])+\"\\t\"+element[1]+\"\\t\"+element[2]]))\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
