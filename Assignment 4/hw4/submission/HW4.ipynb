{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd7fe473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "device = torch.device('cpu')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952045bd",
   "metadata": {},
   "source": [
    "# Dataset Loading from Train, Dev and Test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78e1ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_dev_data(path):\n",
    "    train_dev_df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 2:\n",
    "                id, word, ner_tag = line.strip().split(\" \")\n",
    "                train_dev_df.append([id, word, ner_tag])\n",
    "\n",
    "    train_dev_df = pd.DataFrame(train_dev_df, columns=['id', 'word', 'NER'])\n",
    "    train_dev_df = train_dev_df.dropna()\n",
    "    \n",
    "    X_train_dev, Y_train_dev = list(), list()\n",
    "    x, y = list(), list()\n",
    "    first = 1\n",
    "    for row in train_dev_df.itertuples():\n",
    "        if(row.id == '1' and first == 0):\n",
    "            X_train_dev.append(x)\n",
    "            Y_train_dev.append(y)\n",
    "            x = list()\n",
    "            y = list()\n",
    "        first = 0\n",
    "        x.append(row.word)\n",
    "        y.append(row.NER)\n",
    "\n",
    "    X_train_dev.append(x)\n",
    "    Y_train_dev.append(y)\n",
    "    return X_train_dev, Y_train_dev\n",
    "\n",
    "def prepare_test_data(path):\n",
    "    test_df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 1:\n",
    "                id, word = line.strip().split(\" \")\n",
    "                test_df.append([id, word])\n",
    "\n",
    "    test_df = pd.DataFrame(test_df, columns=['id', 'word'])\n",
    "    test_df = test_df.dropna()\n",
    "    X_test = list()\n",
    "    x = list()\n",
    "    first = 1\n",
    "    for row in test_df.itertuples():\n",
    "        if(row.id == '1' and first == 0):\n",
    "            X_test.append(x)\n",
    "            x = list()\n",
    "        first = 0\n",
    "        x.append(row.word)\n",
    "\n",
    "    X_test.append(x)\n",
    "\n",
    "    return X_test\n",
    "\n",
    "\n",
    "X_train, Y_train = prepare_train_dev_data('./data/train')\n",
    "X_dev, Y_dev = prepare_train_dev_data('./data/dev')\n",
    "X_test = prepare_test_data('./data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114bd09",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b88b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sentence Vector Preparation\n",
    "'''\n",
    "def sentence_vector(x_train_data, word_idx):\n",
    "\n",
    "    x_train_vector = list()\n",
    "    temp = list()\n",
    "    for words in x_train_data:\n",
    "        for word in words:\n",
    "            temp.append(word_idx[word])\n",
    "        x_train_vector.append(temp)\n",
    "        temp = list()\n",
    "\n",
    "    return x_train_vector\n",
    "\n",
    "'''\n",
    "Label Vector Preparation\n",
    "'''\n",
    "def label_vector(y_train_data, label_dict):\n",
    "\n",
    "    y_train_vector = list()\n",
    "    for tags in y_train_data:\n",
    "        temp = list()\n",
    "        for label in tags:\n",
    "            temp.append(label_dict[label])\n",
    "        y_train_vector.append(temp)\n",
    "    return y_train_vector\n",
    "\n",
    "'''\n",
    "Word Dictionary Preparation: We prepare word dictionary by setting the word as Key and its index \n",
    "in the corpus as the Value\n",
    "'''\n",
    "word_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "idx = 2\n",
    "\n",
    "for data in [X_train, X_dev, X_test]:\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word not in word_idx:\n",
    "                word_idx[word] = idx\n",
    "                idx += 1\n",
    "\n",
    "'''\n",
    "Sentence Vector Preparation Driver\n",
    "'''\n",
    "train_x_vec = sentence_vector(X_train, word_idx)\n",
    "test_x_vec = sentence_vector(X_test, word_idx)\n",
    "val_x_vec = sentence_vector(X_dev, word_idx)\n",
    "\n",
    "'''\n",
    "Label Dictionary Preparation\n",
    "'''    \n",
    "label1 = set()\n",
    "for tags_list in Y_train:\n",
    "    for tag in tags_list:\n",
    "        label1.add(tag)\n",
    "\n",
    "label2 = set()\n",
    "for tags_list in Y_dev:\n",
    "    for tag in tags_list:\n",
    "        label2.add(tag)\n",
    "\n",
    "label = label1.union(label2)\n",
    "label_tuples = []\n",
    "counter = 0\n",
    "for tags in label:\n",
    "    label_tuples.append((tags, counter))\n",
    "    counter += 1\n",
    "label_dict = dict(label_tuples)\n",
    "\n",
    "'''\n",
    "Label Vector Preparation Driver\n",
    "'''\n",
    "train_y_vec = label_vector(Y_train, label_dict)\n",
    "val_y_vec = label_vector(Y_dev, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "559975ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "InputDataLoader purpose:\n",
    "We use InputDataLoader to convert each and every vector of the train, test and dev vector to tensors\n",
    "'''\n",
    "class InputDataLoader(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_instance = torch.tensor(self.x[index])\n",
    "        y_instance = torch.tensor(self.y[index])\n",
    "        return x_instance, y_instance\n",
    "\n",
    "class InputTestDataLoader(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_instance = torch.tensor(self.x[index])\n",
    "        return x_instance\n",
    "    \n",
    "'''\n",
    "Collate Usage:\n",
    "I have used custom Collate functionality to determine how individual samples are combined into batches during training or \n",
    "testing, since each sample in the dataset may have different sizes or shapes, they cannot be directly combined into batches.\n",
    "Thus, we pad all samples/sentences to a fixed length and combines them into a tensor.\n",
    "'''\n",
    "class CollateData(object):\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        (xx, yy) = zip(*batch)\n",
    "        \n",
    "        batch_max_len = float('-inf')\n",
    "        for s in xx:\n",
    "            batch_max_len = max(batch_max_len, len(s))\n",
    "        x_len = []\n",
    "        y_len = []\n",
    "        for x in xx:\n",
    "            x_len.append(len(x))\n",
    "        for y in yy:\n",
    "            y_len.append(len(y))\n",
    "\n",
    "        batch_data = 0*np.ones((len(xx), batch_max_len))\n",
    "        batch_labels = -1*np.zeros((len(xx), batch_max_len))\n",
    "        for j in range(len(xx)):\n",
    "            batch_data[j][:len(xx[j])] = xx[j]\n",
    "            batch_labels[j][:len(xx[j])] = yy[j]\n",
    "\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        return batch_data, batch_labels, x_len, y_len\n",
    "\n",
    "class CollateTestData(object):\n",
    "    def __call__(self, batch):\n",
    "        xx = batch\n",
    "        batch_max_len = float('-inf')\n",
    "        for s in xx:\n",
    "            batch_max_len = max(batch_max_len, len(s))\n",
    "        x_len = []\n",
    "        for x in xx:\n",
    "            x_len.append(len(x))\n",
    "            \n",
    "        batch_data = 0*np.ones((len(xx), batch_max_len))\n",
    "        batch_labels = -1*np.zeros((len(xx), batch_max_len))\n",
    "        for j in range(len(xx)):\n",
    "            batch_data[j][:len(xx[j])] = xx[j]\n",
    "\n",
    "        batch_data = torch.LongTensor(batch_data)\n",
    "        batch_data = Variable(batch_data)\n",
    "\n",
    "        return batch_data, x_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879065b",
   "metadata": {},
   "source": [
    "# Task 1: Simple Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77cc6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task-1\n",
    "Simple BiLSTM Model for the Task - 1\n",
    "- embedding dim =  100\n",
    "- number of LSTM layers =  1\n",
    "- LSTM hidden dim =  256\n",
    "- LSTM Dropout =  0.33\n",
    "- Linear output dim =  128\n",
    "'''\n",
    "class Custom_network(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,\n",
    "                 dropout_val, tag_size):\n",
    "        super(Custom_network, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear_out_dim = linear_out_dim\n",
    "        self.tag_size = tag_size\n",
    "        self.num_directions = 2\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1,1)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, linear_out_dim)\n",
    "        self.classifier = nn.Linear(linear_out_dim, self.tag_size)\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim).to(device))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, sen, sen_len):\n",
    "        batch_size = sen.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        embedded = self.embedding(sen).float()\n",
    "        packed_embedded = pack_padded_sequence(embedded, sen_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        dropout = self.dropout(output_unpacked)\n",
    "        lin = self.fc(dropout)\n",
    "        pred = self.elu(lin)\n",
    "        pred = self.classifier(pred)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fab221ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom_network(\n",
      "  (embedding): Embedding(30292, 100)\n",
      "  (LSTM): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
      "  (dropout): Dropout(p=0.33, inplace=False)\n",
      "  (elu): ELU(alpha=1.0)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 1.694171\n",
      "Epoch: 2 \tTraining Loss: 1.128844\n",
      "Epoch: 3 \tTraining Loss: 0.837562\n",
      "Epoch: 4 \tTraining Loss: 0.647057\n",
      "Epoch: 5 \tTraining Loss: 0.500861\n",
      "Epoch: 6 \tTraining Loss: 0.392422\n",
      "Epoch: 7 \tTraining Loss: 0.311816\n",
      "Epoch: 8 \tTraining Loss: 0.246336\n",
      "Epoch: 9 \tTraining Loss: 0.202487\n",
      "Epoch: 10 \tTraining Loss: 0.163046\n",
      "Epoch: 11 \tTraining Loss: 0.137303\n",
      "Epoch: 12 \tTraining Loss: 0.108981\n",
      "Epoch: 13 \tTraining Loss: 0.091317\n",
      "Epoch: 14 \tTraining Loss: 0.085904\n",
      "Epoch: 15 \tTraining Loss: 0.072775\n",
      "Epoch: 16 \tTraining Loss: 0.059469\n",
      "Epoch: 17 \tTraining Loss: 0.050414\n",
      "Epoch: 18 \tTraining Loss: 0.044435\n",
      "Epoch: 19 \tTraining Loss: 0.036675\n",
      "Epoch: 20 \tTraining Loss: 0.031847\n",
      "Epoch: 21 \tTraining Loss: 0.029289\n",
      "Epoch: 22 \tTraining Loss: 0.023000\n",
      "Epoch: 23 \tTraining Loss: 0.024689\n",
      "Epoch: 24 \tTraining Loss: 0.030002\n",
      "Epoch: 25 \tTraining Loss: 0.025718\n",
      "Epoch: 26 \tTraining Loss: 0.019117\n",
      "Epoch: 27 \tTraining Loss: 0.020098\n",
      "Epoch: 28 \tTraining Loss: 0.018212\n",
      "Epoch: 29 \tTraining Loss: 0.015627\n",
      "Epoch: 30 \tTraining Loss: 0.016490\n",
      "Epoch: 31 \tTraining Loss: 0.016625\n",
      "Epoch: 32 \tTraining Loss: 0.016015\n",
      "Epoch: 33 \tTraining Loss: 0.013337\n",
      "Epoch: 34 \tTraining Loss: 0.014164\n",
      "Epoch: 35 \tTraining Loss: 0.013406\n",
      "Epoch: 36 \tTraining Loss: 0.010932\n",
      "Epoch: 37 \tTraining Loss: 0.011986\n",
      "Epoch: 38 \tTraining Loss: 0.011295\n",
      "Epoch: 39 \tTraining Loss: 0.009090\n",
      "Epoch: 40 \tTraining Loss: 0.011683\n",
      "Epoch: 41 \tTraining Loss: 0.009098\n",
      "Epoch: 42 \tTraining Loss: 0.007108\n",
      "Epoch: 43 \tTraining Loss: 0.007067\n",
      "Epoch: 44 \tTraining Loss: 0.007389\n",
      "Epoch: 45 \tTraining Loss: 0.008195\n",
      "Epoch: 46 \tTraining Loss: 0.007430\n",
      "Epoch: 47 \tTraining Loss: 0.006797\n",
      "Epoch: 48 \tTraining Loss: 0.005506\n",
      "Epoch: 49 \tTraining Loss: 0.005375\n",
      "Epoch: 50 \tTraining Loss: 0.006805\n",
      "Epoch: 51 \tTraining Loss: 0.007128\n",
      "Epoch: 52 \tTraining Loss: 0.005514\n",
      "Epoch: 53 \tTraining Loss: 0.004970\n",
      "Epoch: 54 \tTraining Loss: 0.005788\n",
      "Epoch: 55 \tTraining Loss: 0.006574\n",
      "Epoch: 56 \tTraining Loss: 0.005195\n",
      "Epoch: 57 \tTraining Loss: 0.006792\n",
      "Epoch: 58 \tTraining Loss: 0.005147\n",
      "Epoch: 59 \tTraining Loss: 0.005659\n",
      "Epoch: 60 \tTraining Loss: 0.006184\n",
      "Epoch: 61 \tTraining Loss: 0.004983\n",
      "Epoch: 62 \tTraining Loss: 0.005880\n",
      "Epoch: 63 \tTraining Loss: 0.003963\n",
      "Epoch: 64 \tTraining Loss: 0.004357\n",
      "Epoch: 65 \tTraining Loss: 0.005843\n",
      "Epoch: 66 \tTraining Loss: 0.005630\n",
      "Epoch: 67 \tTraining Loss: 0.005629\n",
      "Epoch: 68 \tTraining Loss: 0.004751\n",
      "Epoch: 69 \tTraining Loss: 0.004943\n",
      "Epoch: 70 \tTraining Loss: 0.004914\n",
      "Epoch: 71 \tTraining Loss: 0.004254\n",
      "Epoch: 72 \tTraining Loss: 0.003734\n",
      "Epoch: 73 \tTraining Loss: 0.003963\n",
      "Epoch: 74 \tTraining Loss: 0.004154\n",
      "Epoch: 75 \tTraining Loss: 0.002733\n",
      "Epoch: 76 \tTraining Loss: 0.002785\n",
      "Epoch: 77 \tTraining Loss: 0.002560\n",
      "Epoch: 78 \tTraining Loss: 0.002264\n",
      "Epoch: 79 \tTraining Loss: 0.003226\n",
      "Epoch: 80 \tTraining Loss: 0.003520\n",
      "Epoch: 81 \tTraining Loss: 0.003415\n",
      "Epoch: 82 \tTraining Loss: 0.003267\n",
      "Epoch: 83 \tTraining Loss: 0.002527\n",
      "Epoch: 84 \tTraining Loss: 0.002817\n",
      "Epoch: 85 \tTraining Loss: 0.003299\n",
      "Epoch: 86 \tTraining Loss: 0.003151\n",
      "Epoch: 87 \tTraining Loss: 0.002886\n",
      "Epoch: 88 \tTraining Loss: 0.002559\n",
      "Epoch: 89 \tTraining Loss: 0.002859\n",
      "Epoch: 90 \tTraining Loss: 0.001946\n",
      "Epoch: 91 \tTraining Loss: 0.002004\n",
      "Epoch: 92 \tTraining Loss: 0.002884\n",
      "Epoch: 93 \tTraining Loss: 0.002037\n",
      "Epoch: 94 \tTraining Loss: 0.002319\n",
      "Epoch: 95 \tTraining Loss: 0.002205\n",
      "Epoch: 96 \tTraining Loss: 0.002238\n",
      "Epoch: 97 \tTraining Loss: 0.002487\n",
      "Epoch: 98 \tTraining Loss: 0.002767\n",
      "Epoch: 99 \tTraining Loss: 0.002359\n",
      "Epoch: 100 \tTraining Loss: 0.002480\n",
      "Epoch: 101 \tTraining Loss: 0.002634\n",
      "Epoch: 102 \tTraining Loss: 0.002864\n",
      "Epoch: 103 \tTraining Loss: 0.003313\n",
      "Epoch: 104 \tTraining Loss: 0.002637\n",
      "Epoch: 105 \tTraining Loss: 0.002104\n",
      "Epoch: 106 \tTraining Loss: 0.001718\n",
      "Epoch: 107 \tTraining Loss: 0.002286\n",
      "Epoch: 108 \tTraining Loss: 0.001885\n",
      "Epoch: 109 \tTraining Loss: 0.001746\n",
      "Epoch: 110 \tTraining Loss: 0.001497\n",
      "Epoch: 111 \tTraining Loss: 0.002299\n",
      "Epoch: 112 \tTraining Loss: 0.003040\n",
      "Epoch: 113 \tTraining Loss: 0.003166\n",
      "Epoch: 114 \tTraining Loss: 0.002743\n",
      "Epoch: 115 \tTraining Loss: 0.002373\n",
      "Epoch: 116 \tTraining Loss: 0.002282\n",
      "Epoch: 117 \tTraining Loss: 0.002638\n",
      "Epoch: 118 \tTraining Loss: 0.003511\n",
      "Epoch: 119 \tTraining Loss: 0.002601\n",
      "Epoch: 120 \tTraining Loss: 0.002243\n",
      "Epoch: 121 \tTraining Loss: 0.001482\n",
      "Epoch: 122 \tTraining Loss: 0.001673\n",
      "Epoch: 123 \tTraining Loss: 0.001475\n",
      "Epoch: 124 \tTraining Loss: 0.002126\n",
      "Epoch: 125 \tTraining Loss: 0.001062\n",
      "Epoch: 126 \tTraining Loss: 0.001894\n",
      "Epoch: 127 \tTraining Loss: 0.002158\n",
      "Epoch: 128 \tTraining Loss: 0.002443\n",
      "Epoch: 129 \tTraining Loss: 0.002082\n",
      "Epoch: 130 \tTraining Loss: 0.002121\n",
      "Epoch: 131 \tTraining Loss: 0.001700\n",
      "Epoch: 132 \tTraining Loss: 0.002065\n",
      "Epoch: 133 \tTraining Loss: 0.001812\n",
      "Epoch: 134 \tTraining Loss: 0.001821\n",
      "Epoch: 135 \tTraining Loss: 0.002449\n",
      "Epoch: 136 \tTraining Loss: 0.002210\n",
      "Epoch: 137 \tTraining Loss: 0.001348\n",
      "Epoch: 138 \tTraining Loss: 0.001901\n",
      "Epoch: 139 \tTraining Loss: 0.002164\n",
      "Epoch: 140 \tTraining Loss: 0.002417\n",
      "Epoch: 141 \tTraining Loss: 0.002236\n",
      "Epoch: 142 \tTraining Loss: 0.002775\n",
      "Epoch: 143 \tTraining Loss: 0.002487\n",
      "Epoch: 144 \tTraining Loss: 0.003504\n",
      "Epoch: 145 \tTraining Loss: 0.002183\n",
      "Epoch: 146 \tTraining Loss: 0.001767\n",
      "Epoch: 147 \tTraining Loss: 0.002393\n",
      "Epoch: 148 \tTraining Loss: 0.004060\n",
      "Epoch: 149 \tTraining Loss: 0.002707\n",
      "Epoch: 150 \tTraining Loss: 0.001665\n",
      "Epoch: 151 \tTraining Loss: 0.001772\n",
      "Epoch: 152 \tTraining Loss: 0.001485\n",
      "Epoch: 153 \tTraining Loss: 0.001462\n",
      "Epoch: 154 \tTraining Loss: 0.001711\n",
      "Epoch: 155 \tTraining Loss: 0.002414\n",
      "Epoch: 156 \tTraining Loss: 0.001735\n",
      "Epoch: 157 \tTraining Loss: 0.001355\n",
      "Epoch: 158 \tTraining Loss: 0.001709\n",
      "Epoch: 159 \tTraining Loss: 0.001132\n",
      "Epoch: 160 \tTraining Loss: 0.001684\n",
      "Epoch: 161 \tTraining Loss: 0.001169\n",
      "Epoch: 162 \tTraining Loss: 0.000912\n",
      "Epoch: 163 \tTraining Loss: 0.001351\n",
      "Epoch: 164 \tTraining Loss: 0.001260\n",
      "Epoch: 165 \tTraining Loss: 0.001101\n",
      "Epoch: 166 \tTraining Loss: 0.001117\n",
      "Epoch: 167 \tTraining Loss: 0.001700\n",
      "Epoch: 168 \tTraining Loss: 0.002087\n",
      "Epoch: 169 \tTraining Loss: 0.001248\n",
      "Epoch: 170 \tTraining Loss: 0.001361\n",
      "Epoch: 171 \tTraining Loss: 0.001417\n",
      "Epoch: 172 \tTraining Loss: 0.001136\n",
      "Epoch: 173 \tTraining Loss: 0.002112\n",
      "Epoch: 174 \tTraining Loss: 0.001244\n",
      "Epoch: 175 \tTraining Loss: 0.001758\n",
      "Epoch: 176 \tTraining Loss: 0.001703\n",
      "Epoch: 177 \tTraining Loss: 0.001520\n",
      "Epoch: 178 \tTraining Loss: 0.001086\n",
      "Epoch: 179 \tTraining Loss: 0.000934\n",
      "Epoch: 180 \tTraining Loss: 0.000951\n",
      "Epoch: 181 \tTraining Loss: 0.001403\n",
      "Epoch: 182 \tTraining Loss: 0.001421\n",
      "Epoch: 183 \tTraining Loss: 0.001035\n",
      "Epoch: 184 \tTraining Loss: 0.001180\n",
      "Epoch: 185 \tTraining Loss: 0.001633\n",
      "Epoch: 186 \tTraining Loss: 0.002055\n",
      "Epoch: 187 \tTraining Loss: 0.001467\n",
      "Epoch: 188 \tTraining Loss: 0.001077\n",
      "Epoch: 189 \tTraining Loss: 0.000662\n",
      "Epoch: 190 \tTraining Loss: 0.001046\n",
      "Epoch: 191 \tTraining Loss: 0.000713\n",
      "Epoch: 192 \tTraining Loss: 0.000624\n",
      "Epoch: 193 \tTraining Loss: 0.000742\n",
      "Epoch: 194 \tTraining Loss: 0.001088\n",
      "Epoch: 195 \tTraining Loss: 0.000793\n",
      "Epoch: 196 \tTraining Loss: 0.001037\n",
      "Epoch: 197 \tTraining Loss: 0.000966\n",
      "Epoch: 198 \tTraining Loss: 0.001301\n",
      "Epoch: 199 \tTraining Loss: 0.001013\n",
      "Epoch: 200 \tTraining Loss: 0.000964\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "model_custom = Custom_network(vocab_size=len(word_idx),embedding_dim=100,linear_out_dim=128,\n",
    "                      hidden_dim=256, lstm_layers=1,dropout_val=0.33,tag_size=len(label_dict))\n",
    "model_custom.to(device)\n",
    "print(model_custom)\n",
    "\n",
    "train_loader_input = InputDataLoader(train_x_vec, train_y_vec)\n",
    "custom_collator = CollateData()\n",
    "dataloader = DataLoader(dataset=train_loader_input, batch_size=5, drop_last=True, collate_fn=custom_collator)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "criterion = criterion.to(device)\n",
    "criterion.requres_grad = True\n",
    "optimizer = torch.optim.SGD(model_custom.parameters(), lr=0.1, momentum=0.9)\n",
    "epochs = 200 #200\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    train_loss = 0.0\n",
    "    for input, label, input_len, label_len in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_custom(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input.size(1)\n",
    "        \n",
    "    train_loss = train_loss / len(dataloader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(i, train_loss))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for input, label, input_len, label_len in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_custom(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        optimizer.step()\n",
    "    torch.save(model_custom.state_dict(), 'blstm1.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12330602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51409 tokens with 5931 phrases; found: 5584 phrases; correct: 4361.\n",
      "accuracy:  95.11%; precision:  78.10%; recall:  73.53%; FB1:  75.74\n",
      "              LOC: precision:  88.72%; recall:  79.21%; FB1:  83.69  1640\n",
      "             MISC: precision:  81.81%; recall:  74.70%; FB1:  78.09  841\n",
      "              ORG: precision:  66.78%; recall:  71.04%; FB1:  68.85  1418\n",
      "              PER: precision:  75.43%; recall:  69.08%; FB1:  72.11  1685\n"
     ]
    }
   ],
   "source": [
    "# Development Data\n",
    "dev_loader_input = InputDataLoader(val_x_vec, val_y_vec)\n",
    "custom_collator = CollateData()\n",
    "dataloader_dev = DataLoader(dataset=dev_loader_input, batch_size=32, shuffle=False, drop_last=True, collate_fn=custom_collator)\n",
    "\n",
    "label_dict_temp = {}\n",
    "vocab_dict_temp = {}\n",
    "\n",
    "for k, v in label_dict.items():\n",
    "    label_dict_temp[v] = k\n",
    "for k, v in word_idx.items():\n",
    "    vocab_dict_temp[v] = k\n",
    "    \n",
    "for e in range(1,epochs + 1):\n",
    "    model_custom.load_state_dict(torch.load(\"./blstm1.pt\"))#125\n",
    "    model_custom.to(device)\n",
    "\n",
    "    file = open(\"./dev1_temp.out\", 'w')\n",
    "    file_dev_final = open(\"./dev1.out\", 'w')\n",
    "    for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
    "\n",
    "        pred = model_custom(dev_data.to(device), dev_data_len)\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.detach().numpy()\n",
    "        label = label.detach().numpy()\n",
    "        dev_data = dev_data.detach().numpy()\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        pred = pred.reshape((len(label), -1))\n",
    "\n",
    "        for i in range(len(dev_data)):\n",
    "            for j in range(len(dev_data[i])):\n",
    "                if dev_data[i][j] != 0:\n",
    "                                                    # word, gold, op\n",
    "                    file.write(\" \".join([str(j+1), str(vocab_dict_temp[dev_data[i][j]]), \n",
    "                                         str(label_dict_temp[label[i][j]]), str(label_dict_temp[pred[i][j]])]))\n",
    "                    \n",
    "                                                    # word, op\n",
    "                    file_dev_final.write(\" \".join([str(j+1), str(vocab_dict_temp[dev_data[i][j]]), \n",
    "                                                   str(label_dict_temp[pred[i][j]])]))\n",
    "                    file.write(\"\\n\")\n",
    "                    file_dev_final.write(\"\\n\")\n",
    "            file.write(\"\\n\")\n",
    "            file_dev_final.write(\"\\n\")\n",
    "    file.close()\n",
    "    file_dev_final.close()\n",
    "    \n",
    "        \n",
    "with torch.no_grad():\n",
    "    for input, label, input_len, label_len in dataloader_dev:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_custom(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        optimizer.step()\n",
    "        \n",
    "    !perl conll03eval.txt < dev1_temp.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54717ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing on Testing Dataset \"\"\"\n",
    "test_loader_input = InputTestDataLoader(test_x_vec)\n",
    "custom_test_collator = CollateTestData()\n",
    "dataloader_test = DataLoader(dataset=test_loader_input, batch_size=32, shuffle=False, drop_last=True, \n",
    "                             collate_fn=custom_test_collator)\n",
    "\n",
    "label_dict_temp = {}\n",
    "vocab_dict_temp = {}\n",
    "\n",
    "for k, v in label_dict.items():\n",
    "    label_dict_temp[v] = k\n",
    "for k, v in word_idx.items():\n",
    "    vocab_dict_temp[v] = k\n",
    "    \n",
    "for e in range(1,epochs + 1):\n",
    "    model_custom.load_state_dict(torch.load(\"./blstm1.pt\"))#125\n",
    "    model_custom.to(device)\n",
    "    \n",
    "\n",
    "    file = open(\"test1.out\", 'w')\n",
    "    for test_data, test_data_len in dataloader_test:\n",
    "\n",
    "        pred = model_custom(test_data.to(device), test_data_len)\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.detach().numpy()\n",
    "        test_data = test_data.detach().numpy()\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        pred = pred.reshape((len(test_data), -1))\n",
    "        \n",
    "        for i in range(len(test_data)):\n",
    "            for j in range(len(test_data[i])):\n",
    "                if test_data[i][j] != 0:\n",
    "                    word = vocab_dict_temp[test_data[i][j]]\n",
    "                    op = label_dict_temp[pred[i][j]]\n",
    "                    file.write(\" \".join([str(j+1), word, op]))\n",
    "                    file.write(\"\\n\")\n",
    "\n",
    "            file.write(\"\\n\")        \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d580e",
   "metadata": {},
   "source": [
    "# Task 2: Using GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a58744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task-2\n",
    "BiLSTM Model for Task 2: Using GloVe word embeddings\n",
    "- embedding dim =  100\n",
    "- number of LSTM layers =  1\n",
    "- LSTM hidden dim =  256\n",
    "- LSTM Dropout =  0.33\n",
    "- Linear output dim =  128\n",
    "'''\n",
    "class Glove_network(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,dropout_val, tag_size, emb_matrix):\n",
    "        super(Glove_network, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear_out_dim = linear_out_dim\n",
    "        self.tag_size = tag_size\n",
    "        self.emb_matrix = emb_matrix\n",
    "        self.num_directions = 2\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix))\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, linear_out_dim) \n",
    "        self.classifier = nn.Linear(linear_out_dim, self.tag_size)\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim).to(device))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, sen, sen_len):\n",
    "        batch_size = sen.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        embedded = self.embedding(sen).float()\n",
    "        packed_embedded = pack_padded_sequence(embedded, sen_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        dropout = self.dropout(output_unpacked)\n",
    "        lin = self.fc(dropout)\n",
    "        pred = self.elu(lin)\n",
    "        pred = self.classifier(pred)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7a9c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating Embedding Matrix for GloVe\n",
    "'''\n",
    "glove = pd.read_csv('./glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "\n",
    "glove_emb = {}\n",
    "for k, v in glove.T.items():\n",
    "    glove_emb[k] = v.values\n",
    "\n",
    "    \n",
    "glove_emb_list = []\n",
    "for k in glove_emb:\n",
    "    glove_emb_list.append(glove_emb[k])    \n",
    "glove_vec = np.array(glove_emb_list)\n",
    "\n",
    "glove_emb[\"<UNK>\"] = np.mean(glove_vec, axis=0, keepdims=True).reshape(100,)\n",
    "glove_emb[\"<PAD>\"] = np.zeros((100,), dtype=\"float64\")\n",
    "\n",
    "emb_matrix = np.zeros((len(word_idx), 100))\n",
    "for word_key, index_value in word_idx.items():\n",
    "    if word_key not in glove_emb:\n",
    "        if word_key.lower() not in glove_emb:\n",
    "            emb_matrix[index_value] = glove_emb[\"<UNK>\"]\n",
    "        else:\n",
    "            emb_matrix[index_value] = glove_emb[word_key.lower()] + 5e-3\n",
    "    else:\n",
    "        emb_matrix[index_value] = glove_emb[word_key]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20f26464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove_network(\n",
      "  (embedding): Embedding(30292, 100)\n",
      "  (LSTM): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
      "  (dropout): Dropout(p=0.33, inplace=False)\n",
      "  (elu): ELU(alpha=1.0)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.249401\n",
      "Epoch: 2 \tTraining Loss: 0.119304\n",
      "Epoch: 3 \tTraining Loss: 0.092519\n",
      "Epoch: 4 \tTraining Loss: 0.074013\n",
      "Epoch: 5 \tTraining Loss: 0.064666\n",
      "Epoch: 6 \tTraining Loss: 0.058821\n",
      "Epoch: 7 \tTraining Loss: 0.053629\n",
      "Epoch: 8 \tTraining Loss: 0.049872\n",
      "Epoch: 9 \tTraining Loss: 0.045891\n",
      "Epoch: 10 \tTraining Loss: 0.043236\n",
      "Epoch: 11 \tTraining Loss: 0.040523\n",
      "Epoch: 12 \tTraining Loss: 0.037996\n",
      "Epoch: 13 \tTraining Loss: 0.035928\n",
      "Epoch: 14 \tTraining Loss: 0.033622\n",
      "Epoch: 15 \tTraining Loss: 0.031927\n",
      "Epoch: 16 \tTraining Loss: 0.030241\n",
      "Epoch: 17 \tTraining Loss: 0.028691\n",
      "Epoch: 18 \tTraining Loss: 0.027072\n",
      "Epoch: 19 \tTraining Loss: 0.025635\n",
      "Epoch: 20 \tTraining Loss: 0.024497\n",
      "Epoch: 21 \tTraining Loss: 0.023257\n",
      "Epoch: 22 \tTraining Loss: 0.022140\n",
      "Epoch: 23 \tTraining Loss: 0.021073\n",
      "Epoch: 24 \tTraining Loss: 0.019972\n",
      "Epoch: 25 \tTraining Loss: 0.018973\n",
      "Epoch: 26 \tTraining Loss: 0.018063\n",
      "Epoch: 27 \tTraining Loss: 0.017470\n",
      "Epoch: 28 \tTraining Loss: 0.016284\n",
      "Epoch: 29 \tTraining Loss: 0.015778\n",
      "Epoch: 30 \tTraining Loss: 0.014682\n",
      "Epoch: 31 \tTraining Loss: 0.014205\n",
      "Epoch: 32 \tTraining Loss: 0.013599\n",
      "Epoch: 33 \tTraining Loss: 0.012703\n",
      "Epoch: 34 \tTraining Loss: 0.012203\n",
      "Epoch: 35 \tTraining Loss: 0.011372\n",
      "Epoch: 36 \tTraining Loss: 0.011095\n",
      "Epoch: 37 \tTraining Loss: 0.010538\n",
      "Epoch: 38 \tTraining Loss: 0.010008\n",
      "Epoch: 39 \tTraining Loss: 0.009268\n",
      "Epoch: 40 \tTraining Loss: 0.008904\n",
      "Epoch: 41 \tTraining Loss: 0.008557\n",
      "Epoch: 42 \tTraining Loss: 0.007876\n",
      "Epoch: 43 \tTraining Loss: 0.007468\n",
      "Epoch: 44 \tTraining Loss: 0.007185\n",
      "Epoch: 45 \tTraining Loss: 0.006855\n",
      "Epoch: 46 \tTraining Loss: 0.006455\n",
      "Epoch: 47 \tTraining Loss: 0.006263\n",
      "Epoch: 48 \tTraining Loss: 0.006062\n",
      "Epoch: 49 \tTraining Loss: 0.005796\n",
      "Epoch: 50 \tTraining Loss: 0.005540\n",
      "Epoch: 51 \tTraining Loss: 0.005349\n",
      "Epoch: 52 \tTraining Loss: 0.004854\n",
      "Epoch: 53 \tTraining Loss: 0.004683\n",
      "Epoch: 54 \tTraining Loss: 0.004124\n",
      "Epoch: 55 \tTraining Loss: 0.004220\n",
      "Epoch: 56 \tTraining Loss: 0.003973\n",
      "Epoch: 57 \tTraining Loss: 0.003521\n",
      "Epoch: 58 \tTraining Loss: 0.003480\n",
      "Epoch: 59 \tTraining Loss: 0.003295\n",
      "Epoch: 60 \tTraining Loss: 0.003103\n",
      "Epoch: 61 \tTraining Loss: 0.003097\n",
      "Epoch: 62 \tTraining Loss: 0.002947\n",
      "Epoch: 63 \tTraining Loss: 0.002682\n",
      "Epoch: 64 \tTraining Loss: 0.002668\n",
      "Epoch: 65 \tTraining Loss: 0.002477\n",
      "Epoch: 66 \tTraining Loss: 0.002394\n",
      "Epoch: 67 \tTraining Loss: 0.002162\n",
      "Epoch: 68 \tTraining Loss: 0.002135\n",
      "Epoch: 69 \tTraining Loss: 0.001919\n",
      "Epoch: 70 \tTraining Loss: 0.001817\n",
      "Epoch: 71 \tTraining Loss: 0.001856\n",
      "Epoch: 72 \tTraining Loss: 0.001809\n",
      "Epoch: 73 \tTraining Loss: 0.001649\n",
      "Epoch: 74 \tTraining Loss: 0.001670\n",
      "Epoch: 75 \tTraining Loss: 0.001617\n",
      "Epoch: 76 \tTraining Loss: 0.001368\n",
      "Epoch: 77 \tTraining Loss: 0.001418\n",
      "Epoch: 78 \tTraining Loss: 0.001458\n",
      "Epoch: 79 \tTraining Loss: 0.001449\n",
      "Epoch: 80 \tTraining Loss: 0.001177\n",
      "Epoch: 81 \tTraining Loss: 0.001247\n",
      "Epoch: 82 \tTraining Loss: 0.001193\n",
      "Epoch: 83 \tTraining Loss: 0.001070\n",
      "Epoch: 84 \tTraining Loss: 0.001118\n",
      "Epoch: 85 \tTraining Loss: 0.001054\n",
      "Epoch: 86 \tTraining Loss: 0.001038\n",
      "Epoch: 87 \tTraining Loss: 0.000920\n",
      "Epoch: 88 \tTraining Loss: 0.000933\n",
      "Epoch: 89 \tTraining Loss: 0.000862\n",
      "Epoch: 90 \tTraining Loss: 0.000847\n",
      "Epoch: 91 \tTraining Loss: 0.000878\n",
      "Epoch: 92 \tTraining Loss: 0.000837\n",
      "Epoch: 93 \tTraining Loss: 0.000820\n",
      "Epoch: 94 \tTraining Loss: 0.000738\n",
      "Epoch: 95 \tTraining Loss: 0.000754\n",
      "Epoch: 96 \tTraining Loss: 0.000711\n",
      "Epoch: 97 \tTraining Loss: 0.000769\n",
      "Epoch: 98 \tTraining Loss: 0.000694\n",
      "Epoch: 99 \tTraining Loss: 0.000715\n",
      "Epoch: 100 \tTraining Loss: 0.000685\n",
      "Epoch: 101 \tTraining Loss: 0.000678\n",
      "Epoch: 102 \tTraining Loss: 0.000563\n",
      "Epoch: 103 \tTraining Loss: 0.000483\n",
      "Epoch: 104 \tTraining Loss: 0.000608\n",
      "Epoch: 105 \tTraining Loss: 0.000536\n",
      "Epoch: 106 \tTraining Loss: 0.000567\n",
      "Epoch: 107 \tTraining Loss: 0.000764\n",
      "Epoch: 108 \tTraining Loss: 0.000588\n",
      "Epoch: 109 \tTraining Loss: 0.000569\n",
      "Epoch: 110 \tTraining Loss: 0.000539\n",
      "Epoch: 111 \tTraining Loss: 0.000541\n",
      "Epoch: 112 \tTraining Loss: 0.000543\n",
      "Epoch: 113 \tTraining Loss: 0.000467\n",
      "Epoch: 114 \tTraining Loss: 0.000456\n",
      "Epoch: 115 \tTraining Loss: 0.000435\n",
      "Epoch: 116 \tTraining Loss: 0.000481\n",
      "Epoch: 117 \tTraining Loss: 0.000498\n",
      "Epoch: 118 \tTraining Loss: 0.000454\n",
      "Epoch: 119 \tTraining Loss: 0.000434\n",
      "Epoch: 120 \tTraining Loss: 0.000431\n",
      "Epoch: 121 \tTraining Loss: 0.000385\n",
      "Epoch: 122 \tTraining Loss: 0.000433\n",
      "Epoch: 123 \tTraining Loss: 0.000419\n",
      "Epoch: 124 \tTraining Loss: 0.000414\n",
      "Epoch: 125 \tTraining Loss: 0.000366\n",
      "Epoch: 126 \tTraining Loss: 0.000362\n",
      "Epoch: 127 \tTraining Loss: 0.000355\n",
      "Epoch: 128 \tTraining Loss: 0.000359\n",
      "Epoch: 129 \tTraining Loss: 0.000356\n",
      "Epoch: 130 \tTraining Loss: 0.000350\n",
      "Epoch: 131 \tTraining Loss: 0.000298\n",
      "Epoch: 132 \tTraining Loss: 0.000342\n",
      "Epoch: 133 \tTraining Loss: 0.000270\n",
      "Epoch: 134 \tTraining Loss: 0.000333\n",
      "Epoch: 135 \tTraining Loss: 0.000287\n",
      "Epoch: 136 \tTraining Loss: 0.000303\n",
      "Epoch: 137 \tTraining Loss: 0.000282\n",
      "Epoch: 138 \tTraining Loss: 0.000283\n",
      "Epoch: 139 \tTraining Loss: 0.000266\n",
      "Epoch: 140 \tTraining Loss: 0.000281\n",
      "Epoch: 141 \tTraining Loss: 0.000341\n",
      "Epoch: 142 \tTraining Loss: 0.000278\n",
      "Epoch: 143 \tTraining Loss: 0.000371\n",
      "Epoch: 144 \tTraining Loss: 0.000334\n",
      "Epoch: 145 \tTraining Loss: 0.000298\n",
      "Epoch: 146 \tTraining Loss: 0.000291\n",
      "Epoch: 147 \tTraining Loss: 0.000263\n",
      "Epoch: 148 \tTraining Loss: 0.000250\n",
      "Epoch: 149 \tTraining Loss: 0.000255\n",
      "Epoch: 150 \tTraining Loss: 0.000236\n",
      "Epoch: 151 \tTraining Loss: 0.000280\n",
      "Epoch: 152 \tTraining Loss: 0.000263\n",
      "Epoch: 153 \tTraining Loss: 0.000234\n",
      "Epoch: 154 \tTraining Loss: 0.000247\n",
      "Epoch: 155 \tTraining Loss: 0.000241\n",
      "Epoch: 156 \tTraining Loss: 0.000259\n",
      "Epoch: 157 \tTraining Loss: 0.000198\n",
      "Epoch: 158 \tTraining Loss: 0.000270\n",
      "Epoch: 159 \tTraining Loss: 0.000291\n",
      "Epoch: 160 \tTraining Loss: 0.000180\n",
      "Epoch: 161 \tTraining Loss: 0.000225\n",
      "Epoch: 162 \tTraining Loss: 0.000265\n",
      "Epoch: 163 \tTraining Loss: 0.000238\n",
      "Epoch: 164 \tTraining Loss: 0.000209\n",
      "Epoch: 165 \tTraining Loss: 0.000214\n",
      "Epoch: 166 \tTraining Loss: 0.000224\n",
      "Epoch: 167 \tTraining Loss: 0.000213\n",
      "Epoch: 168 \tTraining Loss: 0.000254\n",
      "Epoch: 169 \tTraining Loss: 0.000208\n",
      "Epoch: 170 \tTraining Loss: 0.000329\n",
      "Epoch: 171 \tTraining Loss: 0.000248\n",
      "Epoch: 172 \tTraining Loss: 0.000216\n",
      "Epoch: 173 \tTraining Loss: 0.000222\n",
      "Epoch: 174 \tTraining Loss: 0.000199\n",
      "Epoch: 175 \tTraining Loss: 0.000179\n",
      "Epoch: 176 \tTraining Loss: 0.000225\n",
      "Epoch: 177 \tTraining Loss: 0.000222\n",
      "Epoch: 178 \tTraining Loss: 0.000219\n",
      "Epoch: 179 \tTraining Loss: 0.000168\n",
      "Epoch: 180 \tTraining Loss: 0.000172\n",
      "Epoch: 181 \tTraining Loss: 0.000183\n",
      "Epoch: 182 \tTraining Loss: 0.000143\n",
      "Epoch: 183 \tTraining Loss: 0.000177\n",
      "Epoch: 184 \tTraining Loss: 0.000160\n",
      "Epoch: 185 \tTraining Loss: 0.000217\n",
      "Epoch: 186 \tTraining Loss: 0.000173\n",
      "Epoch: 187 \tTraining Loss: 0.000175\n",
      "Epoch: 188 \tTraining Loss: 0.000213\n",
      "Epoch: 189 \tTraining Loss: 0.000200\n",
      "Epoch: 190 \tTraining Loss: 0.000189\n",
      "Epoch: 191 \tTraining Loss: 0.000142\n",
      "Epoch: 192 \tTraining Loss: 0.000169\n",
      "Epoch: 193 \tTraining Loss: 0.000188\n",
      "Epoch: 194 \tTraining Loss: 0.000200\n",
      "Epoch: 195 \tTraining Loss: 0.000171\n",
      "Epoch: 196 \tTraining Loss: 0.000177\n",
      "Epoch: 197 \tTraining Loss: 0.000141\n",
      "Epoch: 198 \tTraining Loss: 0.000129\n",
      "Epoch: 199 \tTraining Loss: 0.000119\n",
      "Epoch: 200 \tTraining Loss: 0.000146\n",
      "Epoch: 201 \tTraining Loss: 0.000149\n",
      "Epoch: 202 \tTraining Loss: 0.000146\n",
      "Epoch: 203 \tTraining Loss: 0.000162\n",
      "Epoch: 204 \tTraining Loss: 0.000145\n",
      "Epoch: 205 \tTraining Loss: 0.000146\n",
      "Epoch: 206 \tTraining Loss: 0.000182\n",
      "Epoch: 207 \tTraining Loss: 0.000137\n",
      "Epoch: 208 \tTraining Loss: 0.000159\n",
      "Epoch: 209 \tTraining Loss: 0.000141\n",
      "Epoch: 210 \tTraining Loss: 0.000130\n",
      "Epoch: 211 \tTraining Loss: 0.000131\n",
      "Epoch: 212 \tTraining Loss: 0.000125\n",
      "Epoch: 213 \tTraining Loss: 0.000138\n",
      "Epoch: 214 \tTraining Loss: 0.000131\n",
      "Epoch: 215 \tTraining Loss: 0.000123\n",
      "Epoch: 216 \tTraining Loss: 0.000128\n",
      "Epoch: 217 \tTraining Loss: 0.000146\n",
      "Epoch: 218 \tTraining Loss: 0.000147\n",
      "Epoch: 219 \tTraining Loss: 0.000138\n",
      "Epoch: 220 \tTraining Loss: 0.000159\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "model_glove = Glove_network(vocab_size=len(word_idx),\n",
    "                      embedding_dim=100,\n",
    "                      linear_out_dim=128,\n",
    "                      hidden_dim=256,\n",
    "                      lstm_layers=1,\n",
    "                      dropout_val=0.33,\n",
    "                      tag_size=len(label_dict),\n",
    "                      emb_matrix=emb_matrix)\n",
    "model_glove.to(device)\n",
    "print(model_glove)\n",
    "\n",
    "train_loader_input_glove = InputDataLoader(train_x_vec, train_y_vec)\n",
    "custom_collator = CollateData()\n",
    "dataloader = DataLoader(dataset=train_loader_input_glove, batch_size=32, drop_last=True, collate_fn=custom_collator)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "criterion = criterion.to(device)\n",
    "criterion.requres_grad = True\n",
    "optimizer = torch.optim.SGD(model_glove.parameters(), lr=0.1, momentum=0.9)\n",
    "epochs = 220 #220\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    train_loss = 0.0\n",
    "    for input, label, input_len, label_len in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_glove(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input.size(1)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(i, train_loss))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for input, label, input_len, label_len in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_glove(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        optimizer.step()\n",
    "    torch.save(model_glove.state_dict(), 'blstm2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da8dd61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51409 tokens with 5931 phrases; found: 6026 phrases; correct: 5327.\n",
      "accuracy:  97.99%; precision:  88.40%; recall:  89.82%; FB1:  89.10\n",
      "              LOC: precision:  92.35%; recall:  93.96%; FB1:  93.15  1869\n",
      "             MISC: precision:  81.59%; recall:  81.32%; FB1:  81.46  918\n",
      "              ORG: precision:  82.97%; recall:  84.77%; FB1:  83.86  1362\n",
      "              PER: precision:  91.74%; recall:  93.59%; FB1:  92.66  1877\n"
     ]
    }
   ],
   "source": [
    "#predicting for Development dataset\n",
    "dev_loader_input = InputDataLoader(val_x_vec, val_y_vec)\n",
    "custom_collator = CollateData()\n",
    "dataloader_dev = DataLoader(dataset=dev_loader_input, batch_size=32, shuffle=False, drop_last=True, collate_fn=custom_collator)\n",
    "\n",
    "model_glove.load_state_dict(torch.load(\"./blstm2.pt\"))\n",
    "\n",
    "label_dict_temp = {}\n",
    "vocab_dict_temp = {}\n",
    "\n",
    "for k, v in label_dict.items():\n",
    "    label_dict_temp[v] = k\n",
    "for k, v in word_idx.items():\n",
    "    vocab_dict_temp[v] = k\n",
    "    \n",
    "for e in range(1, epochs+1):\n",
    "    model_glove = Glove_network(vocab_size=len(word_idx), embedding_dim=100,linear_out_dim=128,hidden_dim=256,\n",
    "                        lstm_layers=1,dropout_val=0.33,tag_size=len(label_dict),emb_matrix = emb_matrix)\n",
    "\n",
    "    model_glove.load_state_dict(torch.load(\"./blstm2.pt\"))\n",
    "    model_glove.to(device)\n",
    "    \n",
    "    file = open(\"dev2_temp.out\", 'w')\n",
    "    file_dev2_final = open(\"dev2.out\", 'w')\n",
    "    for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
    "\n",
    "        pred = model_glove(dev_data.to(device), dev_data_len)\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.detach().numpy()\n",
    "        label = label.detach().numpy()\n",
    "        dev_data = dev_data.detach().numpy()\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        pred = pred.reshape((len(label), -1))\n",
    "\n",
    "        for i in range(len(dev_data)):\n",
    "            for j in range(len(dev_data[i])):\n",
    "                if dev_data[i][j] != 0:\n",
    "                        # word, gold, op\n",
    "                    file.write(\" \".join([str(j+1), vocab_dict_temp[dev_data[i][j]], \n",
    "                                         label_dict_temp[label[i][j]], label_dict_temp[pred[i][j]]]))\n",
    "                    \n",
    "                    file_dev2_final.write(\" \".join([str(j+1), vocab_dict_temp[dev_data[i][j]], \n",
    "                                         label_dict_temp[pred[i][j]]]))\n",
    "                    file.write(\"\\n\")\n",
    "                    file_dev2_final.write(\"\\n\")\n",
    "            file.write(\"\\n\")\n",
    "            file_dev2_final.write(\"\\n\")\n",
    "    file.close()\n",
    "    file_dev2_final.close()\n",
    "\n",
    "     \n",
    "with torch.no_grad():\n",
    "    for input, label, input_len, label_len in dataloader_dev:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_glove(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        optimizer.step()\n",
    "        \n",
    "    !perl conll03eval.txt < dev2_temp.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a02fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Dataset\n",
    "test_loader_input = InputTestDataLoader(test_x_vec)\n",
    "custom_test_collator = CollateTestData()\n",
    "dataloader_test = DataLoader(dataset=test_loader_input,batch_size=32, shuffle=False, drop_last=True,\n",
    "                             collate_fn=custom_test_collator)\n",
    "label_dict_temp = {}\n",
    "vocab_dict_temp = {}\n",
    "\n",
    "for k, v in label_dict.items():\n",
    "    label_dict_temp[v] = k\n",
    "for k, v in word_idx.items():\n",
    "    vocab_dict_temp[v] = k\n",
    "    \n",
    "for e in range(1,epochs + 1):\n",
    "    model_glove.load_state_dict(torch.load(\"./blstm2.pt\"))\n",
    "    model_glove.to(device)\n",
    "\n",
    "    file = open(\"test2.out\", 'w')\n",
    "    for test_data, test_data_len in dataloader_test:\n",
    "\n",
    "        pred = model_glove(test_data.to(device), test_data_len)\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.detach().numpy()\n",
    "        test_data = test_data.detach().numpy()\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        pred = pred.reshape((len(test_data), -1))\n",
    "        \n",
    "        for i in range(len(test_data)):\n",
    "            for j in range(len(test_data[i])):\n",
    "                if test_data[i][j] != 0:\n",
    "                    word = vocab_dict_temp[test_data[i][j]]\n",
    "                    op = label_dict_temp[pred[i][j]]\n",
    "                    file.write(\" \".join([str(j+1), word, op]))\n",
    "                    file.write(\"\\n\")\n",
    "\n",
    "            file.write(\"\\n\")        \n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
