{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7fe473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "device = torch.device('cpu')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952045bd",
   "metadata": {},
   "source": [
    "# Dataset Loading from Train, Dev and Test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e1ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_dev_data(path):\n",
    "    train_dev_df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 2:\n",
    "                id, word, ner_tag = line.strip().split(\" \")\n",
    "                train_dev_df.append([id, word, ner_tag])\n",
    "\n",
    "    train_dev_df = pd.DataFrame(train_dev_df, columns=['id', 'word', 'NER'])\n",
    "    train_dev_df = train_dev_df.dropna()\n",
    "    \n",
    "    X_train_dev, Y_train_dev = list(), list()\n",
    "    x, y = list(), list()\n",
    "    first = 1\n",
    "    for row in train_dev_df.itertuples():\n",
    "        if(row.id == '1' and first == 0):\n",
    "            X_train_dev.append(x)\n",
    "            Y_train_dev.append(y)\n",
    "            x = list()\n",
    "            y = list()\n",
    "        first = 0\n",
    "        x.append(row.word)\n",
    "        y.append(row.NER)\n",
    "\n",
    "    X_train_dev.append(x)\n",
    "    Y_train_dev.append(y)\n",
    "    return X_train_dev, Y_train_dev\n",
    "\n",
    "def prepare_test_data(path):\n",
    "    test_df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 1:\n",
    "                id, word = line.strip().split(\" \")\n",
    "                test_df.append([id, word])\n",
    "\n",
    "    test_df = pd.DataFrame(test_df, columns=['id', 'word'])\n",
    "    test_df = test_df.dropna()\n",
    "    X_test = list()\n",
    "    x = list()\n",
    "    first = 1\n",
    "    for row in test_df.itertuples():\n",
    "        if(row.id == '1' and first == 0):\n",
    "            X_test.append(x)\n",
    "            x = list()\n",
    "        first = 0\n",
    "        x.append(row.word)\n",
    "\n",
    "    X_test.append(x)\n",
    "\n",
    "    return X_test\n",
    "\n",
    "\n",
    "X_train, Y_train = prepare_train_dev_data('./data/train')\n",
    "X_dev, Y_dev = prepare_train_dev_data('./data/dev')\n",
    "X_test = prepare_test_data('./data/test')\n",
    "\n",
    "# print(len(X_train), len(Y_train))\n",
    "# print(len(X_dev), len(Y_dev))\n",
    "# print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b88b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sentence Vector Preparation\n",
    "'''\n",
    "def sentence_vector(x_train_data, word_idx):\n",
    "\n",
    "    x_train_vector = list()\n",
    "    temp = list()\n",
    "    for words in x_train_data:\n",
    "        for word in words:\n",
    "            temp.append(word_idx[word])\n",
    "        x_train_vector.append(temp)\n",
    "        temp = list()\n",
    "\n",
    "    return x_train_vector\n",
    "\n",
    "'''\n",
    "Label Vector Preparation\n",
    "'''\n",
    "def label_vector(y_train_data, label_dict):\n",
    "\n",
    "    y_train_vector = list()\n",
    "    for tags in y_train_data:\n",
    "        temp = list()\n",
    "        for label in tags:\n",
    "            temp.append(label_dict[label])\n",
    "        y_train_vector.append(temp)\n",
    "    return y_train_vector\n",
    "\n",
    "'''\n",
    "Word Dictionary Preparation: We prepare word dictionary by setting the word as Key and its index \n",
    "in the corpus as the Value\n",
    "'''\n",
    "word_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "idx = 2\n",
    "\n",
    "for data in [X_train, X_dev, X_test]:\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word not in word_idx:\n",
    "                word_idx[word] = idx\n",
    "                idx += 1\n",
    "\n",
    "'''\n",
    "Sentence Vector Preparation Driver\n",
    "'''\n",
    "train_x_vec = sentence_vector(X_train, word_idx)\n",
    "test_x_vec = sentence_vector(X_test, word_idx)\n",
    "val_x_vec = sentence_vector(X_dev, word_idx)\n",
    "\n",
    "'''\n",
    "Label Dictionary Preparation\n",
    "'''    \n",
    "label1 = set()\n",
    "for tags_list in Y_train:\n",
    "    for tag in tags_list:\n",
    "        label1.add(tag)\n",
    "\n",
    "label2 = set()\n",
    "for tags_list in Y_dev:\n",
    "    for tag in tags_list:\n",
    "        label2.add(tag)\n",
    "\n",
    "label = label1.union(label2)\n",
    "label_tuples = []\n",
    "counter = 0\n",
    "for tags in label:\n",
    "    label_tuples.append((tags, counter))\n",
    "    counter += 1\n",
    "label_dict = dict(label_tuples)\n",
    "\n",
    "'''\n",
    "Label Vector Preparation Driver\n",
    "'''\n",
    "train_y_vec = label_vector(Y_train, label_dict)\n",
    "val_y_vec = label_vector(Y_dev, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "559975ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "InputDataLoader purpose:\n",
    "We use InputDataLoader to convert each and every vector of the train, test and dev vector to tensors\n",
    "'''\n",
    "class BiLSTM_DataLoader(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_instance = torch.tensor(self.x[index])  # , dtype=torch.long\n",
    "        y_instance = torch.tensor(self.y[index])  # , dtype=torch.float\n",
    "        return x_instance, y_instance\n",
    "\n",
    "class BiLSTM_TestLoader(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_instance = torch.tensor(self.x[index])  # , dtype=torch.long\n",
    "        # y_instance = torch.tensor(self.y[index])  # , dtype=torch.float\n",
    "        return x_instance\n",
    "    \n",
    "'''\n",
    "Collate Usage:\n",
    "I have used custom Collate functionality to determine how individual samples are combined into batches during training or \n",
    "testing, since each sample in the dataset may have different sizes or shapes, they cannot be directly combined into batches.\n",
    "Thus, we pad all samples/sentences to a fixed length and combines them into a tensor.\n",
    "'''\n",
    "class CollateData(object):\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        (xx, yy) = zip(*batch)\n",
    "        \n",
    "        batch_max_len = float('-inf')\n",
    "        for s in xx:\n",
    "            batch_max_len = max(batch_max_len, len(s))\n",
    "        x_len = []\n",
    "        y_len = []\n",
    "        for x in xx:\n",
    "            x_len.append(len(x))\n",
    "        for y in yy:\n",
    "            y_len.append(len(y))\n",
    "\n",
    "        batch_data = 0*np.ones((len(xx), batch_max_len))\n",
    "        batch_labels = -1*np.zeros((len(xx), batch_max_len))\n",
    "        for j in range(len(xx)):\n",
    "            batch_data[j][:len(xx[j])] = xx[j]\n",
    "            batch_labels[j][:len(xx[j])] = yy[j]\n",
    "\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        return batch_data, batch_labels, x_len, y_len\n",
    "\n",
    "class CollateTestData(object):\n",
    "    def __call__(self, batch):\n",
    "        xx = batch\n",
    "        batch_max_len = float('-inf')\n",
    "        for s in xx:\n",
    "            batch_max_len = max(batch_max_len, len(s))\n",
    "        x_len = []\n",
    "        for x in xx:\n",
    "            x_len.append(len(x))\n",
    "            \n",
    "        batch_data = 0*np.ones((len(xx), batch_max_len))\n",
    "        batch_labels = -1*np.zeros((len(xx), batch_max_len))\n",
    "        for j in range(len(xx)):\n",
    "            batch_data[j][:len(xx[j])] = xx[j]\n",
    "\n",
    "        batch_data = torch.LongTensor(batch_data)\n",
    "        batch_data = Variable(batch_data)\n",
    "\n",
    "        return batch_data, x_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ba8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_class_weights(label_dict, train_y, val_y):\n",
    "#     class_weights = dict()\n",
    "#     for key in label_dict:\n",
    "#         class_weights[key] = 0\n",
    "#     total_nm_tags = 0\n",
    "#     for data in [train_y, val_y]:\n",
    "#         for tags in data:\n",
    "#             for tag in tags:\n",
    "#                 total_nm_tags += 1\n",
    "#                 class_weights[tag] += 1\n",
    "\n",
    "#     class_wt = list()\n",
    "#     for key in class_weights.keys():\n",
    "#         if class_weights[key]:\n",
    "#             score = round(math.log(0.35*total_nm_tags / class_weights[key]), 2)\n",
    "#             class_weights[key] = score if score > 1.0 else 1.0\n",
    "#         else:\n",
    "#             class_weights[key] = 1.0\n",
    "#         class_wt.append(class_weights[key])\n",
    "#     class_wt = torch.tensor(class_wt)\n",
    "#     return class_wt\n",
    "\n",
    "\n",
    "# class_wt = initialize_class_weights(label_dict, Y_train, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77cc6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task-1\n",
    "Simple BiLSTM Model for the Task - 1\n",
    "'''\n",
    "class Custom_network(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,\n",
    "                 dropout_val, tag_size):\n",
    "        super(Custom_network, self).__init__()\n",
    "        self.embedding_dim = embedding_dim  # Embedding Dimension = 100\n",
    "        self.lstm_layers = lstm_layers  # LSTM Layers = 1\n",
    "        self.hidden_dim = hidden_dim  # hidden_dim = 256\n",
    "        self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128\n",
    "        self.tag_size = tag_size  # Tag Size = 9\n",
    "        self.num_directions = 2\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1,1)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, linear_out_dim)\n",
    "        self.classifier = nn.Linear(linear_out_dim, self.tag_size)\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.relu = nn.ELU()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim).to(device))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, sen, sen_len):  # sen_len\n",
    "        # Set initial states\n",
    "        batch_size = sen.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        embedded = self.embedding(sen).float()\n",
    "        packed_embedded = pack_padded_sequence(embedded, sen_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        dropout = self.dropout(output_unpacked)\n",
    "        lin = self.fc(dropout)\n",
    "        pred = self.relu(lin)\n",
    "        pred = self.classifier(pred)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab221ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom_network(\n",
      "  (embedding): Embedding(30292, 100)\n",
      "  (LSTM): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
      "  (dropout): Dropout(p=0.33, inplace=False)\n",
      "  (relu): ELU(alpha=1.0)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.366335\n",
      "Epoch: 2 \tTraining Loss: 0.274573\n",
      "Epoch: 3 \tTraining Loss: 0.247937\n",
      "Epoch: 4 \tTraining Loss: 0.226641\n",
      "Epoch: 5 \tTraining Loss: 0.207683\n",
      "Epoch: 6 \tTraining Loss: 0.190008\n",
      "Epoch: 7 \tTraining Loss: 0.174929\n",
      "Epoch: 8 \tTraining Loss: 0.161943\n",
      "Epoch: 9 \tTraining Loss: 0.149855\n"
     ]
    }
   ],
   "source": [
    "model_custom = Custom_network(vocab_size=len(word_idx),\n",
    "                      embedding_dim=100,\n",
    "                      linear_out_dim=128,\n",
    "                      hidden_dim=256,\n",
    "                      lstm_layers=1,\n",
    "                      dropout_val=0.33,\n",
    "                      tag_size=len(label_dict))\n",
    "model_custom.to(device)\n",
    "print(model_custom)\n",
    "\n",
    "train_loader_input = BiLSTM_DataLoader(train_x_vec, train_y_vec)\n",
    "# custom_collator = CollateData(word_idx, label_dict)\n",
    "custom_collator = CollateData()\n",
    "dataloader = DataLoader(dataset=train_loader_input, batch_size=32, drop_last=True, collate_fn=custom_collator)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_wt)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "criterion = criterion.to(device)\n",
    "criterion.requres_grad = True\n",
    "# optimizer = torch.optim.Adam(model_custom.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.SGD(model_custom.parameters(), lr=0.1, momentum=0.9)\n",
    "epochs = 220 #200\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    train_loss = 0.0\n",
    "    for input, label, input_len, label_len in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_custom(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input.size(1)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(i, train_loss))\n",
    "#     torch.save(model_custom.state_dict(),\n",
    "#                'BiLSTM_b1_epoch_' + str(i) + '.pt')\n",
    "\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for input, label, input_len, label_len in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_custom(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        optimizer.step()\n",
    "    torch.save(model_custom.state_dict(), 'blstm1.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da6381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f5c0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader_input = BiLSTM_DataLoader(val_x_vec, val_y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c87209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(val_x_vec)):\n",
    "#     val_x_vec[i] = torch.tensor(val_x_vec[i])\n",
    "# for i in range(len(val_y_vec)):\n",
    "#     val_y_vec[i] = torch.tensor(val_y_vec[i])\n",
    "\n",
    "li_x = []\n",
    "li_y = []\n",
    "for x, y in zip(val_x_vec, val_y_vec):\n",
    "    li_x.append(x)\n",
    "    li_y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3595754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a00dddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# abc = TensorDataset(torch.LongTensor(val_x_vec), torch.LongTensor(val_y_vec))\n",
    "abc[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12330602",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader_input = BiLSTM_DataLoader(val_x_vec, val_y_vec)\n",
    "custom_collator = CollateData()\n",
    "dataloader_dev = DataLoader(dataset=dev_loader_input, batch_size=32, shuffle=False, drop_last=True, collate_fn=custom_collator)\n",
    "\n",
    "label_dict_temp = {}\n",
    "vocab_dict_temp = {}\n",
    "\n",
    "for k, v in label_dict.items():\n",
    "    label_dict_temp[v] = k\n",
    "for k, v in word_idx.items():\n",
    "    vocab_dict_temp[v] = k\n",
    "    \n",
    "for e in range(1,epochs + 1):\n",
    "#     model_custom.load_state_dict(torch.load(\"./BiLSTM_b1_epoch_\"+str(e)+\".pt\"))#125\n",
    "    model_custom.load_state_dict(torch.load(\"./blstm1.pt\"))#125\n",
    "    model_custom.to(device)\n",
    "\n",
    "    \n",
    "#     print(label_dict)\n",
    "#     rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "#     rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "    file = open(\"./dev1.out\", 'w')\n",
    "    for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
    "\n",
    "        pred = model_custom(dev_data.to(device), dev_data_len)\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.detach().numpy()\n",
    "        label = label.detach().numpy()\n",
    "        dev_data = dev_data.detach().numpy()\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        pred = pred.reshape((len(label), -1))\n",
    "\n",
    "        for i in range(len(dev_data)):\n",
    "            for j in range(len(dev_data[i])):\n",
    "                if dev_data[i][j] != 0:\n",
    "                    word = vocab_dict_temp[dev_data[i][j]]\n",
    "                    gold = label_dict_temp[label[i][j]]\n",
    "                    op = label_dict_temp[pred[i][j]]\n",
    "                    file.write(\" \".join([str(j+1), str(word), str(gold), str(op)]))\n",
    "                    file.write(\"\\n\")\n",
    "            file.write(\"\\n\")\n",
    "    file.close()\n",
    "#     !perl conll03eval.txt < dev1.out\n",
    "    \n",
    "        \n",
    "with torch.no_grad():\n",
    "    for input, label, input_len, label_len in dataloader_dev:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_custom(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        optimizer.step()\n",
    "        \n",
    "    !perl conll03eval.txt < dev1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54717ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing on Testing Dataset \"\"\"\n",
    "test_loader_input = BiLSTM_TestLoader(test_x_vec)\n",
    "custom_test_collator = CollateTestData()\n",
    "dataloader_test = DataLoader(dataset=test_loader_input, batch_size=32, shuffle=False, drop_last=True, \n",
    "                             collate_fn=custom_test_collator)\n",
    "\n",
    "label_dict_temp = {}\n",
    "vocab_dict_temp = {}\n",
    "\n",
    "for k, v in label_dict.items():\n",
    "    label_dict_temp[v] = k\n",
    "for k, v in word_idx.items():\n",
    "    vocab_dict_temp[v] = k\n",
    "    \n",
    "for e in range(1,epochs + 1):\n",
    "#     model_custom.load_state_dict(torch.load(\"./BiLSTM_b1_epoch_\"+str(e)+\".pt\"))#125\n",
    "    model_custom.load_state_dict(torch.load(\"./blstm1.pt\"))#125\n",
    "    model_custom.to(device)\n",
    "\n",
    "    \n",
    "#     print(label_dict)\n",
    "#     rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "#     rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "    file = open(\"test1.out\", 'w')\n",
    "    for test_data, test_data_len in dataloader_test:\n",
    "\n",
    "        pred = model_custom(test_data.to(device), test_data_len)\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.detach().numpy()\n",
    "        test_data = test_data.detach().numpy()\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        pred = pred.reshape((len(test_data), -1))\n",
    "        \n",
    "        for i in range(len(test_data)):\n",
    "            for j in range(len(test_data[i])):\n",
    "                if test_data[i][j] != 0:\n",
    "                    word = vocab_dict_temp[test_data[i][j]]\n",
    "                    op = label_dict_temp[pred[i][j]]\n",
    "                    file.write(\" \".join([str(j+1), word, op]))\n",
    "                    file.write(\"\\n\")\n",
    "\n",
    "            file.write(\"\\n\")        \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task-2\n",
    "BiLSTM Model for Task 2: Using GloVe word embeddings\n",
    "'''\n",
    "class Glove_network(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,\n",
    "                 dropout_val, tag_size, emb_matrix):\n",
    "        super(Glove_network, self).__init__()\n",
    "        self.embedding_dim = embedding_dim  # Embedding Dimension = 100\n",
    "        self.lstm_layers = lstm_layers  # LSTM Layers = 1\n",
    "        self.hidden_dim = hidden_dim  # hidden_dim = 256\n",
    "        self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128\n",
    "        self.tag_size = tag_size  # Tag Size = 9\n",
    "        self.emb_matrix = emb_matrix\n",
    "        self.num_directions = 2\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix))\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, linear_out_dim) \n",
    "        self.classifier = nn.Linear(linear_out_dim, self.tag_size)\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.relu = nn.ELU()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim).to(device))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, sen, sen_len):  # sen_len\n",
    "        # Set initial states\n",
    "        batch_size = sen.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        embedded = self.embedding(sen).float()\n",
    "        packed_embedded = pack_padded_sequence(embedded, sen_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        dropout = self.dropout(output_unpacked)\n",
    "        lin = self.fc(dropout)\n",
    "        pred = self.relu(lin)\n",
    "        pred = self.classifier(pred)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK -2\n",
    "\n",
    "glove = pd.read_csv('./glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "# glove_emb = {key: val.values for key, val in glove.T.items()}\n",
    "glove_emb = {}\n",
    "for k, v in glove.T.items():\n",
    "    glove_emb[k] = v.values\n",
    "\n",
    "# word_idx = prep_word_index(X_train, X_dev, X_test)\n",
    "glove_emb_list = []\n",
    "for k in glove_emb:\n",
    "    glove_emb_list.append(glove_emb[k])    \n",
    "glove_vec = np.array(glove_emb_list)\n",
    "\n",
    "glove_emb[\"<PAD>\"] = np.zeros((100,), dtype=\"float64\")\n",
    "glove_emb[\"<UNK>\"] = np.mean(glove_vec, axis=0, keepdims=True).reshape(100,)\n",
    "\n",
    "# def create_emb_matrix(word_idx, emb_dict, dimension):\n",
    "\n",
    "emb_matrix = np.zeros((len(word_idx), 100))\n",
    "for word, idx in word_idx.items():\n",
    "    if word in glove_emb:\n",
    "        emb_matrix[idx] = glove_emb[word]\n",
    "    else:\n",
    "        if word.lower() in glove_emb:\n",
    "            emb_matrix[idx] = glove_emb[word.lower()] + 5e-3\n",
    "        else:\n",
    "            emb_matrix[idx] = glove_emb[\"<UNK>\"]\n",
    "\n",
    "#     return emb_matrix\n",
    "\n",
    "# emb_matrix = create_emb_matrix(\n",
    "#     word_idx=word_idx, emb_dict=glove_emb, dimension=100)\n",
    "\n",
    "vocab_size = emb_matrix.shape[0]\n",
    "vector_size = emb_matrix.shape[1]\n",
    "print(vocab_size, vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f26464",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = Glove_network(vocab_size=len(word_idx),\n",
    "                      embedding_dim=100,\n",
    "                      linear_out_dim=128,\n",
    "                      hidden_dim=256,\n",
    "                      lstm_layers=1,\n",
    "                      dropout_val=0.33,\n",
    "                      tag_size=len(label_dict),\n",
    "                      emb_matrix=emb_matrix)\n",
    "model_glove.to(device)\n",
    "print(model_glove)\n",
    "\n",
    "train_loader_input_glove = BiLSTM_DataLoader(train_x_vec, train_y_vec)\n",
    "custom_collator = CollateData()\n",
    "dataloader = DataLoader(dataset=train_loader_input_glove, batch_size=32, drop_last=True, collate_fn=custom_collator)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_wt)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "criterion = criterion.to(device)\n",
    "criterion.requres_grad = True\n",
    "# optimizer = torch.optim.SGD(model_glove.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer = torch.optim.SGD(model_glove.parameters(), lr=0.1, momentum=0.9)\n",
    "# scheduler = StepLR(optimizer, step_size=15, gamma=0.9)\n",
    "epochs = 220\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    train_loss = 0.0\n",
    "    for input, label, input_len, label_len in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_glove(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input.size(1)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(i, train_loss))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for input, label, input_len, label_len in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_glove(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        optimizer.step()\n",
    "    torch.save(model_glove.state_dict(), 'blstm2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8dd61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting for validation dataset\n",
    "dev_loader_input = BiLSTM_DataLoader(val_x_vec, val_y_vec)\n",
    "custom_collator = CollateData()\n",
    "dataloader_dev = DataLoader(dataset=dev_loader_input, batch_size=32, shuffle=False, drop_last=True, collate_fn=custom_collator)\n",
    "\n",
    "model_glove.load_state_dict(torch.load(\"./blstm2.pt\"))#125\n",
    "\n",
    "label_dict_temp = {}\n",
    "vocab_dict_temp = {}\n",
    "\n",
    "for k, v in label_dict.items():\n",
    "    label_dict_temp[v] = k\n",
    "for k, v in word_idx.items():\n",
    "    vocab_dict_temp[v] = k\n",
    "    \n",
    "for e in range(1, epochs+1):\n",
    "    model_glove = Glove_network(vocab_size=len(word_idx),\n",
    "                        embedding_dim=100,\n",
    "                        linear_out_dim=128,\n",
    "                        hidden_dim=256,\n",
    "                        lstm_layers=1,\n",
    "                        dropout_val=0.33,\n",
    "                        tag_size=len(label_dict),\n",
    "                        emb_matrix = emb_matrix)\n",
    "\n",
    "#     model_glove.load_state_dict(torch.load(\"./Glove_network\"+str(e)+\".pt\"))#125\n",
    "    model_glove.load_state_dict(torch.load(\"./blstm2.pt\"))\n",
    "    model_glove.to(device)\n",
    "#     rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "#     rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "    \n",
    "    file = open(\"dev2.out\", 'w')\n",
    "    for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
    "\n",
    "        pred = model_glove(dev_data.to(device), dev_data_len)\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.detach().numpy()\n",
    "        label = label.detach().numpy()\n",
    "        dev_data = dev_data.detach().numpy()\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        pred = pred.reshape((len(label), -1))\n",
    "\n",
    "        for i in range(len(dev_data)):\n",
    "            for j in range(len(dev_data[i])):\n",
    "                if dev_data[i][j] != 0:\n",
    "                    word = vocab_dict_temp[dev_data[i][j]]\n",
    "                    gold = label_dict_temp[label[i][j]]\n",
    "                    op = label_dict_temp[pred[i][j]]\n",
    "                    file.write(\" \".join([str(j+1), word, gold, op]))\n",
    "                    file.write(\"\\n\")\n",
    "            file.write(\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "#     !perl conll03eval.txt < dev2.out\n",
    "     \n",
    "with torch.no_grad():\n",
    "    for input, label, input_len, label_len in dataloader_dev:\n",
    "        optimizer.zero_grad()\n",
    "        output = model_glove(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        optimizer.step()\n",
    "        \n",
    "    !perl conll03eval.txt < dev2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_input = BiLSTM_TestLoader(test_x_vec)\n",
    "custom_test_collator = CollateTestData()\n",
    "dataloader_test = DataLoader(dataset=test_loader_input,batch_size=32, shuffle=False, drop_last=True,\n",
    "                             collate_fn=custom_test_collator)\n",
    "label_dict_temp = {}\n",
    "vocab_dict_temp = {}\n",
    "\n",
    "for k, v in label_dict.items():\n",
    "    label_dict_temp[v] = k\n",
    "for k, v in word_idx.items():\n",
    "    vocab_dict_temp[v] = k\n",
    "    \n",
    "for e in range(1,epochs + 1):\n",
    "#     model_glove.load_state_dict(torch.load(\"./Glove_network\"+str(e)+\".pt\"))#125\n",
    "    model_glove.load_state_dict(torch.load(\"./blstm2.pt\"))\n",
    "    model_glove.to(device)\n",
    "#     rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "#     rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "    file = open(\"test1.out\", 'w')\n",
    "    for test_data, test_data_len in dataloader_test:\n",
    "\n",
    "        pred = model_glove(test_data.to(device), test_data_len)\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.detach().numpy()\n",
    "        test_data = test_data.detach().numpy()\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        pred = pred.reshape((len(test_data), -1))\n",
    "        \n",
    "        for i in range(len(test_data)):\n",
    "            for j in range(len(test_data[i])):\n",
    "                if test_data[i][j] != 0:\n",
    "                    word = vocab_dict_temp[test_data[i][j]]\n",
    "                    op = label_dict_temp[pred[i][j]]\n",
    "                    file.write(\" \".join([str(j+1), word, op]))\n",
    "                    file.write(\"\\n\")\n",
    "\n",
    "            file.write(\"\\n\")        \n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
