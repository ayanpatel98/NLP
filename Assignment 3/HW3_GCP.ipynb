{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5965d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install bs4\n",
    "# ! pip install contractions\n",
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca7dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d65dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/ayanpatel_69/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ayanpatel_69/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/ayanpatel_69/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim.downloader as api\n",
    "import gensim.models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gc\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3eb19",
   "metadata": {},
   "source": [
    "# Task 1: Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5176f682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n",
    "df = df[['star_rating', 'review_body']]\n",
    "class_one = df[(df['star_rating']==1) | (df['star_rating']==2)]\n",
    "class_two = df[df['star_rating']==3]\n",
    "class_three = df[(df['star_rating']==4) | (df['star_rating']==5)]\n",
    "\n",
    "class_one.loc[:, \"label\"] =1\n",
    "class_two.loc[:, \"label\"] =2\n",
    "class_three.loc[:, \"label\"] =3\n",
    "\n",
    "class_one = class_one.sample(n=20000, random_state=100)\n",
    "class_two = class_two.sample(n=20000, random_state=100)\n",
    "class_three = class_three.sample(n=20000, random_state=100)\n",
    "# dataset = pd.concat([class_one, class_two, class_three])\n",
    "df = pd.concat([class_one, class_two, class_three])\n",
    "\n",
    "df.reset_index(drop=True)\n",
    "train = df.sample(frac=0.8, random_state=100)\n",
    "test = df.drop(train.index)\n",
    "\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)\n",
    "\n",
    "train.to_csv('train.csv', index = False)\n",
    "test.to_csv('test.csv', index = False)\n",
    "\n",
    "del globals()['class_one'], globals()['class_two'], globals()['class_three'], globals()['df']\n",
    "train = test = df = dataset = [[99999, 99999]]\n",
    "del df, train, test, dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084dd4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pretrained Word2Vec model:\n",
    "pretrained_w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b7b44b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71968073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check semantic similarities of the generated vectors:\n",
      "[('queen', 0.7118193507194519)]\n",
      "Excellent ~ Outstanding: 0.5567486\n",
      "time ~ schedule: 0.26993576\n"
     ]
    }
   ],
   "source": [
    "print('Check semantic similarities of the generated vectors:')\n",
    "print(pretrained_w2v.most_similar(positive=['king', 'woman'], negative=['man'], topn = 1))\n",
    "print('Excellent ~ Outstanding:', pretrained_w2v.similarity('excellent', 'outstanding'))\n",
    "print('time ~ schedule:', pretrained_w2v.similarity('time', 'schedule'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40149bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d5b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert all reviews to lower case\n",
    "train['review_body'] = train['review_body'].str.lower()\n",
    "test['review_body'] = test['review_body'].str.lower()\n",
    "\n",
    "'''\n",
    "URL Remover code\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "test['review_body'] = test['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "def html_tag_remover(review):\n",
    "    soup = BeautifulSoup(review, 'html.parser')\n",
    "    review = soup.get_text()\n",
    "    return review  \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: html_tag_remover(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: html_tag_remover(review))\n",
    "\n",
    "'''\n",
    "remove non-alphabetical characters\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n",
    "\n",
    "'''\n",
    "remove extra spaces\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub(' +', ' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub(' +', ' ', review))\n",
    "\n",
    "'''\n",
    "perform contractions on the reviews\n",
    "'''\n",
    "def expand_contractions(review):\n",
    "    review = contractions.fix(review)\n",
    "    return review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: expand_contractions(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: expand_contractions(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7713746",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "remove the stop words AND perform lemmatization\n",
    "\n",
    "'''\n",
    "avg_len_before_prepro = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    stop_words_english = set(stopwords.words('english'))\n",
    "    review_word_tokens = word_tokenize(review)\n",
    "    filtered_review = [word for word in review_word_tokens if not word in stop_words_english]\n",
    "    return filtered_review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: remove_stopwords(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: remove_stopwords(review))\n",
    "\n",
    "def review_lemmatize(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_review = [lemmatizer.lemmatize(word) for word in review]\n",
    "    return ' '.join(lemmatized_review)    \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "\n",
    "avg_len_after_prepro = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0836a8",
   "metadata": {},
   "source": [
    "# Task 2: Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38dd8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Sentences = [sentence.split(' ') for sentence in train['review_body'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e685c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Word2Vec\n",
    "custom_model = gensim.models.Word2Vec(all_Sentences, vector_size = 300, min_count=9, window=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31afd952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check semantic similarities of the generated vectors:\n",
      "('touted', 0.7953194379806519)\n",
      "Excellent ~ Outstanding: 0.7759805\n",
      "time ~ schedule: 0.20508367\n"
     ]
    }
   ],
   "source": [
    "print('Check semantic similarities of the generated vectors:')\n",
    "print(custom_model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn = 1)[0])\n",
    "print('Excellent ~ Outstanding:', custom_model.wv.similarity('excellent', 'outstanding'))\n",
    "print('time ~ schedule:', custom_model.wv.similarity('time', 'schedule'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e8889e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_Sentences, custom_model\n",
    "gc.collect()\n",
    "all_Sentences = [1]\n",
    "custom_model = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821ed51",
   "metadata": {},
   "source": [
    "# Task 3: Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fc2a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word2Vec vectors\n",
    "def average_vectors(review, label):\n",
    "    temp_review = review.split(' ')\n",
    "    review_vector = np.array([pretrained_w2v[word] for word in temp_review if word in pretrained_w2v])\n",
    "    if len(review_vector) >=1:\n",
    "#         review_vector = []\n",
    "#         for word in words:\n",
    "#             review_vector.append(pretrained_w2v[word])\n",
    "        return review_vector, label\n",
    "\n",
    "def average_vectors_concat(review, label):\n",
    "    temp_review = review.split(' ')\n",
    "    review_vector = np.array([pretrained_w2v[word] for word in temp_review[:10] if word in pretrained_w2v])\n",
    "    \n",
    "    # can be the case where the words in the review are not found in the W2V vocabulary\n",
    "    if len(review_vector)==0:\n",
    "        review_vector = np.zeros((1, 300))\n",
    "    review_vector = np.concatenate(review_vector, axis=0)\n",
    "    \n",
    "    # In the case where the total dim of the feature vector is <3000 add the padding with zeros\n",
    "    if len(review_vector)<3000:\n",
    "        review_vector = np.concatenate([review_vector, np.zeros(3000-len(data))])\n",
    "    return review_vector/10, label\n",
    "\n",
    "#     review_vector = np.array([pretrained_w2v[word] for word in temp_review[:20] if word in pretrained_w2v])\n",
    "    \n",
    "#     # can be the case where the words in the review are not found in the W2V vocabulary\n",
    "#     if len(review_vector)==0:\n",
    "#         review_vector = np.zeros((20, 300))\n",
    "    \n",
    "#     # In the case where the total dim of the feature vector is <20 add the padding with zeros\n",
    "#     elif len(review_vector)<20:\n",
    "#         review_vector = np.concatenate([review_vector, np.zeros((20-len(review_vector), 300))])\n",
    "        \n",
    "#     return review_vector, label\n",
    "    \n",
    "    \n",
    "def featurization(dataset, concat = False):\n",
    "    features = []\n",
    "    y_labels = []\n",
    "    concat = concat\n",
    "    \n",
    "    for review, label in zip(dataset['review_body'], dataset['label']):\n",
    "        try:\n",
    "            if not concat:\n",
    "                x, y = average_vectors(review, label)\n",
    "                features.append(np.mean(x, axis=0))\n",
    "            else:\n",
    "                x, y = average_vectors_concat(review, label)\n",
    "                features.append(x)\n",
    "                \n",
    "            y_labels.append(y)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    return features, y_labels\n",
    "\n",
    "# Vectors without concatenation\n",
    "w2v_pretrain_train_x, w2v_pretrain_train_y = featurization(train)\n",
    "w2v_pretrain_test_x, w2v_pretrain_test_y = featurization(test)\n",
    "\n",
    "# Vectors with concatenation\n",
    "w2v_pretrain_train_concat_x, w2v_pretrain_train_concat_y = featurization(train, True)\n",
    "w2v_pretrain_test_concat_x, w2v_pretrain_test_concat_y = featurization(test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5239dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Feature Extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 0.001)\n",
    "tfidf_X_train = tfidf_vectorizer.fit_transform(list(train['review_body']))\n",
    "tfidf_X_train = pd.DataFrame(tfidf_X_train.toarray())\n",
    "\n",
    "tfidf_X_test = tfidf_vectorizer.transform(list(test['review_body']))\n",
    "tfidf_X_test = pd.DataFrame(tfidf_X_test.toarray())\n",
    "\n",
    "tfidf_Y_train = train['label']\n",
    "tfidf_Y_test = test['label']\n",
    "\n",
    "tfidf_Y_train = tfidf_Y_train.astype('int')\n",
    "tfidf_Y_test = tfidf_Y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bebea69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Perceptron Model on Average Word2Vec Features\n",
    "perceptr_w2v = Perceptron(random_state = 100, eta0=0.1)\n",
    "perceptr_w2v.fit(w2v_pretrain_train_x, w2v_pretrain_train_y)\n",
    "Y_pred_w2v_test = perceptr_w2v.predict(w2v_pretrain_test_x)\n",
    "\n",
    "# Training Perceptron Model on TF-IDF Features\n",
    "perceptr_tfidf = Perceptron(random_state = 100, eta0=0.1)\n",
    "perceptr_tfidf.fit(tfidf_X_train, tfidf_Y_train)\n",
    "Y_pred_tfidf_test = perceptr_tfidf.predict(tfidf_X_test)\n",
    "\n",
    "target_names = ['class 1', 'class 2', 'class 3']\n",
    "report_w2v = classification_report(w2v_pretrain_test_y, Y_pred_w2v_test, target_names=target_names, output_dict=True)\n",
    "report_tfidf = classification_report(tfidf_Y_test, Y_pred_tfidf_test, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7515f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy values PERCEPTRON for w2v and tfidf features:\n",
      "0.5805374728759807 0.6170833333333333\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy values PERCEPTRON for w2v and tfidf features:')\n",
    "print(report_w2v['accuracy'], report_tfidf['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a35497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training SVM Model on Average Word2Vec Features\n",
    "svm_w2v = LinearSVC(random_state=100, max_iter=1000)\n",
    "svm_w2v.fit(w2v_pretrain_train_x, w2v_pretrain_train_y)\n",
    "Y_pred_w2v_svm_test = svm_w2v.predict(w2v_pretrain_test_x)\n",
    "\n",
    "# Training SVM Model on TFIDF Features\n",
    "svm_tfidf = LinearSVC(random_state=100, max_iter=1000)\n",
    "svm_tfidf.fit(tfidf_X_train, tfidf_Y_train)\n",
    "Y_pred_tfidf_svm_test = svm_tfidf.predict(tfidf_X_test)\n",
    "\n",
    "report_svm_w2v = classification_report(w2v_pretrain_test_y, Y_pred_w2v_svm_test, target_names=target_names, output_dict=True)\n",
    "report_svm_tfidf = classification_report(tfidf_Y_test, Y_pred_tfidf_svm_test, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66f59b74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy values SVM for w2v and tfidf features:\n",
      "0.627691537305959 0.6685\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy values SVM for w2v and tfidf features:')\n",
    "print(report_svm_w2v['accuracy'], report_svm_tfidf['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8a1d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del globals()['pretrained_w2v']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c042fdd",
   "metadata": {},
   "source": [
    "# Task 4: Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f23f3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e243849",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e99310ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n",
      "MLP_concat(\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 300):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)  \n",
    "        \n",
    "#         self.sig = nn.Sigmoid()\n",
    "#         self.soft = nn.Softmax(dim = 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class MLP_concat(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 3000):\n",
    "        super(MLP_concat, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)  \n",
    "        \n",
    "#         self.sig = nn.Sigmoid()\n",
    "#         self.soft = nn.Softmax(dim = 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "model_concat = MLP_concat()\n",
    "model = model\n",
    "model_concat = model_concat\n",
    "print(model)\n",
    "print(model_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17d6f7",
   "metadata": {},
   "source": [
    "-- Task 4(a) using the average Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95bf3f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.094978 \tValidation Loss: 1.088335 \tEpoch Accuracy: 0.429311\n",
      "Epoch: 2 \tTraining Loss: 1.051001 \tValidation Loss: 1.003671 \tEpoch Accuracy: 0.502504\n",
      "Epoch: 3 \tTraining Loss: 0.954951 \tValidation Loss: 0.933668 \tEpoch Accuracy: 0.549407\n",
      "Epoch: 4 \tTraining Loss: 0.907306 \tValidation Loss: 0.918480 \tEpoch Accuracy: 0.562093\n",
      "Epoch: 5 \tTraining Loss: 0.883382 \tValidation Loss: 0.882331 \tEpoch Accuracy: 0.592472\n",
      "Epoch: 6 \tTraining Loss: 0.861866 \tValidation Loss: 0.866874 \tEpoch Accuracy: 0.604740\n",
      "Epoch: 7 \tTraining Loss: 0.844556 \tValidation Loss: 0.853242 \tEpoch Accuracy: 0.615173\n",
      "Epoch: 8 \tTraining Loss: 0.832535 \tValidation Loss: 0.839693 \tEpoch Accuracy: 0.621015\n",
      "Epoch: 9 \tTraining Loss: 0.824333 \tValidation Loss: 0.840188 \tEpoch Accuracy: 0.624019\n",
      "Epoch: 10 \tTraining Loss: 0.818082 \tValidation Loss: 0.829889 \tEpoch Accuracy: 0.629027\n",
      "Epoch: 11 \tTraining Loss: 0.813533 \tValidation Loss: 0.825328 \tEpoch Accuracy: 0.629277\n",
      "Epoch: 12 \tTraining Loss: 0.809082 \tValidation Loss: 0.826268 \tEpoch Accuracy: 0.629528\n",
      "Epoch: 13 \tTraining Loss: 0.805373 \tValidation Loss: 0.825963 \tEpoch Accuracy: 0.627107\n",
      "Epoch: 14 \tTraining Loss: 0.801969 \tValidation Loss: 0.819267 \tEpoch Accuracy: 0.630529\n",
      "Epoch: 15 \tTraining Loss: 0.799598 \tValidation Loss: 0.817369 \tEpoch Accuracy: 0.637289\n",
      "Epoch: 16 \tTraining Loss: 0.796704 \tValidation Loss: 0.829689 \tEpoch Accuracy: 0.623185\n",
      "Epoch: 17 \tTraining Loss: 0.793402 \tValidation Loss: 0.813423 \tEpoch Accuracy: 0.637623\n",
      "Epoch: 18 \tTraining Loss: 0.790954 \tValidation Loss: 0.810770 \tEpoch Accuracy: 0.638291\n",
      "Epoch: 19 \tTraining Loss: 0.788016 \tValidation Loss: 0.814062 \tEpoch Accuracy: 0.636872\n",
      "Epoch: 20 \tTraining Loss: 0.785509 \tValidation Loss: 0.813929 \tEpoch Accuracy: 0.634869\n"
     ]
    }
   ],
   "source": [
    "train_data=TensorDataset(torch.FloatTensor(w2v_pretrain_train_x), torch.LongTensor(w2v_pretrain_train_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(w2v_pretrain_test_x), torch.LongTensor(w2v_pretrain_test_y))\n",
    "\n",
    "# Data Loader\n",
    "train_batch_size=16\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_batch_size=16\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, (target-1))\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, (target-1))\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == (target-1)).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bea9c9",
   "metadata": {},
   "source": [
    "-- Test Dataset Accuracy classwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cac3daa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67      4085\n",
      "           1       0.58      0.47      0.52      3979\n",
      "           2       0.64      0.77      0.70      3918\n",
      "\n",
      "    accuracy                           0.63     11982\n",
      "   macro avg       0.63      0.64      0.63     11982\n",
      "weighted avg       0.63      0.63      0.63     11982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test_batch_size= len(w2v_pretrain_test_x)\n",
    "# test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "model.eval() # prep model for evaluation\n",
    "main_tar = []\n",
    "predss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, (target-1))\n",
    "        # update running validation loss \n",
    "        #valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        #correct += (ypred == target-1).float().sum()\n",
    "        for i in np.array(target-1):\n",
    "            main_tar.append(i)\n",
    "        for j in np.array(ypred):\n",
    "            predss.append(j)\n",
    "        \n",
    "#         mat = confusion_matrix((target-1),ypred)\n",
    "#         ans = mat.diagonal()/mat.sum(axis=1)\n",
    "        \n",
    "# print(\"Accuracy Values for each class\")\n",
    "\n",
    "# for i,acc in enumerate(ans):\n",
    "#     print(f\"Class {i+1} : {acc: .6f}\")\n",
    "print(classification_report(main_tar, predss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12cf86",
   "metadata": {},
   "source": [
    "Task 4(b) 10 word vectors concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "538558ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.976190 \tValidation Loss: 0.952041 \tEpoch Accuracy: 0.537061\n",
      "Epoch: 2 \tTraining Loss: 0.898768 \tValidation Loss: 0.924447 \tEpoch Accuracy: 0.553955\n",
      "Epoch: 3 \tTraining Loss: 0.849942 \tValidation Loss: 0.932289 \tEpoch Accuracy: 0.558015\n",
      "Epoch: 4 \tTraining Loss: 0.782588 \tValidation Loss: 0.964667 \tEpoch Accuracy: 0.541776\n",
      "Epoch: 5 \tTraining Loss: 0.667988 \tValidation Loss: 1.040065 \tEpoch Accuracy: 0.529597\n",
      "Epoch: 6 \tTraining Loss: 0.510079 \tValidation Loss: 1.256078 \tEpoch Accuracy: 0.530251\n",
      "Epoch: 7 \tTraining Loss: 0.352025 \tValidation Loss: 1.622535 \tEpoch Accuracy: 0.524358\n",
      "Epoch: 8 \tTraining Loss: 0.227284 \tValidation Loss: 1.925074 \tEpoch Accuracy: 0.514536\n",
      "Epoch: 9 \tTraining Loss: 0.149157 \tValidation Loss: 2.293204 \tEpoch Accuracy: 0.510084\n",
      "Epoch: 10 \tTraining Loss: 0.116174 \tValidation Loss: 2.711413 \tEpoch Accuracy: 0.498298\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data=TensorDataset(torch.FloatTensor(w2v_pretrain_train_concat_x), torch.LongTensor(w2v_pretrain_train_concat_y))\n",
    "\n",
    "test_data=TensorDataset(torch.FloatTensor(w2v_pretrain_test_concat_x), torch.LongTensor(w2v_pretrain_test_concat_y))\n",
    "\n",
    "# Data Loader\n",
    "train_batch_size=16\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_batch_size=16\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.Adam(model_concat.parameters(), lr=0.002)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model_concat.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model_concat.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16d917b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Values for each class\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.54      0.53      2600\n",
      "           1       0.45      0.42      0.43      2602\n",
      "           2       0.53      0.54      0.53      2434\n",
      "\n",
      "    accuracy                           0.50      7636\n",
      "   macro avg       0.50      0.50      0.50      7636\n",
      "weighted avg       0.50      0.50      0.50      7636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test_batch_size= len(w2v_pretrain_test_concat_x)\n",
    "# test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "model_concat.eval() # prep model for evaluation\n",
    "main_tar = []\n",
    "predss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        #valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        #correct += (ypred == target-1).float().sum()\n",
    "        for i in np.array(target-1):\n",
    "            main_tar.append(i)\n",
    "        for j in np.array(ypred):\n",
    "            predss.append(j)\n",
    "#         mat = confusion_matrix(target-1,ypred)\n",
    "#         ans = mat.diagonal()/mat.sum(axis=1)\n",
    "        \n",
    "print(\"Accuracy Values for each class\")\n",
    "\n",
    "# for i,acc in enumerate(ans):\n",
    "#     print(f\"Class {i+1} : {acc: .6f}\")\n",
    "print(classification_report(main_tar, predss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe698dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del globals()['tfidf_X_train'], globals()['tfidf_X_test'], globals()['tfidf_Y_train'], globals()['tfidf_Y_test']\n",
    "\n",
    "del globals()['w2v_pretrain_train_x'], globals()['w2v_pretrain_train_y']\n",
    "del globals()['w2v_pretrain_test_x'], globals()['w2v_pretrain_test_y']\n",
    "\n",
    "del globals()['w2v_pretrain_train_concat_x'], globals()['w2v_pretrain_train_concat_y']\n",
    "del globals()['w2v_pretrain_test_concat_x'], globals()['w2v_pretrain_test_concat_y']\n",
    "\n",
    "del globals()['model'], globals()['model_concat'], globals()['train_data'], globals()['test_data']\n",
    "del globals()['Y_pred_w2v_test'], globals()['Y_pred_tfidf_test'], globals()['Y_pred_w2v_svm_test'], globals()['Y_pred_tfidf_svm_test']\n",
    "\n",
    "del globals()['train_loader'], globals()['test_loader']\n",
    "del globals()['main_tar'], globals()['predss']\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a9aa9",
   "metadata": {},
   "source": [
    "# Task 5 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bcf18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained Word2Vec model:\n",
    "# pretrained_w2v = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8373e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "limiting the maximum review length to 20 by truncating longer reviews and padding\n",
    "shorter reviews with a null value (0)\n",
    "'''\n",
    "# Average word2Vec vectors\n",
    "def average_vectors_rnn(review):\n",
    "    temp_review = review.split(' ')\n",
    "        \n",
    "#     review_vector = []\n",
    "#     for word in words:\n",
    "#         review_vector.append()\n",
    "#     review_vector = np.array(review_vector)\n",
    "\n",
    "    review_vector = np.array([pretrained_w2v[word] for word in temp_review[:20] if word in pretrained_w2v])\n",
    "    \n",
    "    # can be the case where the words in the review are not found in the W2V vocabulary\n",
    "    if len(review_vector)==0:\n",
    "        review_vector = np.zeros((20, 300))\n",
    "    \n",
    "    # In the case where the total dim of the feature vector is <20 add the padding with zeros\n",
    "    elif len(review_vector)<20:\n",
    "        review_vector = np.concatenate([review_vector, np.zeros((20-len(review_vector), 300))])\n",
    "        \n",
    "    return review_vector\n",
    "    \n",
    "def featurization_rnn(dataset):\n",
    "    features = []\n",
    "\n",
    "    for review in dataset['review_body']:\n",
    "        x = average_vectors_rnn(review)\n",
    "        features.append(x)\n",
    "        \n",
    "    return features\n",
    "\n",
    "# Review Vectors for  first 20 words each\n",
    "w2v_pretrain_train_x = featurization_rnn(train)\n",
    "w2v_pretrain_train_y = train['label']\n",
    "w2v_pretrain_test_x = featurization_rnn(test)\n",
    "w2v_pretrain_test_y = test['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603cec1",
   "metadata": {},
   "source": [
    "\n",
    "- Task 5(a): Train a simple RNN for sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47a6873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_model(\n",
      "  (rnn_layer): RNN(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class rnn_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rnn_model, self).__init__()\n",
    "\n",
    "        # self.hidden_size = hidden_size\n",
    "\n",
    "        # self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.rnn_layer = nn.RNN(300, 20, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(20,3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.view(-1,20,300)\n",
    "        output, hidden = self.rnn_layer(output)\n",
    "        output=self.fc(output[:,-1,:])\n",
    "                \n",
    "        # hidden = self.i2h(combined)\n",
    "        # output = self.i2o(combined)\n",
    "        # output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "model = rnn_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "640bb8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=TensorDataset(torch.FloatTensor(w2v_pretrain_train_x), torch.LongTensor(w2v_pretrain_train_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(w2v_pretrain_test_x), torch.LongTensor(w2v_pretrain_test_y))\n",
    "\n",
    "# Data Loader\n",
    "train_batch_size=256\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_batch_size=256\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "007b9941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.979274 \tValidation Loss: 0.980452 \tEpoch Accuracy: 0.511667\n",
      "Epoch: 2 \tTraining Loss: 0.972369 \tValidation Loss: 0.978157 \tEpoch Accuracy: 0.514167\n",
      "Epoch: 3 \tTraining Loss: 0.972523 \tValidation Loss: 0.977714 \tEpoch Accuracy: 0.509667\n",
      "Epoch: 4 \tTraining Loss: 0.972811 \tValidation Loss: 0.974993 \tEpoch Accuracy: 0.511833\n",
      "Epoch: 5 \tTraining Loss: 0.970838 \tValidation Loss: 0.973990 \tEpoch Accuracy: 0.509417\n",
      "Epoch: 6 \tTraining Loss: 0.968697 \tValidation Loss: 0.971937 \tEpoch Accuracy: 0.514000\n",
      "Epoch: 7 \tTraining Loss: 0.973769 \tValidation Loss: 0.978835 \tEpoch Accuracy: 0.507667\n",
      "Epoch: 8 \tTraining Loss: 0.971384 \tValidation Loss: 0.974247 \tEpoch Accuracy: 0.510000\n",
      "Epoch: 9 \tTraining Loss: 0.969083 \tValidation Loss: 0.973396 \tEpoch Accuracy: 0.509167\n",
      "Epoch: 10 \tTraining Loss: 0.969155 \tValidation Loss: 0.974671 \tEpoch Accuracy: 0.511167\n",
      "Epoch: 11 \tTraining Loss: 0.969270 \tValidation Loss: 0.978291 \tEpoch Accuracy: 0.514333\n",
      "Epoch: 12 \tTraining Loss: 0.972533 \tValidation Loss: 0.978381 \tEpoch Accuracy: 0.515500\n",
      "Epoch: 13 \tTraining Loss: 0.971877 \tValidation Loss: 0.977336 \tEpoch Accuracy: 0.515500\n",
      "Epoch: 14 \tTraining Loss: 0.971071 \tValidation Loss: 0.977846 \tEpoch Accuracy: 0.514667\n",
      "Epoch: 15 \tTraining Loss: 0.973354 \tValidation Loss: 0.984246 \tEpoch Accuracy: 0.510083\n",
      "Epoch: 16 \tTraining Loss: 0.974722 \tValidation Loss: 0.980575 \tEpoch Accuracy: 0.510083\n",
      "Epoch: 17 \tTraining Loss: 0.972284 \tValidation Loss: 0.977464 \tEpoch Accuracy: 0.514250\n",
      "Epoch: 18 \tTraining Loss: 0.970334 \tValidation Loss: 0.975858 \tEpoch Accuracy: 0.509750\n",
      "Epoch: 19 \tTraining Loss: 0.969209 \tValidation Loss: 0.972438 \tEpoch Accuracy: 0.514333\n",
      "Epoch: 20 \tTraining Loss: 0.968985 \tValidation Loss: 0.972974 \tEpoch Accuracy: 0.515000\n",
      "Accuracy Values for each class\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.564639  0.508686  0.535204      4087\n",
      "           1   0.406170  0.357017  0.380011      3983\n",
      "           2   0.556155  0.681679  0.612553      3930\n",
      "\n",
      "    accuracy                       0.515000     12000\n",
      "   macro avg   0.508988  0.515794  0.509256     12000\n",
      "weighted avg   0.509262  0.515000  0.509025     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))\n",
    "    \n",
    "model.eval() # prep model for evaluation\n",
    "main_tar = []\n",
    "predss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        for i in np.array(target-1):\n",
    "            main_tar.append(i)\n",
    "        for j in np.array(ypred):\n",
    "            predss.append(j)\n",
    "        \n",
    "print(\"Accuracy Values for each class\")\n",
    "\n",
    "print(classification_report(main_tar, predss, digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f20bd8",
   "metadata": {},
   "source": [
    "Task 5(b): Considering a gated recurrent unit cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98bd037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru_model(\n",
      "  (gru_layer): GRU(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class gru_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(gru_model, self).__init__()\n",
    "\n",
    "        # self.hidden_size = hidden_size\n",
    "\n",
    "        # self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.gru_layer = nn.GRU(300, 20, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(20,3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.view(-1,20,300)\n",
    "        output, hidden = self.gru_layer(output)\n",
    "        output=self.fc(output[:,-1,:])\n",
    "                \n",
    "        # hidden = self.i2h(combined)\n",
    "        # output = self.i2o(combined)\n",
    "        # output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "model_gru = gru_model()\n",
    "print(model_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d94143ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.046855 \tValidation Loss: 0.914843 \tEpoch Accuracy: 0.547333\n",
      "Epoch: 2 \tTraining Loss: 0.874030 \tValidation Loss: 0.859234 \tEpoch Accuracy: 0.601833\n",
      "Epoch: 3 \tTraining Loss: 0.825780 \tValidation Loss: 0.819515 \tEpoch Accuracy: 0.629833\n",
      "Epoch: 4 \tTraining Loss: 0.799137 \tValidation Loss: 0.795593 \tEpoch Accuracy: 0.639417\n",
      "Epoch: 5 \tTraining Loss: 0.785343 \tValidation Loss: 0.796183 \tEpoch Accuracy: 0.636333\n",
      "Epoch: 6 \tTraining Loss: 0.772140 \tValidation Loss: 0.779161 \tEpoch Accuracy: 0.649917\n",
      "Epoch: 7 \tTraining Loss: 0.762275 \tValidation Loss: 0.772579 \tEpoch Accuracy: 0.651083\n",
      "Epoch: 8 \tTraining Loss: 0.753916 \tValidation Loss: 0.774624 \tEpoch Accuracy: 0.652333\n",
      "Epoch: 9 \tTraining Loss: 0.746189 \tValidation Loss: 0.767008 \tEpoch Accuracy: 0.660583\n",
      "Epoch: 10 \tTraining Loss: 0.739877 \tValidation Loss: 0.766179 \tEpoch Accuracy: 0.658667\n",
      "Epoch: 11 \tTraining Loss: 0.734509 \tValidation Loss: 0.765252 \tEpoch Accuracy: 0.663250\n",
      "Epoch: 12 \tTraining Loss: 0.727795 \tValidation Loss: 0.763964 \tEpoch Accuracy: 0.661667\n",
      "Epoch: 13 \tTraining Loss: 0.722525 \tValidation Loss: 0.766957 \tEpoch Accuracy: 0.660583\n",
      "Epoch: 14 \tTraining Loss: 0.717892 \tValidation Loss: 0.762696 \tEpoch Accuracy: 0.664167\n",
      "Epoch: 15 \tTraining Loss: 0.716447 \tValidation Loss: 0.765928 \tEpoch Accuracy: 0.657667\n",
      "Epoch: 16 \tTraining Loss: 0.711190 \tValidation Loss: 0.762456 \tEpoch Accuracy: 0.661833\n",
      "Epoch: 17 \tTraining Loss: 0.705159 \tValidation Loss: 0.761759 \tEpoch Accuracy: 0.659417\n",
      "Epoch: 18 \tTraining Loss: 0.703686 \tValidation Loss: 0.759256 \tEpoch Accuracy: 0.662250\n",
      "Epoch: 19 \tTraining Loss: 0.697522 \tValidation Loss: 0.759551 \tEpoch Accuracy: 0.662417\n",
      "Epoch: 20 \tTraining Loss: 0.694835 \tValidation Loss: 0.764165 \tEpoch Accuracy: 0.663333\n",
      "Accuracy Values for each class\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.703308  0.644972  0.672878      4087\n",
      "           1   0.566803  0.628421  0.596023      3983\n",
      "           2   0.735401  0.717812  0.726500      3930\n",
      "\n",
      "    accuracy                       0.663333     12000\n",
      "   macro avg   0.668504  0.663735  0.665134     12000\n",
      "weighted avg   0.668510  0.663333  0.664930     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.Adam(model_gru.parameters(), lr=0.001)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model_gru.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_gru(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model_gru.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_gru(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))\n",
    "    \n",
    "model_gru.eval() # prep model for evaluation\n",
    "main_tar = []\n",
    "predss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_gru(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        for i in np.array(target-1):\n",
    "            main_tar.append(i)\n",
    "        for j in np.array(ypred):\n",
    "            predss.append(j)\n",
    "        \n",
    "print(\"Accuracy Values for each class\")\n",
    "\n",
    "print(classification_report(main_tar, predss, digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81594d",
   "metadata": {},
   "source": [
    "- Task 5(c): Considering a  LSTM unit cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6da72788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_model(\n",
      "  (lstm_layer): LSTM(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class lstm_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lstm_model, self).__init__()\n",
    "\n",
    "        # self.hidden_size = hidden_size\n",
    "\n",
    "        # self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.lstm_layer = nn.LSTM(300, 20, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(20,3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.view(-1,20,300)\n",
    "        output, hidden = self.lstm_layer(output)\n",
    "        output=self.fc(output[:,-1,:])\n",
    "                \n",
    "        # hidden = self.i2h(combined)\n",
    "        # output = self.i2o(combined)\n",
    "        # output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "model_lstm = lstm_model()\n",
    "print(model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a1da4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.047219 \tValidation Loss: 0.926294 \tEpoch Accuracy: 0.546333\n",
      "Epoch: 2 \tTraining Loss: 0.883900 \tValidation Loss: 0.875389 \tEpoch Accuracy: 0.587417\n",
      "Epoch: 3 \tTraining Loss: 0.842375 \tValidation Loss: 0.839077 \tEpoch Accuracy: 0.612833\n",
      "Epoch: 4 \tTraining Loss: 0.820837 \tValidation Loss: 0.837278 \tEpoch Accuracy: 0.609333\n",
      "Epoch: 5 \tTraining Loss: 0.805212 \tValidation Loss: 0.817233 \tEpoch Accuracy: 0.626500\n",
      "Epoch: 6 \tTraining Loss: 0.791845 \tValidation Loss: 0.807118 \tEpoch Accuracy: 0.632083\n",
      "Epoch: 7 \tTraining Loss: 0.777682 \tValidation Loss: 0.798242 \tEpoch Accuracy: 0.639583\n",
      "Epoch: 8 \tTraining Loss: 0.768709 \tValidation Loss: 0.790986 \tEpoch Accuracy: 0.641417\n",
      "Epoch: 9 \tTraining Loss: 0.759653 \tValidation Loss: 0.785584 \tEpoch Accuracy: 0.648583\n",
      "Epoch: 10 \tTraining Loss: 0.750460 \tValidation Loss: 0.786421 \tEpoch Accuracy: 0.648917\n",
      "Epoch: 11 \tTraining Loss: 0.744512 \tValidation Loss: 0.780563 \tEpoch Accuracy: 0.649917\n",
      "Epoch: 12 \tTraining Loss: 0.738970 \tValidation Loss: 0.775536 \tEpoch Accuracy: 0.654000\n",
      "Epoch: 13 \tTraining Loss: 0.731796 \tValidation Loss: 0.776149 \tEpoch Accuracy: 0.654750\n",
      "Epoch: 14 \tTraining Loss: 0.725611 \tValidation Loss: 0.777108 \tEpoch Accuracy: 0.653000\n",
      "Epoch: 15 \tTraining Loss: 0.720295 \tValidation Loss: 0.775788 \tEpoch Accuracy: 0.656667\n",
      "Epoch: 16 \tTraining Loss: 0.716368 \tValidation Loss: 0.777593 \tEpoch Accuracy: 0.656750\n",
      "Epoch: 17 \tTraining Loss: 0.709826 \tValidation Loss: 0.774424 \tEpoch Accuracy: 0.656250\n",
      "Epoch: 18 \tTraining Loss: 0.705521 \tValidation Loss: 0.776059 \tEpoch Accuracy: 0.655833\n",
      "Epoch: 19 \tTraining Loss: 0.700211 \tValidation Loss: 0.772425 \tEpoch Accuracy: 0.659083\n",
      "Epoch: 20 \tTraining Loss: 0.696247 \tValidation Loss: 0.774204 \tEpoch Accuracy: 0.657417\n",
      "Accuracy Values for each class\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.694154  0.650844  0.671802      4087\n",
      "           1   0.578285  0.565654  0.571900      3983\n",
      "           2   0.696629  0.757252  0.725677      3930\n",
      "\n",
      "    accuracy                       0.657417     12000\n",
      "   macro avg   0.656356  0.657917  0.656460     12000\n",
      "weighted avg   0.656506  0.657417  0.656287     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model_lstm.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_lstm(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model_lstm.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_lstm(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))\n",
    "    \n",
    "model_lstm.eval() # prep model for evaluation\n",
    "main_tar = []\n",
    "predss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_lstm(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        for i in np.array(target-1):\n",
    "            main_tar.append(i)\n",
    "        for j in np.array(ypred):\n",
    "            predss.append(j)\n",
    "        \n",
    "print(\"Accuracy Values for each class\")\n",
    "\n",
    "print(classification_report(main_tar, predss, digits=6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
