{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uy1LQxi709c8"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Installed bs4 (beautiful soup for pulling data out of HTML and XML files.)\n",
    "Installed contractions to perform as many contractions as possible on our dataset.\n",
    "Installed gensim to work with Word2vec feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDqfY2qrgS0T",
    "outputId": "20e28a68-0bbb-4eb4-ee8d-9358d1295d37"
   },
   "outputs": [],
   "source": [
    "# ! pip install bs4\n",
    "# ! pip install contractions\n",
    "# ! pip install gensim\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2UjDX4h0vzM",
    "outputId": "8d5075a9-edb1-4c2a-aec1-c2094520c796"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import copy\n",
    "import sklearn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965
    },
    "id": "8BinJCBDZapC",
    "outputId": "e675a25f-ee22-48ac-b981-651c1b331178"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_496\\3343637191.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df= pd.read_csv('./data.tsv', sep='\\t', dtype=str, lineterminator='\\n', error_bad_lines=False)\n",
      "Skipping line 10093: expected 15 fields, saw 22\n",
      "Skipping line 31965: expected 15 fields, saw 22\n",
      "Skipping line 49886: expected 15 fields, saw 22\n",
      "Skipping line 49905: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 67579: expected 15 fields, saw 22\n",
      "Skipping line 75367: expected 15 fields, saw 22\n",
      "Skipping line 92462: expected 15 fields, saw 22\n",
      "Skipping line 105041: expected 15 fields, saw 22\n",
      "Skipping line 109697: expected 15 fields, saw 22\n",
      "Skipping line 121931: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 139492: expected 15 fields, saw 22\n",
      "Skipping line 158729: expected 15 fields, saw 22\n",
      "Skipping line 165784: expected 15 fields, saw 22\n",
      "Skipping line 176996: expected 15 fields, saw 22\n",
      "Skipping line 182928: expected 15 fields, saw 22\n",
      "Skipping line 195841: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 196938: expected 15 fields, saw 22\n",
      "Skipping line 202535: expected 15 fields, saw 22\n",
      "Skipping line 261147: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 265777: expected 15 fields, saw 22\n",
      "Skipping line 277693: expected 15 fields, saw 22\n",
      "Skipping line 280010: expected 15 fields, saw 22\n",
      "Skipping line 296315: expected 15 fields, saw 22\n",
      "Skipping line 299043: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 334564: expected 15 fields, saw 22\n",
      "Skipping line 337801: expected 15 fields, saw 22\n",
      "Skipping line 341391: expected 15 fields, saw 22\n",
      "Skipping line 354940: expected 15 fields, saw 22\n",
      "Skipping line 366330: expected 15 fields, saw 22\n",
      "Skipping line 367649: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 399174: expected 15 fields, saw 22\n",
      "Skipping line 414439: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 473579: expected 15 fields, saw 22\n",
      "Skipping line 483540: expected 15 fields, saw 22\n",
      "Skipping line 499744: expected 15 fields, saw 22\n",
      "Skipping line 505775: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 547693: expected 15 fields, saw 22\n",
      "Skipping line 561254: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 609329: expected 15 fields, saw 22\n",
      "Skipping line 642814: expected 15 fields, saw 22\n",
      "Skipping line 643189: expected 15 fields, saw 22\n",
      "Skipping line 647075: expected 15 fields, saw 22\n",
      "Skipping line 647457: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 660868: expected 15 fields, saw 22\n",
      "Skipping line 668514: expected 15 fields, saw 22\n",
      "Skipping line 673314: expected 15 fields, saw 22\n",
      "Skipping line 700416: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 723492: expected 15 fields, saw 22\n",
      "Skipping line 725052: expected 15 fields, saw 22\n",
      "Skipping line 726222: expected 15 fields, saw 22\n",
      "Skipping line 744078: expected 15 fields, saw 22\n",
      "Skipping line 753129: expected 15 fields, saw 22\n",
      "Skipping line 758347: expected 15 fields, saw 22\n",
      "Skipping line 759076: expected 15 fields, saw 22\n",
      "Skipping line 759139: expected 15 fields, saw 22\n",
      "Skipping line 768106: expected 15 fields, saw 22\n",
      "Skipping line 777835: expected 15 fields, saw 22\n",
      "Skipping line 779763: expected 15 fields, saw 22\n",
      "Skipping line 781395: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 787023: expected 15 fields, saw 22\n",
      "Skipping line 811679: expected 15 fields, saw 22\n",
      "Skipping line 811739: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 855784: expected 15 fields, saw 22\n",
      "Skipping line 878325: expected 15 fields, saw 22\n",
      "Skipping line 886822: expected 15 fields, saw 22\n",
      "Skipping line 890742: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 919607: expected 15 fields, saw 22\n",
      "Skipping line 920655: expected 15 fields, saw 22\n",
      "Skipping line 923107: expected 15 fields, saw 22\n",
      "Skipping line 930890: expected 15 fields, saw 22\n",
      "Skipping line 932841: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1003214: expected 15 fields, saw 22\n",
      "Skipping line 1007588: expected 15 fields, saw 22\n",
      "Skipping line 1018374: expected 15 fields, saw 22\n",
      "Skipping line 1022909: expected 15 fields, saw 22\n",
      "Skipping line 1030983: expected 15 fields, saw 22\n",
      "Skipping line 1048441: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1056292: expected 15 fields, saw 22\n",
      "Skipping line 1056518: expected 15 fields, saw 22\n",
      "Skipping line 1073064: expected 15 fields, saw 22\n",
      "Skipping line 1088887: expected 15 fields, saw 22\n",
      "Skipping line 1103881: expected 15 fields, saw 22\n",
      "Skipping line 1111021: expected 15 fields, saw 22\n",
      "Skipping line 1111314: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1119421: expected 15 fields, saw 22\n",
      "Skipping line 1119549: expected 15 fields, saw 22\n",
      "Skipping line 1130122: expected 15 fields, saw 22\n",
      "Skipping line 1132767: expected 15 fields, saw 22\n",
      "Skipping line 1143315: expected 15 fields, saw 22\n",
      "Skipping line 1151947: expected 15 fields, saw 22\n",
      "Skipping line 1154207: expected 15 fields, saw 22\n",
      "Skipping line 1154616: expected 15 fields, saw 22\n",
      "Skipping line 1155875: expected 15 fields, saw 22\n",
      "Skipping line 1164714: expected 15 fields, saw 22\n",
      "Skipping line 1164959: expected 15 fields, saw 22\n",
      "Skipping line 1169410: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1184604: expected 15 fields, saw 22\n",
      "Skipping line 1203964: expected 15 fields, saw 22\n",
      "Skipping line 1211287: expected 15 fields, saw 22\n",
      "Skipping line 1217834: expected 15 fields, saw 22\n",
      "Skipping line 1235346: expected 15 fields, saw 22\n",
      "Skipping line 1238073: expected 15 fields, saw 22\n",
      "Skipping line 1238439: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1246837: expected 15 fields, saw 22\n",
      "Skipping line 1263235: expected 15 fields, saw 22\n",
      "Skipping line 1265620: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1312400: expected 15 fields, saw 22\n",
      "Skipping line 1314122: expected 15 fields, saw 22\n",
      "Skipping line 1319707: expected 15 fields, saw 22\n",
      "Skipping line 1337672: expected 15 fields, saw 22\n",
      "Skipping line 1343961: expected 15 fields, saw 22\n",
      "Skipping line 1346372: expected 15 fields, saw 22\n",
      "Skipping line 1358447: expected 15 fields, saw 22\n",
      "Skipping line 1370844: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1406108: expected 15 fields, saw 22\n",
      "Skipping line 1435069: expected 15 fields, saw 22\n",
      "Skipping line 1439866: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1442123: expected 15 fields, saw 22\n",
      "Skipping line 1463237: expected 15 fields, saw 22\n",
      "Skipping line 1469027: expected 15 fields, saw 22\n",
      "Skipping line 1469598: expected 15 fields, saw 22\n",
      "Skipping line 1482636: expected 15 fields, saw 22\n",
      "Skipping line 1484745: expected 15 fields, saw 22\n",
      "Skipping line 1499831: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1508882: expected 15 fields, saw 22\n",
      "Skipping line 1514887: expected 15 fields, saw 22\n",
      "Skipping line 1527564: expected 15 fields, saw 22\n",
      "Skipping line 1569519: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1583105: expected 15 fields, saw 22\n",
      "Skipping line 1604380: expected 15 fields, saw 22\n",
      "Skipping line 1607380: expected 15 fields, saw 22\n",
      "Skipping line 1631601: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1642095: expected 15 fields, saw 22\n",
      "Skipping line 1646714: expected 15 fields, saw 22\n",
      "Skipping line 1655248: expected 15 fields, saw 22\n",
      "Skipping line 1657807: expected 15 fields, saw 22\n",
      "Skipping line 1667534: expected 15 fields, saw 22\n",
      "Skipping line 1668489: expected 15 fields, saw 22\n",
      "Skipping line 1691733: expected 15 fields, saw 22\n",
      "Skipping line 1701102: expected 15 fields, saw 22\n",
      "Skipping line 1701499: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1704450: expected 15 fields, saw 22\n",
      "Skipping line 1706154: expected 15 fields, saw 22\n",
      "Skipping line 1712789: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1773984: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1846441: expected 15 fields, saw 22\n",
      "Skipping line 1848019: expected 15 fields, saw 22\n",
      "Skipping line 1856015: expected 15 fields, saw 22\n",
      "Skipping line 1858248: expected 15 fields, saw 22\n",
      "Skipping line 1859629: expected 15 fields, saw 22\n",
      "Skipping line 1873117: expected 15 fields, saw 22\n",
      "Skipping line 1894414: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1902421: expected 15 fields, saw 22\n",
      "Skipping line 1909201: expected 15 fields, saw 22\n",
      "Skipping line 1914394: expected 15 fields, saw 22\n",
      "Skipping line 1936976: expected 15 fields, saw 22\n",
      "Skipping line 1940327: expected 15 fields, saw 22\n",
      "Skipping line 1945664: expected 15 fields, saw 22\n",
      "Skipping line 1946171: expected 15 fields, saw 22\n",
      "Skipping line 1946284: expected 15 fields, saw 22\n",
      "Skipping line 1946835: expected 15 fields, saw 22\n",
      "Skipping line 1952446: expected 15 fields, saw 22\n",
      "Skipping line 1953387: expected 15 fields, saw 22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 1979093: expected 15 fields, saw 22\n",
      "Skipping line 1982997: expected 15 fields, saw 22\n",
      "Skipping line 1992924: expected 15 fields, saw 22\n",
      "Skipping line 1996161: expected 15 fields, saw 22\n",
      "Skipping line 2003175: expected 15 fields, saw 22\n",
      "Skipping line 2024153: expected 15 fields, saw 22\n",
      "Skipping line 2026345: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2041159: expected 15 fields, saw 22\n",
      "Skipping line 2042954: expected 15 fields, saw 22\n",
      "Skipping line 2044244: expected 15 fields, saw 22\n",
      "Skipping line 2047949: expected 15 fields, saw 22\n",
      "Skipping line 2051022: expected 15 fields, saw 22\n",
      "Skipping line 2052365: expected 15 fields, saw 22\n",
      "Skipping line 2064460: expected 15 fields, saw 22\n",
      "Skipping line 2077010: expected 15 fields, saw 22\n",
      "Skipping line 2083893: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2097514: expected 15 fields, saw 22\n",
      "Skipping line 2100479: expected 15 fields, saw 22\n",
      "Skipping line 2103183: expected 15 fields, saw 22\n",
      "Skipping line 2108608: expected 15 fields, saw 22\n",
      "Skipping line 2116577: expected 15 fields, saw 22\n",
      "Skipping line 2127375: expected 15 fields, saw 22\n",
      "Skipping line 2128053: expected 15 fields, saw 22\n",
      "Skipping line 2135954: expected 15 fields, saw 22\n",
      "Skipping line 2137154: expected 15 fields, saw 22\n",
      "Skipping line 2140279: expected 15 fields, saw 22\n",
      "Skipping line 2150764: expected 15 fields, saw 22\n",
      "Skipping line 2151464: expected 15 fields, saw 22\n",
      "Skipping line 2151588: expected 15 fields, saw 22\n",
      "Skipping line 2157049: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2163762: expected 15 fields, saw 22\n",
      "Skipping line 2167939: expected 15 fields, saw 22\n",
      "Skipping line 2172050: expected 15 fields, saw 22\n",
      "Skipping line 2177960: expected 15 fields, saw 22\n",
      "Skipping line 2202813: expected 15 fields, saw 22\n",
      "Skipping line 2207828: expected 15 fields, saw 22\n",
      "Skipping line 2211189: expected 15 fields, saw 22\n",
      "Skipping line 2211589: expected 15 fields, saw 22\n",
      "Skipping line 2214034: expected 15 fields, saw 22\n",
      "Skipping line 2214264: expected 15 fields, saw 22\n",
      "Skipping line 2214462: expected 15 fields, saw 22\n",
      "Skipping line 2215027: expected 15 fields, saw 22\n",
      "Skipping line 2215639: expected 15 fields, saw 22\n",
      "Skipping line 2216007: expected 15 fields, saw 22\n",
      "Skipping line 2217132: expected 15 fields, saw 22\n",
      "Skipping line 2226703: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2231683: expected 15 fields, saw 22\n",
      "Skipping line 2245222: expected 15 fields, saw 22\n",
      "Skipping line 2256136: expected 15 fields, saw 22\n",
      "Skipping line 2269399: expected 15 fields, saw 22\n",
      "Skipping line 2283979: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2340899: expected 15 fields, saw 22\n",
      "Skipping line 2342134: expected 15 fields, saw 22\n",
      "Skipping line 2342748: expected 15 fields, saw 22\n",
      "Skipping line 2348402: expected 15 fields, saw 22\n",
      "Skipping line 2355164: expected 15 fields, saw 22\n",
      "Skipping line 2357020: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2366077: expected 15 fields, saw 22\n",
      "Skipping line 2366997: expected 15 fields, saw 22\n",
      "Skipping line 2367353: expected 15 fields, saw 22\n",
      "Skipping line 2414691: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2464571: expected 15 fields, saw 22\n",
      "Skipping line 2466302: expected 15 fields, saw 22\n",
      "Skipping line 2487679: expected 15 fields, saw 22\n",
      "Skipping line 2487771: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2506605: expected 15 fields, saw 22\n",
      "Skipping line 2511369: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2558281: expected 15 fields, saw 22\n",
      "Skipping line 2607202: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2625718: expected 15 fields, saw 22\n",
      "Skipping line 2640978: expected 15 fields, saw 22\n",
      "Skipping line 2650635: expected 15 fields, saw 22\n",
      "Skipping line 2670724: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2690954: expected 15 fields, saw 22\n",
      "Skipping line 2713810: expected 15 fields, saw 22\n",
      "Skipping line 2715292: expected 15 fields, saw 22\n",
      "Skipping line 2724453: expected 15 fields, saw 22\n",
      "Skipping line 2724458: expected 15 fields, saw 22\n",
      "Skipping line 2735678: expected 15 fields, saw 22\n",
      "Skipping line 2740358: expected 15 fields, saw 22\n",
      "Skipping line 2751188: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2763890: expected 15 fields, saw 22\n",
      "Skipping line 2766982: expected 15 fields, saw 22\n",
      "Skipping line 2813747: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2819306: expected 15 fields, saw 22\n",
      "Skipping line 2883075: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 2975635: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 3391761: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 3474241: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 3690054: expected 15 fields, saw 22\n",
      "Skipping line 3720113: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 3763182: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 4929700: expected 15 fields, saw 22\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>1797882</td>\n",
       "      <td>R3I2DHQBR577SS</td>\n",
       "      <td>B001ANOOOE</td>\n",
       "      <td>2102612</td>\n",
       "      <td>The Naked Bee Vitmin C Moisturizing Sunscreen ...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>18381298</td>\n",
       "      <td>R1QNE9NQFJC2Y4</td>\n",
       "      <td>B0016J22EQ</td>\n",
       "      <td>106393691</td>\n",
       "      <td>Alba Botanica Sunless Tanning Lotion, 4 Ounce</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Thank you Alba Bontanica!</td>\n",
       "      <td>The great thing about this cream is that it do...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>19242472</td>\n",
       "      <td>R3LIDG2Q4LJBAO</td>\n",
       "      <td>B00HU6UQAG</td>\n",
       "      <td>375449471</td>\n",
       "      <td>Elysee Infusion Skin Therapy Elixir, 2oz.</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>19551372</td>\n",
       "      <td>R3KSZHPAEVPEAL</td>\n",
       "      <td>B002HWS7RM</td>\n",
       "      <td>255651889</td>\n",
       "      <td>Diane D722 Color, Perm And Conditioner Process...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>GOOD DEAL!</td>\n",
       "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>14802407</td>\n",
       "      <td>RAI2OIG50KZ43</td>\n",
       "      <td>B00SM99KWU</td>\n",
       "      <td>116158747</td>\n",
       "      <td>Biore UV Aqua Rich Watery Essence SPF50+/PA+++...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>this soaks in quick and provides a nice base f...</td>\n",
       "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace customer_id       review_id  product_id product_parent  \\\n",
       "0          US     1797882  R3I2DHQBR577SS  B001ANOOOE        2102612   \n",
       "1          US    18381298  R1QNE9NQFJC2Y4  B0016J22EQ      106393691   \n",
       "2          US    19242472  R3LIDG2Q4LJBAO  B00HU6UQAG      375449471   \n",
       "3          US    19551372  R3KSZHPAEVPEAL  B002HWS7RM      255651889   \n",
       "4          US    14802407   RAI2OIG50KZ43  B00SM99KWU      116158747   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  The Naked Bee Vitmin C Moisturizing Sunscreen ...           Beauty   \n",
       "1      Alba Botanica Sunless Tanning Lotion, 4 Ounce           Beauty   \n",
       "2          Elysee Infusion Skin Therapy Elixir, 2oz.           Beauty   \n",
       "3  Diane D722 Color, Perm And Conditioner Process...           Beauty   \n",
       "4  Biore UV Aqua Rich Watery Essence SPF50+/PA+++...           Beauty   \n",
       "\n",
       "  star_rating helpful_votes total_votes vine verified_purchase  \\\n",
       "0           5             0           0    N                 Y   \n",
       "1           5             0           0    N                 Y   \n",
       "2           5             0           0    N                 Y   \n",
       "3           5             0           0    N                 Y   \n",
       "4           5             0           0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                         Five Stars   \n",
       "1                          Thank you Alba Bontanica!   \n",
       "2                                         Five Stars   \n",
       "3                                         GOOD DEAL!   \n",
       "4  this soaks in quick and provides a nice base f...   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                   Love this, excellent sun block!!  2015-08-31  \n",
       "1  The great thing about this cream is that it do...  2015-08-31  \n",
       "2  Great Product, I'm 65 years old and this is al...  2015-08-31  \n",
       "3  I use them as shower caps & conditioning caps....  2015-08-31  \n",
       "4  This is my go-to daily sunblock. It leaves no ...  2015-08-31  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting dataset and displaying first 5 rows\n",
    "df= pd.read_csv('./data.tsv', sep='\\t', dtype=str, lineterminator='\\n', error_bad_lines=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "k23mVQXI04jy"
   },
   "outputs": [],
   "source": [
    "# Keeping only reviews and ratings\n",
    "df = df[['review_body','star_rating']]\n",
    "\n",
    "# removing rows with null values\n",
    "df=df.dropna()\n",
    "df=df.reset_index(drop=True)\n",
    "\n",
    "# Changing rating field datatype to int\n",
    "df['star_rating']=df['star_rating'].astype(int)\n",
    "\n",
    "# Rename to rating and reviews for easiness\n",
    "df = df.rename(columns={'review_body': 'review', 'star_rating': 'rating'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u14p82TV04rS",
    "outputId": "a955b872-5c79-4b1a-df7d-ee8f560d3586"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    20000\n",
       "2    20000\n",
       "3    20000\n",
       "4    20000\n",
       "5    20000\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a balanced dataset of of 100,000 rows of 20,000 rows for each rating (1-5)\n",
    "\n",
    "k=3 # random seed value\n",
    "df1 = df.query(\"rating == 1\").sample(n=20000,random_state=k)\n",
    "df2 = df.query(\"rating == 2\").sample(n=20000,random_state=k)\n",
    "df3 = df.query(\"rating == 3\").sample(n=20000,random_state=k)\n",
    "df4 = df.query(\"rating == 4\").sample(n=20000,random_state=k)\n",
    "df5 = df.query(\"rating == 5\").sample(n=20000,random_state=k)\n",
    "df=pd.concat([df1, df2, df3, df4, df5])\n",
    "df.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bW8NeTmReEAA"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LStKV0g204w6",
    "outputId": "a6f023d3-d068-4377-e927-f09d5de0dcf2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Remove  all HTML and URLS from reviews\n",
    "df['review']=df['review'].apply(lambda x: BeautifulSoup(x).get_text())\n",
    "df['review']=df['review'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LYx8Pzda5yvA"
   },
   "outputs": [],
   "source": [
    "# perform contractions\n",
    "# Used the contractions package to perform contractions\n",
    "df.review=df.review.astype(str)\n",
    "df['review'] = df['review'].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zqn_zemL55ma",
    "outputId": "e413644a-2ffb-42f6-de25-5419f522c5a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_496\\3446312180.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['review']=df['review'].str.replace('[^a-zA-Z ]', '')\n"
     ]
    }
   ],
   "source": [
    "# Remove non alphabetical characters\n",
    "df['review']=df['review'].str.replace('[^a-zA-Z ]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k6Lzs5Wm55tM"
   },
   "outputs": [],
   "source": [
    "# Remove leading and trailing spaces\n",
    "df['review'] = df['review'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "LKPTt-Er04ze",
    "outputId": "a039ddc2-dc2c-4fdf-f9f6-14c85648950d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198266</th>\n",
       "      <td>Note the blurry picture  That is to disguise t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119369</th>\n",
       "      <td>Too small and thin consistency</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672827</th>\n",
       "      <td>This was the worst purchase I have ever made T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249575</th>\n",
       "      <td>NEVER GOT MY ORDER</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235550</th>\n",
       "      <td>Do Not waste your time  Mine never came on and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    review  rating\n",
       "198266   Note the blurry picture  That is to disguise t...       1\n",
       "2119369                     Too small and thin consistency       1\n",
       "672827   This was the worst purchase I have ever made T...       1\n",
       "2249575                                 NEVER GOT MY ORDER       1\n",
       "2235550  Do Not waste your time  Mine never came on and...       1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aT7DmoWh3wmm"
   },
   "source": [
    "## Saving cleaned dataset to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PgQuc3i074zV"
   },
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "df.to_csv(\"cleanedDataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iw7roisF4KbI"
   },
   "source": [
    "## Loading cleaned dataset and cleaning some more in case of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "J5ANbn6GBO15"
   },
   "outputs": [],
   "source": [
    "dfc = pd.read_csv(\"cleanedDataset.csv\")\n",
    "dfc = dfc.loc[:, ~dfc.columns.str.match('Unnamed')] # Remove \"Unnamed\" index column which forms in saved csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhmmEJs2CQBk",
    "outputId": "d2d5eb39-e62e-44e7-deaf-e7e923b1181f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sometimes null values comeup after loading (mostly for csv files stored in google drive)\n",
    "dfc['review'].isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PsK7Y2V65bv3"
   },
   "outputs": [],
   "source": [
    "# Remove null values in case formed\n",
    "dfc=dfc.dropna()\n",
    "dfc=dfc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vhSMxal9BnuR"
   },
   "outputs": [],
   "source": [
    "# dfc['review']=dfc['review'].astype(str)\n",
    "# Dividing into X and Y numpy arrays based on reviews and labels\n",
    "X = dfc['review'].to_numpy()\n",
    "Y = dfc['rating'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Au_lwd8L7Lzw",
    "outputId": "8807e0fc-f5c2-42c3-8cd0-cee8d25dcff9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Note the blurry picture  That is to disguise the fact that these are tiny travel sizes  Yes I thought it seemed to good to be true but as an extensive online shopper I have found numbers of amazing deals in the past This is not  Basically I paid eight bucks for  dollars worth of conditioner  Good product but not from this seller',\n",
       "       'Too small and thin consistency',\n",
       "       'This was the worst purchase I have ever made The shapes do not stay They are completely blurry or just blobs',\n",
       "       ...,\n",
       "       'Applies smoothly feels wonderful Makeup does not crumble or fade It is slightly tinted but not so much that you really have to worry about what color it is I do not think it is thick enough to matter',\n",
       "       'Genuine Product Shipped in time It smells great Perfect gift to your datespouse',\n",
       "       'Not much to say about this product except for the fact that it works and keeps an expensive electric razor running like new  I have owned Norelco and Braun razors for the last  years and the current Norelco is the best yet  This kit has kept it running in top shape'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3aaRgC9eFHbd"
   },
   "outputs": [],
   "source": [
    "# Converting review which is a string to a list of tokenized words for training our own word2vec model later\n",
    "from nltk.tokenize import word_tokenize\n",
    "for i in range(len(X)):\n",
    "    X[i] = [t for t in word_tokenize(X[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2TRH-kkJ7uFQ",
    "outputId": "9f4596a5-627e-4095-ace6-9aa4b1d94f36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Note', 'the', 'blurry', 'picture', 'That', 'is', 'to', 'disguise', 'the', 'fact', 'that', 'these', 'are', 'tiny', 'travel', 'sizes', 'Yes', 'I', 'thought', 'it', 'seemed', 'to', 'good', 'to', 'be', 'true', 'but', 'as', 'an', 'extensive', 'online', 'shopper', 'I', 'have', 'found', 'numbers', 'of', 'amazing', 'deals', 'in', 'the', 'past', 'This', 'is', 'not', 'Basically', 'I', 'paid', 'eight', 'bucks', 'for', 'dollars', 'worth', 'of', 'conditioner', 'Good', 'product', 'but', 'not', 'from', 'this', 'seller']),\n",
       "       list(['Too', 'small', 'and', 'thin', 'consistency']),\n",
       "       list(['This', 'was', 'the', 'worst', 'purchase', 'I', 'have', 'ever', 'made', 'The', 'shapes', 'do', 'not', 'stay', 'They', 'are', 'completely', 'blurry', 'or', 'just', 'blobs']),\n",
       "       ...,\n",
       "       list(['Applies', 'smoothly', 'feels', 'wonderful', 'Makeup', 'does', 'not', 'crumble', 'or', 'fade', 'It', 'is', 'slightly', 'tinted', 'but', 'not', 'so', 'much', 'that', 'you', 'really', 'have', 'to', 'worry', 'about', 'what', 'color', 'it', 'is', 'I', 'do', 'not', 'think', 'it', 'is', 'thick', 'enough', 'to', 'matter']),\n",
       "       list(['Genuine', 'Product', 'Shipped', 'in', 'time', 'It', 'smells', 'great', 'Perfect', 'gift', 'to', 'your', 'datespouse']),\n",
       "       list(['Not', 'much', 'to', 'say', 'about', 'this', 'product', 'except', 'for', 'the', 'fact', 'that', 'it', 'works', 'and', 'keeps', 'an', 'expensive', 'electric', 'razor', 'running', 'like', 'new', 'I', 'have', 'owned', 'Norelco', 'and', 'Braun', 'razors', 'for', 'the', 'last', 'years', 'and', 'the', 'current', 'Norelco', 'is', 'the', 'best', 'yet', 'This', 'kit', 'has', 'kept', 'it', 'running', 'in', 'top', 'shape'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VMeMqn6GEBWD"
   },
   "outputs": [],
   "source": [
    "# To lower RAM usage so that  the session doesn't crash\n",
    "df=0\n",
    "dfc=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6IKKxvdeNmv"
   },
   "source": [
    "# 2. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZHwBbe6A0u_"
   },
   "source": [
    "## a) Using pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8dfJwdZS042S"
   },
   "outputs": [],
   "source": [
    "# Loading pretrained “word2vec-google-news-300” Word2Vec model\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TS2vmgArH12k",
    "outputId": "0863a447-8ca4-46c5-f357-e7b0b3117876"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of words (3,000,000) of vocabulary for pretrained model. Runtime always disconnected when tried to display all the words.\n",
    "len(wv.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOb4wGNt83Re"
   },
   "source": [
    "Checking semantic similarities using three examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33lOnx1J044d",
    "outputId": "e819ef04-9156-4965-9951-385f5ee06c46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7332698\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "# husband - man + woman = wife\n",
    "# Finding cosine similarity using numpy\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "vec_hus = wv['husband']\n",
    "vec_man = wv['man']\n",
    "test_vec = (vec_hus - vec_man) + wv['woman']\n",
    "wife_vec = wv['wife']\n",
    "result = dot(test_vec, wife_vec)/(norm(test_vec)*norm(wife_vec))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VOdZDZq0472",
    "outputId": "e298e84e-d96d-4534-c23c-b37730fa956e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8411032"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2\n",
    "wv.similarity('excited','thrilled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMKqBQuZ96iJ",
    "outputId": "25c974b4-1e0c-4563-d42c-cfc436d2b3d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57388663"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 3\n",
    "wv.similarity('appropriate','suitable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DypvKDNAsUY"
   },
   "source": [
    "## b) Using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-qpasBwAQ39"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=X, size=300, window=11, min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ru79Z-0mEKZN",
    "outputId": "e9bf2f97-ec80-468e-e38f-730af3e6a1ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7475"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of words (7475) of vocabulary for our trained model\n",
    "len(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obPD7ZFNFxt0",
    "outputId": "9050490e-6e81-400a-f8e8-71d99ba98dcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70438445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "# husband - man + woman = wife\n",
    "# Finding cosine similarity using numpy\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "vec_hus = model['husband']\n",
    "vec_man = model['man']\n",
    "test_vec = (vec_hus - vec_man) + model['woman']\n",
    "wife_vec = model['wife']\n",
    "result = dot(test_vec, wife_vec)/(norm(test_vec)*norm(wife_vec))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_9g9s4jGFGD",
    "outputId": "5969945b-37fa-4650-d851-0738718d33dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55498016"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2\n",
    "model.similarity('excited','thrilled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqNzJZcvGFrw",
    "outputId": "60a8f515-67ca-44a0-ad4f-3c7aa827603d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8048134"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 3\n",
    "model.similarity('appropriate','suitable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL2zVQVIG2Ao"
   },
   "source": [
    "# Questions:\n",
    "What do you conclude from comparing vectors\n",
    "generated by yourself and the pretrained model? Which of the Word2Vec\n",
    "models seems to encode semantic similarities between words better?\n",
    "# Answers:\n",
    "It seems that the pretrained Word2vec model encodes semantic similarities better than my trained model.I think the pretrained model seems to encode similarities better than my model, as it has gained a lot of information from all the words it has been trained on.<br>\n",
    "I can see that  the there are a lot more vocabulary keys generated by the pretrained word2vec model. This is because it is trained on a very large dataset for a long period of time which makes it a more accurate model for getting word embeddings for many different words. Word2Vec is trained on the Google News dataset (about 100 billion words). It has several use cases such as Recommendation Engines, Knowledge Discovery, and the one we are using it for, Text Classification.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVu_IpCvG5t3"
   },
   "source": [
    "# 3. Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je6JIJdiMUZ_"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYIhHuUgGOyA"
   },
   "outputs": [],
   "source": [
    "# Converting reviews with tokenized words back to sentences for TF-IDF\n",
    "Xtf = copy.deepcopy(X)\n",
    "for i in range(len(X)):\n",
    "  Xtf[i] = ' '.join(Xtf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FbU5897OGl4P",
    "outputId": "628e0368-2095-48eb-a1ec-69ec2414edb9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'not happy with the present way to small they should really put the size as is not make it times bigger in the picture'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Back to a sentence\n",
    "Xtf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z764wU7_HPwt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word')\n",
    "Xtf = tfidfvectorizer.fit_transform(Xtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "we6xeRWyHbER"
   },
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "Xtf_train, Xtf_test, Ytf_train, Ytf_test = train_test_split(Xtf, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "btk2Z3pKR28Z",
    "outputId": "8e2f36a0-dc09-48f0-cf05-6fc9ad542061"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79967, 46508)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtf_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Q_vfI_MMY93"
   },
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFF_4wgZHeqY",
    "outputId": "36f45a07-4ba9-49e5-afaa-3e499307b33a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "Xw2v_mean = copy.deepcopy(X)\n",
    "words_in_wv = set(wv.wv.vocab.keys()) # Making a set of all the word2vec words in the pretrained model\n",
    "\n",
    "# taking average of all word vectors in a review only if their word vectors exist in the pretrained model\n",
    "for i in range(len(Xw2v_mean)):\n",
    "  Xw2v_mean[i] = np.mean(np.array([wv[w] for w in Xw2v_mean[i] if w in words_in_wv]), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AMYC0qcHkcn",
    "outputId": "1f5613b4-bdbd-45ee-9255-0bdebfabb9f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All word have 300 different magnitude values in their vectors or embeddings\n",
    "# Finding out number of reviews which have number of vector values not equal to 300 \n",
    "len(Xw2v_mean[[y.size != 300 for y in Xw2v_mean]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9TFDEcjSa0w"
   },
   "outputs": [],
   "source": [
    "# Removing indices which have less than 300 vector values\n",
    "indices = np.array([y.size==300 for y in Xw2v_mean])\n",
    "Xw2v_mean = Xw2v_mean[indices]\n",
    "Yw2v_mean = Y[indices] # getting labels with those indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a0i80tLNxJg",
    "outputId": "6c2e763c-1d10-4a9e-ad56-adacec7a07b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking again\n",
    "len(Xw2v_mean[[y.size != 300 for y in Xw2v_mean]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvBYm1dbOyu5",
    "outputId": "1258270e-0913-4f14-9a61-fb72e8e0b34d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99899,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xw2v_mean.shape # earlier it was (99959,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hU4GfqNFPE0K"
   },
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "Xw2v_train, Xw2v_test, Yw2v_train, Yw2v_test = train_test_split(Xw2v_mean, Yw2v_mean, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tMx523OR6r9",
    "outputId": "81acae39-ec4e-4897-a7bc-8b0bd07a3dc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xw2v_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkO8cX-WPfgd"
   },
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9I61JirZPhdl",
    "outputId": "5d543806-fdf7-4bad-ae3c-2ef5bad39d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.46      0.62      0.53      4013\n",
      "           2       0.32      0.23      0.27      4050\n",
      "           3       0.33      0.32      0.33      4035\n",
      "           4       0.38      0.36      0.37      3977\n",
      "           5       0.56      0.59      0.57      3917\n",
      "\n",
      "    accuracy                           0.42     19992\n",
      "   macro avg       0.41      0.42      0.41     19992\n",
      "weighted avg       0.41      0.42      0.41     19992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting results with TF-IDF features\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "clf_tf = Perceptron(tol=1e-3, random_state=3)\n",
    "clf_tf.fit(Xtf_train, Ytf_train)\n",
    "y_pred = clf_tf.predict(Xtf_test)\n",
    "print(classification_report(Ytf_test, y_pred, target_names=[\"1\",\"2\",\"3\",\"4\",\"5\"], output_dict = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSh8CyZdQJe3",
    "outputId": "4b25f0d9-7cb2-4597-d250-4916c429b8a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.37      0.45      4012\n",
      "           2       0.36      0.29      0.32      4049\n",
      "           3       0.40      0.05      0.08      3999\n",
      "           4       0.31      0.04      0.06      4073\n",
      "           5       0.28      0.94      0.43      3847\n",
      "\n",
      "    accuracy                           0.33     19980\n",
      "   macro avg       0.39      0.34      0.27     19980\n",
      "weighted avg       0.39      0.33      0.27     19980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting results with Word2vec features\n",
    "clf_w2v = Perceptron(tol=1e-3, random_state=3)\n",
    "clf_w2v.fit(Xw2v_train.tolist(), Yw2v_train) # converting to nparray of list as it was an np array of objects with datatype 'object' which needs to be float\n",
    "y_pred = clf_w2v.predict(Xw2v_test.tolist())\n",
    "print(classification_report(Yw2v_test, y_pred, target_names=[\"1\",\"2\",\"3\",\"4\",\"5\"], output_dict = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_id4mfDQaQN"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MizC9C72QWpo",
    "outputId": "7f7679e4-40de-46ca-df6f-216bde678ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.66      0.60      4013\n",
      "           2       0.39      0.32      0.35      4050\n",
      "           3       0.42      0.39      0.41      4035\n",
      "           4       0.45      0.41      0.43      3977\n",
      "           5       0.63      0.73      0.68      3917\n",
      "\n",
      "    accuracy                           0.50     19992\n",
      "   macro avg       0.49      0.50      0.49     19992\n",
      "weighted avg       0.49      0.50      0.49     19992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting results with Tf-IDF features\n",
    "from sklearn.svm import LinearSVC\n",
    "SVM_tf = LinearSVC(random_state = 0)\n",
    "SVM_tf.fit(Xtf_train,Ytf_train)\n",
    "y_pred = SVM_tf.predict(Xtf_test)\n",
    "print(classification_report(Ytf_test, y_pred, target_names=[\"1\",\"2\",\"3\",\"4\",\"5\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxL1WG_0QhEs",
    "outputId": "47366db8-7341-4c64-f143-63f1444ac795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.71      0.59      4012\n",
      "           2       0.39      0.25      0.30      4049\n",
      "           3       0.40      0.38      0.39      3999\n",
      "           4       0.46      0.30      0.36      4073\n",
      "           5       0.57      0.76      0.65      3847\n",
      "\n",
      "    accuracy                           0.48     19980\n",
      "   macro avg       0.46      0.48      0.46     19980\n",
      "weighted avg       0.46      0.48      0.46     19980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting results with Word2vec features\n",
    "SVM_w2v = LinearSVC(random_state = 0)\n",
    "SVM_w2v.fit(Xw2v_train.tolist(),Yw2v_train)\n",
    "y_pred = SVM_w2v.predict(Xw2v_test.tolist())\n",
    "print(classification_report(Yw2v_test, y_pred, target_names=[\"1\",\"2\",\"3\",\"4\",\"5\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV-o-a_TXnqr"
   },
   "source": [
    "Accuracy values:\n",
    "<br>\n",
    "Perceptron:<br>•\n",
    "Tf-IDF : 0.42<br>•\n",
    "W2V : 0.33\n",
    "<br>\n",
    "SVM:<br>•\n",
    "Tf-IDF : 0.50<br>•\n",
    "W2V : 0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxTNIqKXPUQk"
   },
   "source": [
    "# Question\n",
    "What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?\n",
    "# Answer\n",
    "Based on my results TF-IDF performed better than Word2vec for Perceptron model, but performed about the same for SVM model. Also, SVM for W2V model took a long time to train. Its probably because it took long to decide the margin, only if that affects the training time. Overall SVM performed better than Perceptron. As perceptron only does well with linearly separable datapoints.<br>\n",
    "There are 46508 features per review in TF-IDF, whereas there are 300 features in W2V, which we got from averaging all the words in a review. The reason why W2V performed worse could be that the averaging of word vector values makes the feature lose information which connects it to the label and feeding this data to a simple model sucha as a Perceptron won't give a good accuracy. A RNN could produce better results for this five class classification task. Also, TF-IDF is a statistical measure that we can apply to terms in a document and then use that to form a vector. So, its method is much more better as it is specific to this dataset. Whereas, W2V word embeddings are coming from a pretrained vector which has a large amount of information which may not be specific  to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vticIs5TvOG"
   },
   "outputs": [],
   "source": [
    "# To lower RAM usage so that  the session doesn't crash\n",
    "Xtf_train=0\n",
    "Xtf_test=0\n",
    "Ytf_train=0\n",
    "Ytf_test=0\n",
    "Xtf=0\n",
    "Xw2v_train=0\n",
    "Xw2v_test=0\n",
    "Yw2v_train=0\n",
    "Yw2v_test=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q4mwH2zC5HZ"
   },
   "source": [
    "# Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaDQy2Vr3FQu"
   },
   "source": [
    "## a) Averaged vectors for all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywvb8M-Im801"
   },
   "outputs": [],
   "source": [
    "# In case session crashed\n",
    "# uncomment below lines and run this cell\n",
    "# if not ignore cell\n",
    "\n",
    "# # Loading pretrained “word2vec-google-news-300” Word2Vec model\n",
    "# import gensim.downloader as api\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import copy\n",
    "# dfc = pd.read_csv(\"cleanedDataset.csv\")\n",
    "# dfc = dfc.loc[:, ~dfc.columns.str.match('Unnamed')] # Remove \"Unnamed\" index column which forms in saved csv files\n",
    "\n",
    "# # Remove null values in case formed\n",
    "# dfc=dfc.dropna()\n",
    "# dfc=dfc.reset_index(drop=True)\n",
    "\n",
    "# # dfc['review']=dfc['review'].astype(str)\n",
    "# # Dividing into X and Y numpy arrays based on reviews and labels\n",
    "# X = dfc['review'].to_numpy()\n",
    "# Y = dfc['rating'].to_numpy()\n",
    "\n",
    "# # Converting review which is a string to a list of tokenized words for training our own word2vec model later\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# for i in range(len(X)):\n",
    "#     X[i] = [t for t in word_tokenize(X[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkGstNiTfwxA"
   },
   "outputs": [],
   "source": [
    "# Taking mean w2v features from previous question and going straight to split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(Xw2v_mean, Yw2v_mean, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iq-xEwekLgsl"
   },
   "outputs": [],
   "source": [
    "# converting to nparray of list as it was an np array of objects with datatype 'object' which needs to be float\n",
    "def arrayToList(my_array):\n",
    "  return np.array(my_array.tolist())\n",
    "\n",
    "train_x = arrayToList(train_x)\n",
    "test_x = arrayToList(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvqtcfTXnMrJ"
   },
   "outputs": [],
   "source": [
    "# reducing labels from 1-5 to 0-4\n",
    "# Run this only once or you will end up subtracting two times\n",
    "test_y = test_y - 1\n",
    "train_y = train_y - 1\n",
    "\n",
    "\n",
    "# # Code to check unique counts per label\n",
    "# unique, counts = np.unique(test_y, return_counts=True)\n",
    "# dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPVgPp55LabR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# create Tensor Dataset\n",
    "# train_data=TensorDataset(torch.cuda.FloatTensor(train_x), torch.cuda.LongTensor(train_y))\n",
    "# test_data=TensorDataset(torch.cuda.FloatTensor(test_x), torch.cuda.LongTensor(test_y))\n",
    "\n",
    "# Creating non cuda TensorDataset as CUDA didn't provide type of error when it occurs. Thus, was very difficult to debug.\n",
    "train_data=TensorDataset(torch.FloatTensor(train_x), torch.LongTensor(train_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(test_x), torch.LongTensor(test_y))\n",
    "\n",
    "# dataloader\n",
    "# Using 256 batch size as per tutorial\n",
    "train_batch_size=256\n",
    "\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z16uC4DfC4Yp",
    "outputId": "cecff36e-e833-4965-a299-7979cc560eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# MLP class\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # number of hidden nodes in each layer\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (300 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 5)\n",
    "        self.fc3 = nn.Linear(hidden_2, 5)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        # initialize weights\n",
    "        # self.weight_init()\n",
    "        # was not learning so commented out weight initialization\n",
    "        # Pytorch auto updates weights and initializing step above was changing it back to initial weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        # x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        # x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def weight_init(self):\n",
    "        # Initialize weights\n",
    "        nn.init.zeros_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc3.weight)\n",
    "        nn.init.ones_(self.fc1.bias)\n",
    "        nn.init.ones_(self.fc2.bias)\n",
    "        nn.init.ones_(self.fc3.bias)\n",
    "        \n",
    "\n",
    "model = MLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAE3CMuD45rs"
   },
   "source": [
    "## Reason for using two accuracies\n",
    "Also, I am gathering two accuracies as I was unsure about my calculation of accuracy.\n",
    "The true accuracy is just named that way to differentiate between the first one and second one. Both are correct.<br>\n",
    "Validation accuracy is gathering correct predictions and dividing it by amount of data in a epoch.<br>\n",
    "The True accuracy is using sklearn metric to get accuracy per batch and in the end is averaging it over all the batches for that epoch.<br>\n",
    "Both give the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DSFtaXzC4bY",
    "outputId": "81e9ae90-2c94-40b3-8357-1c11041b155d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.620935 \tValidation Loss: 1.616416 \tValidation Acc: 0.192543 \tTrue Acc: 0.192230\n",
      "Epoch: 2 \tTraining Loss: 1.612387 \tValidation Loss: 1.611551 \tValidation Acc: 0.192543 \tTrue Acc: 0.193236\n",
      "Epoch: 3 \tTraining Loss: 1.610055 \tValidation Loss: 1.610068 \tValidation Acc: 0.192643 \tTrue Acc: 0.192329\n",
      "Epoch: 4 \tTraining Loss: 1.609388 \tValidation Loss: 1.609539 \tValidation Acc: 0.193093 \tTrue Acc: 0.191769\n",
      "Epoch: 5 \tTraining Loss: 1.609161 \tValidation Loss: 1.609287 \tValidation Acc: 0.192993 \tTrue Acc: 0.192675\n",
      "Epoch: 6 \tTraining Loss: 1.609051 \tValidation Loss: 1.609169 \tValidation Acc: 0.192643 \tTrue Acc: 0.193335\n",
      "Epoch: 7 \tTraining Loss: 1.608971 \tValidation Loss: 1.609044 \tValidation Acc: 0.194745 \tTrue Acc: 0.195411\n",
      "Epoch: 8 \tTraining Loss: 1.608896 \tValidation Loss: 1.608956 \tValidation Acc: 0.199049 \tTrue Acc: 0.196648\n",
      "Epoch: 9 \tTraining Loss: 1.608810 \tValidation Loss: 1.608852 \tValidation Acc: 0.201552 \tTrue Acc: 0.201131\n",
      "Epoch: 10 \tTraining Loss: 1.608709 \tValidation Loss: 1.608760 \tValidation Acc: 0.207608 \tTrue Acc: 0.207114\n",
      "Epoch: 11 \tTraining Loss: 1.608610 \tValidation Loss: 1.608652 \tValidation Acc: 0.221872 \tTrue Acc: 0.222211\n",
      "Epoch: 12 \tTraining Loss: 1.608501 \tValidation Loss: 1.608537 \tValidation Acc: 0.235986 \tTrue Acc: 0.237160\n",
      "Epoch: 13 \tTraining Loss: 1.608386 \tValidation Loss: 1.608431 \tValidation Acc: 0.226827 \tTrue Acc: 0.227106\n",
      "Epoch: 14 \tTraining Loss: 1.608262 \tValidation Loss: 1.608276 \tValidation Acc: 0.232382 \tTrue Acc: 0.234606\n",
      "Epoch: 15 \tTraining Loss: 1.608116 \tValidation Loss: 1.608129 \tValidation Acc: 0.219770 \tTrue Acc: 0.219129\n",
      "Epoch: 16 \tTraining Loss: 1.607961 \tValidation Loss: 1.607967 \tValidation Acc: 0.232082 \tTrue Acc: 0.230287\n",
      "Epoch: 17 \tTraining Loss: 1.607781 \tValidation Loss: 1.607779 \tValidation Acc: 0.252152 \tTrue Acc: 0.253132\n",
      "Epoch: 18 \tTraining Loss: 1.607575 \tValidation Loss: 1.607549 \tValidation Acc: 0.266517 \tTrue Acc: 0.270339\n",
      "Epoch: 19 \tTraining Loss: 1.607341 \tValidation Loss: 1.607306 \tValidation Acc: 0.266567 \tTrue Acc: 0.266367\n",
      "Epoch: 20 \tTraining Loss: 1.607070 \tValidation Loss: 1.607030 \tValidation Acc: 0.291942 \tTrue Acc: 0.290431\n",
      "Epoch: 21 \tTraining Loss: 1.606758 \tValidation Loss: 1.606689 \tValidation Acc: 0.301051 \tTrue Acc: 0.300435\n",
      "Epoch: 22 \tTraining Loss: 1.606395 \tValidation Loss: 1.606306 \tValidation Acc: 0.305355 \tTrue Acc: 0.306698\n",
      "Epoch: 23 \tTraining Loss: 1.605970 \tValidation Loss: 1.605859 \tValidation Acc: 0.319720 \tTrue Acc: 0.319884\n",
      "Epoch: 24 \tTraining Loss: 1.605473 \tValidation Loss: 1.605322 \tValidation Acc: 0.323323 \tTrue Acc: 0.324449\n",
      "Epoch: 25 \tTraining Loss: 1.604881 \tValidation Loss: 1.604667 \tValidation Acc: 0.333534 \tTrue Acc: 0.332526\n",
      "Epoch: 26 \tTraining Loss: 1.604168 \tValidation Loss: 1.603882 \tValidation Acc: 0.339039 \tTrue Acc: 0.336959\n",
      "Epoch: 27 \tTraining Loss: 1.603302 \tValidation Loss: 1.602947 \tValidation Acc: 0.335886 \tTrue Acc: 0.332839\n",
      "Epoch: 28 \tTraining Loss: 1.602238 \tValidation Loss: 1.601777 \tValidation Acc: 0.348048 \tTrue Acc: 0.348876\n",
      "Epoch: 29 \tTraining Loss: 1.600924 \tValidation Loss: 1.600322 \tValidation Acc: 0.348248 \tTrue Acc: 0.348068\n",
      "Epoch: 30 \tTraining Loss: 1.599271 \tValidation Loss: 1.598492 \tValidation Acc: 0.350200 \tTrue Acc: 0.353013\n",
      "Epoch: 31 \tTraining Loss: 1.597174 \tValidation Loss: 1.596164 \tValidation Acc: 0.344294 \tTrue Acc: 0.344162\n",
      "Epoch: 32 \tTraining Loss: 1.594473 \tValidation Loss: 1.593139 \tValidation Acc: 0.343093 \tTrue Acc: 0.343981\n",
      "Epoch: 33 \tTraining Loss: 1.590955 \tValidation Loss: 1.589180 \tValidation Acc: 0.348098 \tTrue Acc: 0.345909\n",
      "Epoch: 34 \tTraining Loss: 1.586322 \tValidation Loss: 1.583978 \tValidation Acc: 0.339590 \tTrue Acc: 0.338509\n",
      "Epoch: 35 \tTraining Loss: 1.580190 \tValidation Loss: 1.577076 \tValidation Acc: 0.342693 \tTrue Acc: 0.345596\n",
      "Epoch: 36 \tTraining Loss: 1.571976 \tValidation Loss: 1.567774 \tValidation Acc: 0.337187 \tTrue Acc: 0.339152\n",
      "Epoch: 37 \tTraining Loss: 1.561060 \tValidation Loss: 1.555452 \tValidation Acc: 0.334985 \tTrue Acc: 0.335970\n",
      "Epoch: 38 \tTraining Loss: 1.546790 \tValidation Loss: 1.539658 \tValidation Acc: 0.337337 \tTrue Acc: 0.337289\n",
      "Epoch: 39 \tTraining Loss: 1.528808 \tValidation Loss: 1.520245 \tValidation Acc: 0.341091 \tTrue Acc: 0.339992\n",
      "Epoch: 40 \tTraining Loss: 1.507446 \tValidation Loss: 1.497863 \tValidation Acc: 0.344745 \tTrue Acc: 0.346618\n",
      "Epoch: 41 \tTraining Loss: 1.483876 \tValidation Loss: 1.474271 \tValidation Acc: 0.351702 \tTrue Acc: 0.352485\n",
      "Epoch: 42 \tTraining Loss: 1.459822 \tValidation Loss: 1.450793 \tValidation Acc: 0.360210 \tTrue Acc: 0.361897\n",
      "Epoch: 43 \tTraining Loss: 1.436872 \tValidation Loss: 1.428894 \tValidation Acc: 0.373323 \tTrue Acc: 0.371835\n",
      "Epoch: 44 \tTraining Loss: 1.415842 \tValidation Loss: 1.409887 \tValidation Acc: 0.383684 \tTrue Acc: 0.384082\n",
      "Epoch: 45 \tTraining Loss: 1.397231 \tValidation Loss: 1.392490 \tValidation Acc: 0.381882 \tTrue Acc: 0.384312\n",
      "Epoch: 46 \tTraining Loss: 1.381158 \tValidation Loss: 1.377342 \tValidation Acc: 0.390190 \tTrue Acc: 0.388499\n",
      "Epoch: 47 \tTraining Loss: 1.367291 \tValidation Loss: 1.364362 \tValidation Acc: 0.398198 \tTrue Acc: 0.400432\n",
      "Epoch: 48 \tTraining Loss: 1.355425 \tValidation Loss: 1.353451 \tValidation Acc: 0.405956 \tTrue Acc: 0.406085\n",
      "Epoch: 49 \tTraining Loss: 1.345162 \tValidation Loss: 1.344354 \tValidation Acc: 0.404555 \tTrue Acc: 0.403695\n",
      "Epoch: 50 \tTraining Loss: 1.336388 \tValidation Loss: 1.336107 \tValidation Acc: 0.407508 \tTrue Acc: 0.407618\n",
      "Epoch: 51 \tTraining Loss: 1.328623 \tValidation Loss: 1.330243 \tValidation Acc: 0.407608 \tTrue Acc: 0.406711\n",
      "Epoch: 52 \tTraining Loss: 1.321809 \tValidation Loss: 1.325626 \tValidation Acc: 0.413013 \tTrue Acc: 0.414062\n",
      "Epoch: 53 \tTraining Loss: 1.315744 \tValidation Loss: 1.317575 \tValidation Acc: 0.414765 \tTrue Acc: 0.415793\n",
      "Epoch: 54 \tTraining Loss: 1.310183 \tValidation Loss: 1.314822 \tValidation Acc: 0.414615 \tTrue Acc: 0.416650\n",
      "Epoch: 55 \tTraining Loss: 1.305264 \tValidation Loss: 1.308418 \tValidation Acc: 0.418869 \tTrue Acc: 0.420853\n",
      "Epoch: 56 \tTraining Loss: 1.300618 \tValidation Loss: 1.305244 \tValidation Acc: 0.417568 \tTrue Acc: 0.416551\n",
      "Epoch: 57 \tTraining Loss: 1.296409 \tValidation Loss: 1.299929 \tValidation Acc: 0.422022 \tTrue Acc: 0.422963\n",
      "Epoch: 58 \tTraining Loss: 1.292341 \tValidation Loss: 1.296160 \tValidation Acc: 0.424775 \tTrue Acc: 0.424677\n",
      "Epoch: 59 \tTraining Loss: 1.288569 \tValidation Loss: 1.292922 \tValidation Acc: 0.425976 \tTrue Acc: 0.424858\n",
      "Epoch: 60 \tTraining Loss: 1.285011 \tValidation Loss: 1.290290 \tValidation Acc: 0.427928 \tTrue Acc: 0.424776\n",
      "Epoch: 61 \tTraining Loss: 1.281663 \tValidation Loss: 1.286182 \tValidation Acc: 0.429680 \tTrue Acc: 0.430528\n",
      "Epoch: 62 \tTraining Loss: 1.278435 \tValidation Loss: 1.283547 \tValidation Acc: 0.431031 \tTrue Acc: 0.430858\n",
      "Epoch: 63 \tTraining Loss: 1.275367 \tValidation Loss: 1.280910 \tValidation Acc: 0.431532 \tTrue Acc: 0.433363\n",
      "Epoch: 64 \tTraining Loss: 1.272508 \tValidation Loss: 1.277814 \tValidation Acc: 0.433884 \tTrue Acc: 0.434682\n",
      "Epoch: 65 \tTraining Loss: 1.269785 \tValidation Loss: 1.275205 \tValidation Acc: 0.435636 \tTrue Acc: 0.432391\n",
      "Epoch: 66 \tTraining Loss: 1.267076 \tValidation Loss: 1.272761 \tValidation Acc: 0.437588 \tTrue Acc: 0.437335\n",
      "Epoch: 67 \tTraining Loss: 1.264472 \tValidation Loss: 1.270356 \tValidation Acc: 0.438238 \tTrue Acc: 0.440994\n",
      "Epoch: 68 \tTraining Loss: 1.262096 \tValidation Loss: 1.267943 \tValidation Acc: 0.440290 \tTrue Acc: 0.440005\n",
      "Epoch: 69 \tTraining Loss: 1.259678 \tValidation Loss: 1.266268 \tValidation Acc: 0.439540 \tTrue Acc: 0.438258\n",
      "Epoch: 70 \tTraining Loss: 1.257261 \tValidation Loss: 1.263810 \tValidation Acc: 0.441842 \tTrue Acc: 0.441538\n",
      "Epoch: 71 \tTraining Loss: 1.255095 \tValidation Loss: 1.261647 \tValidation Acc: 0.442392 \tTrue Acc: 0.442082\n",
      "Epoch: 72 \tTraining Loss: 1.252930 \tValidation Loss: 1.259459 \tValidation Acc: 0.445195 \tTrue Acc: 0.446862\n",
      "Epoch: 73 \tTraining Loss: 1.250675 \tValidation Loss: 1.257682 \tValidation Acc: 0.445946 \tTrue Acc: 0.444587\n",
      "Epoch: 74 \tTraining Loss: 1.248616 \tValidation Loss: 1.255485 \tValidation Acc: 0.445245 \tTrue Acc: 0.446911\n",
      "Epoch: 75 \tTraining Loss: 1.246589 \tValidation Loss: 1.253848 \tValidation Acc: 0.448649 \tTrue Acc: 0.449268\n",
      "Epoch: 76 \tTraining Loss: 1.244571 \tValidation Loss: 1.251930 \tValidation Acc: 0.449800 \tTrue Acc: 0.450405\n",
      "Epoch: 77 \tTraining Loss: 1.242708 \tValidation Loss: 1.251763 \tValidation Acc: 0.450050 \tTrue Acc: 0.449647\n",
      "Epoch: 78 \tTraining Loss: 1.240829 \tValidation Loss: 1.249482 \tValidation Acc: 0.452803 \tTrue Acc: 0.451361\n",
      "Epoch: 79 \tTraining Loss: 1.238898 \tValidation Loss: 1.247462 \tValidation Acc: 0.453704 \tTrue Acc: 0.452251\n",
      "Epoch: 80 \tTraining Loss: 1.237064 \tValidation Loss: 1.245788 \tValidation Acc: 0.454304 \tTrue Acc: 0.457872\n",
      "Epoch: 81 \tTraining Loss: 1.235266 \tValidation Loss: 1.243295 \tValidation Acc: 0.455756 \tTrue Acc: 0.456290\n",
      "Epoch: 82 \tTraining Loss: 1.233603 \tValidation Loss: 1.241589 \tValidation Acc: 0.455205 \tTrue Acc: 0.456751\n",
      "Epoch: 83 \tTraining Loss: 1.231885 \tValidation Loss: 1.239662 \tValidation Acc: 0.456957 \tTrue Acc: 0.458482\n",
      "Epoch: 84 \tTraining Loss: 1.230269 \tValidation Loss: 1.238359 \tValidation Acc: 0.457708 \tTrue Acc: 0.456207\n",
      "Epoch: 85 \tTraining Loss: 1.228657 \tValidation Loss: 1.237541 \tValidation Acc: 0.459259 \tTrue Acc: 0.457740\n",
      "Epoch: 86 \tTraining Loss: 1.227139 \tValidation Loss: 1.235598 \tValidation Acc: 0.458458 \tTrue Acc: 0.456949\n",
      "Epoch: 87 \tTraining Loss: 1.225519 \tValidation Loss: 1.234918 \tValidation Acc: 0.460010 \tTrue Acc: 0.459487\n",
      "Epoch: 88 \tTraining Loss: 1.223987 \tValidation Loss: 1.232288 \tValidation Acc: 0.461712 \tTrue Acc: 0.462174\n",
      "Epoch: 89 \tTraining Loss: 1.222575 \tValidation Loss: 1.232544 \tValidation Acc: 0.461812 \tTrue Acc: 0.462273\n",
      "Epoch: 90 \tTraining Loss: 1.221145 \tValidation Loss: 1.230764 \tValidation Acc: 0.462513 \tTrue Acc: 0.461959\n",
      "Epoch: 91 \tTraining Loss: 1.219672 \tValidation Loss: 1.228206 \tValidation Acc: 0.463614 \tTrue Acc: 0.465058\n",
      "Epoch: 92 \tTraining Loss: 1.218347 \tValidation Loss: 1.227307 \tValidation Acc: 0.463864 \tTrue Acc: 0.462289\n",
      "Epoch: 93 \tTraining Loss: 1.217010 \tValidation Loss: 1.225672 \tValidation Acc: 0.465165 \tTrue Acc: 0.463575\n",
      "Epoch: 94 \tTraining Loss: 1.215734 \tValidation Loss: 1.225158 \tValidation Acc: 0.465616 \tTrue Acc: 0.464020\n",
      "Epoch: 95 \tTraining Loss: 1.214492 \tValidation Loss: 1.226264 \tValidation Acc: 0.464965 \tTrue Acc: 0.464382\n",
      "Epoch: 96 \tTraining Loss: 1.213353 \tValidation Loss: 1.222864 \tValidation Acc: 0.467167 \tTrue Acc: 0.465552\n",
      "Epoch: 97 \tTraining Loss: 1.212112 \tValidation Loss: 1.223874 \tValidation Acc: 0.466667 \tTrue Acc: 0.468074\n",
      "Epoch: 98 \tTraining Loss: 1.210927 \tValidation Loss: 1.220443 \tValidation Acc: 0.467417 \tTrue Acc: 0.467811\n",
      "Epoch: 99 \tTraining Loss: 1.209745 \tValidation Loss: 1.218985 \tValidation Acc: 0.468569 \tTrue Acc: 0.469953\n",
      "Epoch: 100 \tTraining Loss: 1.208595 \tValidation Loss: 1.218670 \tValidation Acc: 0.470671 \tTrue Acc: 0.467003\n",
      "Epoch: 101 \tTraining Loss: 1.207483 \tValidation Loss: 1.217681 \tValidation Acc: 0.469419 \tTrue Acc: 0.469788\n",
      "Epoch: 102 \tTraining Loss: 1.206562 \tValidation Loss: 1.217553 \tValidation Acc: 0.469369 \tTrue Acc: 0.466723\n",
      "Epoch: 103 \tTraining Loss: 1.205443 \tValidation Loss: 1.214927 \tValidation Acc: 0.471672 \tTrue Acc: 0.470003\n",
      "Epoch: 104 \tTraining Loss: 1.204452 \tValidation Loss: 1.215220 \tValidation Acc: 0.471672 \tTrue Acc: 0.472013\n",
      "Epoch: 105 \tTraining Loss: 1.203479 \tValidation Loss: 1.213036 \tValidation Acc: 0.472072 \tTrue Acc: 0.473414\n",
      "Epoch: 106 \tTraining Loss: 1.202556 \tValidation Loss: 1.214082 \tValidation Acc: 0.471922 \tTrue Acc: 0.471255\n",
      "Epoch: 107 \tTraining Loss: 1.201630 \tValidation Loss: 1.211545 \tValidation Acc: 0.473423 \tTrue Acc: 0.472739\n",
      "Epoch: 108 \tTraining Loss: 1.200734 \tValidation Loss: 1.213015 \tValidation Acc: 0.472422 \tTrue Acc: 0.474766\n",
      "Epoch: 109 \tTraining Loss: 1.199751 \tValidation Loss: 1.210171 \tValidation Acc: 0.474675 \tTrue Acc: 0.471964\n",
      "Epoch: 110 \tTraining Loss: 1.198921 \tValidation Loss: 1.216380 \tValidation Acc: 0.471772 \tTrue Acc: 0.472112\n",
      "Epoch: 111 \tTraining Loss: 1.197974 \tValidation Loss: 1.208391 \tValidation Acc: 0.475676 \tTrue Acc: 0.472953\n",
      "Epoch: 112 \tTraining Loss: 1.197192 \tValidation Loss: 1.207492 \tValidation Acc: 0.474024 \tTrue Acc: 0.473332\n",
      "Epoch: 113 \tTraining Loss: 1.196335 \tValidation Loss: 1.206645 \tValidation Acc: 0.476627 \tTrue Acc: 0.475903\n",
      "Epoch: 114 \tTraining Loss: 1.195608 \tValidation Loss: 1.205652 \tValidation Acc: 0.477427 \tTrue Acc: 0.476694\n",
      "Epoch: 115 \tTraining Loss: 1.194492 \tValidation Loss: 1.204539 \tValidation Acc: 0.475926 \tTrue Acc: 0.478227\n",
      "Epoch: 116 \tTraining Loss: 1.193977 \tValidation Loss: 1.204580 \tValidation Acc: 0.476076 \tTrue Acc: 0.477370\n",
      "Epoch: 117 \tTraining Loss: 1.193085 \tValidation Loss: 1.204008 \tValidation Acc: 0.477728 \tTrue Acc: 0.477996\n",
      "Epoch: 118 \tTraining Loss: 1.192403 \tValidation Loss: 1.204705 \tValidation Acc: 0.475826 \tTrue Acc: 0.476117\n",
      "Epoch: 119 \tTraining Loss: 1.191500 \tValidation Loss: 1.202836 \tValidation Acc: 0.475876 \tTrue Acc: 0.476167\n",
      "Epoch: 120 \tTraining Loss: 1.190886 \tValidation Loss: 1.201889 \tValidation Acc: 0.478428 \tTrue Acc: 0.480699\n",
      "Epoch: 121 \tTraining Loss: 1.190073 \tValidation Loss: 1.204632 \tValidation Acc: 0.476076 \tTrue Acc: 0.475359\n",
      "Epoch: 122 \tTraining Loss: 1.189343 \tValidation Loss: 1.201989 \tValidation Acc: 0.478178 \tTrue Acc: 0.478441\n",
      "Epoch: 123 \tTraining Loss: 1.188800 \tValidation Loss: 1.202192 \tValidation Acc: 0.478428 \tTrue Acc: 0.478689\n",
      "Epoch: 124 \tTraining Loss: 1.188139 \tValidation Loss: 1.198815 \tValidation Acc: 0.478829 \tTrue Acc: 0.479084\n",
      "Epoch: 125 \tTraining Loss: 1.187467 \tValidation Loss: 1.197914 \tValidation Acc: 0.479129 \tTrue Acc: 0.480386\n",
      "Epoch: 126 \tTraining Loss: 1.186676 \tValidation Loss: 1.197634 \tValidation Acc: 0.480681 \tTrue Acc: 0.481919\n",
      "Epoch: 127 \tTraining Loss: 1.185850 \tValidation Loss: 1.197006 \tValidation Acc: 0.481782 \tTrue Acc: 0.480996\n",
      "Epoch: 128 \tTraining Loss: 1.185421 \tValidation Loss: 1.196032 \tValidation Acc: 0.478829 \tTrue Acc: 0.480090\n",
      "Epoch: 129 \tTraining Loss: 1.184837 \tValidation Loss: 1.196591 \tValidation Acc: 0.482032 \tTrue Acc: 0.482249\n",
      "Epoch: 130 \tTraining Loss: 1.184214 \tValidation Loss: 1.197761 \tValidation Acc: 0.480130 \tTrue Acc: 0.480370\n",
      "Epoch: 131 \tTraining Loss: 1.183564 \tValidation Loss: 1.197457 \tValidation Acc: 0.481231 \tTrue Acc: 0.481458\n",
      "Epoch: 132 \tTraining Loss: 1.182749 \tValidation Loss: 1.200223 \tValidation Acc: 0.476977 \tTrue Acc: 0.479266\n",
      "Epoch: 133 \tTraining Loss: 1.182263 \tValidation Loss: 1.196714 \tValidation Acc: 0.479479 \tTrue Acc: 0.474700\n",
      "Epoch: 134 \tTraining Loss: 1.181591 \tValidation Loss: 1.194592 \tValidation Acc: 0.483283 \tTrue Acc: 0.484490\n",
      "Epoch: 135 \tTraining Loss: 1.181053 \tValidation Loss: 1.192417 \tValidation Acc: 0.482482 \tTrue Acc: 0.482694\n",
      "Epoch: 136 \tTraining Loss: 1.180516 \tValidation Loss: 1.191408 \tValidation Acc: 0.481982 \tTrue Acc: 0.481194\n",
      "Epoch: 137 \tTraining Loss: 1.179907 \tValidation Loss: 1.191426 \tValidation Acc: 0.482032 \tTrue Acc: 0.483254\n",
      "Epoch: 138 \tTraining Loss: 1.179288 \tValidation Loss: 1.197295 \tValidation Acc: 0.480931 \tTrue Acc: 0.484177\n",
      "Epoch: 139 \tTraining Loss: 1.178862 \tValidation Loss: 1.191076 \tValidation Acc: 0.483534 \tTrue Acc: 0.483732\n",
      "Epoch: 140 \tTraining Loss: 1.178117 \tValidation Loss: 1.190257 \tValidation Acc: 0.484835 \tTrue Acc: 0.484012\n",
      "Epoch: 141 \tTraining Loss: 1.177698 \tValidation Loss: 1.189527 \tValidation Acc: 0.483133 \tTrue Acc: 0.483337\n",
      "Epoch: 142 \tTraining Loss: 1.177152 \tValidation Loss: 1.188458 \tValidation Acc: 0.482432 \tTrue Acc: 0.480634\n",
      "Epoch: 143 \tTraining Loss: 1.176645 \tValidation Loss: 1.188226 \tValidation Acc: 0.486537 \tTrue Acc: 0.485694\n",
      "Epoch: 144 \tTraining Loss: 1.176028 \tValidation Loss: 1.187683 \tValidation Acc: 0.485085 \tTrue Acc: 0.484260\n",
      "Epoch: 145 \tTraining Loss: 1.175571 \tValidation Loss: 1.188344 \tValidation Acc: 0.483483 \tTrue Acc: 0.484688\n",
      "Epoch: 146 \tTraining Loss: 1.175062 \tValidation Loss: 1.188067 \tValidation Acc: 0.481832 \tTrue Acc: 0.482051\n",
      "Epoch: 147 \tTraining Loss: 1.174554 \tValidation Loss: 1.188975 \tValidation Acc: 0.482482 \tTrue Acc: 0.483699\n",
      "Epoch: 148 \tTraining Loss: 1.174063 \tValidation Loss: 1.186385 \tValidation Acc: 0.485636 \tTrue Acc: 0.482793\n",
      "Epoch: 149 \tTraining Loss: 1.173761 \tValidation Loss: 1.185085 \tValidation Acc: 0.485836 \tTrue Acc: 0.486007\n",
      "Epoch: 150 \tTraining Loss: 1.173022 \tValidation Loss: 1.186552 \tValidation Acc: 0.484735 \tTrue Acc: 0.480897\n",
      "Epoch: 151 \tTraining Loss: 1.172609 \tValidation Loss: 1.186286 \tValidation Acc: 0.482633 \tTrue Acc: 0.478821\n",
      "Epoch: 152 \tTraining Loss: 1.172347 \tValidation Loss: 1.185514 \tValidation Acc: 0.487788 \tTrue Acc: 0.488941\n",
      "Epoch: 153 \tTraining Loss: 1.171732 \tValidation Loss: 1.184992 \tValidation Acc: 0.487387 \tTrue Acc: 0.487540\n",
      "Epoch: 154 \tTraining Loss: 1.171172 \tValidation Loss: 1.183573 \tValidation Acc: 0.487988 \tTrue Acc: 0.488133\n",
      "Epoch: 155 \tTraining Loss: 1.170689 \tValidation Loss: 1.186503 \tValidation Acc: 0.485385 \tTrue Acc: 0.482545\n",
      "Epoch: 156 \tTraining Loss: 1.170297 \tValidation Loss: 1.183817 \tValidation Acc: 0.488388 \tTrue Acc: 0.489534\n",
      "Epoch: 157 \tTraining Loss: 1.169849 \tValidation Loss: 1.185374 \tValidation Acc: 0.484084 \tTrue Acc: 0.479249\n",
      "Epoch: 158 \tTraining Loss: 1.169371 \tValidation Loss: 1.183170 \tValidation Acc: 0.487037 \tTrue Acc: 0.486188\n",
      "Epoch: 159 \tTraining Loss: 1.168723 \tValidation Loss: 1.184751 \tValidation Acc: 0.482032 \tTrue Acc: 0.485265\n",
      "Epoch: 160 \tTraining Loss: 1.168575 \tValidation Loss: 1.182100 \tValidation Acc: 0.487838 \tTrue Acc: 0.488990\n",
      "Epoch: 161 \tTraining Loss: 1.168171 \tValidation Loss: 1.187059 \tValidation Acc: 0.484034 \tTrue Acc: 0.486237\n",
      "Epoch: 162 \tTraining Loss: 1.167792 \tValidation Loss: 1.184248 \tValidation Acc: 0.484484 \tTrue Acc: 0.487688\n",
      "Epoch: 163 \tTraining Loss: 1.167227 \tValidation Loss: 1.182375 \tValidation Acc: 0.488589 \tTrue Acc: 0.487721\n",
      "Epoch: 164 \tTraining Loss: 1.166899 \tValidation Loss: 1.193645 \tValidation Acc: 0.480931 \tTrue Acc: 0.480156\n",
      "Epoch: 165 \tTraining Loss: 1.166485 \tValidation Loss: 1.179350 \tValidation Acc: 0.489990 \tTrue Acc: 0.491116\n",
      "Epoch: 166 \tTraining Loss: 1.165972 \tValidation Loss: 1.178564 \tValidation Acc: 0.489239 \tTrue Acc: 0.491380\n",
      "Epoch: 167 \tTraining Loss: 1.165617 \tValidation Loss: 1.178505 \tValidation Acc: 0.488338 \tTrue Acc: 0.486468\n",
      "Epoch: 168 \tTraining Loss: 1.165188 \tValidation Loss: 1.178179 \tValidation Acc: 0.490841 \tTrue Acc: 0.490951\n",
      "Epoch: 169 \tTraining Loss: 1.164748 \tValidation Loss: 1.177813 \tValidation Acc: 0.487137 \tTrue Acc: 0.491314\n",
      "Epoch: 170 \tTraining Loss: 1.164448 \tValidation Loss: 1.176954 \tValidation Acc: 0.490591 \tTrue Acc: 0.487688\n",
      "Epoch: 171 \tTraining Loss: 1.163930 \tValidation Loss: 1.179014 \tValidation Acc: 0.487287 \tTrue Acc: 0.490457\n",
      "Epoch: 172 \tTraining Loss: 1.163609 \tValidation Loss: 1.176586 \tValidation Acc: 0.489940 \tTrue Acc: 0.491067\n",
      "Epoch: 173 \tTraining Loss: 1.163244 \tValidation Loss: 1.177795 \tValidation Acc: 0.486136 \tTrue Acc: 0.484293\n",
      "Epoch: 174 \tTraining Loss: 1.162771 \tValidation Loss: 1.180061 \tValidation Acc: 0.485736 \tTrue Acc: 0.484902\n",
      "Epoch: 175 \tTraining Loss: 1.162423 \tValidation Loss: 1.175563 \tValidation Acc: 0.490290 \tTrue Acc: 0.493424\n",
      "Epoch: 176 \tTraining Loss: 1.162218 \tValidation Loss: 1.176229 \tValidation Acc: 0.488488 \tTrue Acc: 0.488627\n",
      "Epoch: 177 \tTraining Loss: 1.161803 \tValidation Loss: 1.175255 \tValidation Acc: 0.490440 \tTrue Acc: 0.489550\n",
      "Epoch: 178 \tTraining Loss: 1.161353 \tValidation Loss: 1.175474 \tValidation Acc: 0.488889 \tTrue Acc: 0.486007\n",
      "Epoch: 179 \tTraining Loss: 1.161122 \tValidation Loss: 1.174980 \tValidation Acc: 0.489890 \tTrue Acc: 0.489006\n",
      "Epoch: 180 \tTraining Loss: 1.160595 \tValidation Loss: 1.190340 \tValidation Acc: 0.478829 \tTrue Acc: 0.479084\n",
      "Epoch: 181 \tTraining Loss: 1.160500 \tValidation Loss: 1.178972 \tValidation Acc: 0.486136 \tTrue Acc: 0.487309\n",
      "Epoch: 182 \tTraining Loss: 1.160118 \tValidation Loss: 1.174683 \tValidation Acc: 0.491341 \tTrue Acc: 0.490440\n",
      "Epoch: 183 \tTraining Loss: 1.159577 \tValidation Loss: 1.173230 \tValidation Acc: 0.491642 \tTrue Acc: 0.491742\n",
      "Epoch: 184 \tTraining Loss: 1.159332 \tValidation Loss: 1.196321 \tValidation Acc: 0.477878 \tTrue Acc: 0.479150\n",
      "Epoch: 185 \tTraining Loss: 1.158950 \tValidation Loss: 1.173699 \tValidation Acc: 0.489840 \tTrue Acc: 0.492979\n",
      "Epoch: 186 \tTraining Loss: 1.158466 \tValidation Loss: 1.172325 \tValidation Acc: 0.490741 \tTrue Acc: 0.489847\n",
      "Epoch: 187 \tTraining Loss: 1.158286 \tValidation Loss: 1.174341 \tValidation Acc: 0.491692 \tTrue Acc: 0.490787\n",
      "Epoch: 188 \tTraining Loss: 1.157917 \tValidation Loss: 1.173714 \tValidation Acc: 0.489239 \tTrue Acc: 0.491380\n",
      "Epoch: 189 \tTraining Loss: 1.157457 \tValidation Loss: 1.171766 \tValidation Acc: 0.492593 \tTrue Acc: 0.495698\n",
      "Epoch: 190 \tTraining Loss: 1.157167 \tValidation Loss: 1.171606 \tValidation Acc: 0.491742 \tTrue Acc: 0.494858\n",
      "Epoch: 191 \tTraining Loss: 1.156823 \tValidation Loss: 1.171714 \tValidation Acc: 0.492292 \tTrue Acc: 0.490374\n",
      "Epoch: 192 \tTraining Loss: 1.156545 \tValidation Loss: 1.171954 \tValidation Acc: 0.490390 \tTrue Acc: 0.488496\n",
      "Epoch: 193 \tTraining Loss: 1.156215 \tValidation Loss: 1.171557 \tValidation Acc: 0.491391 \tTrue Acc: 0.490490\n",
      "Epoch: 194 \tTraining Loss: 1.155821 \tValidation Loss: 1.174922 \tValidation Acc: 0.488789 \tTrue Acc: 0.487919\n",
      "Epoch: 195 \tTraining Loss: 1.155514 \tValidation Loss: 1.169962 \tValidation Acc: 0.493093 \tTrue Acc: 0.491166\n",
      "Epoch: 196 \tTraining Loss: 1.155018 \tValidation Loss: 1.175701 \tValidation Acc: 0.484535 \tTrue Acc: 0.483716\n",
      "Epoch: 197 \tTraining Loss: 1.154924 \tValidation Loss: 1.177531 \tValidation Acc: 0.483133 \tTrue Acc: 0.486353\n",
      "Epoch: 198 \tTraining Loss: 1.154661 \tValidation Loss: 1.170159 \tValidation Acc: 0.490741 \tTrue Acc: 0.490852\n",
      "Epoch: 199 \tTraining Loss: 1.154377 \tValidation Loss: 1.169017 \tValidation Acc: 0.492643 \tTrue Acc: 0.492731\n",
      "Epoch: 200 \tTraining Loss: 1.153929 \tValidation Loss: 1.168581 \tValidation Acc: 0.492242 \tTrue Acc: 0.490325\n",
      "Epoch: 201 \tTraining Loss: 1.153655 \tValidation Loss: 1.170246 \tValidation Acc: 0.490991 \tTrue Acc: 0.489089\n",
      "Epoch: 202 \tTraining Loss: 1.153303 \tValidation Loss: 1.170571 \tValidation Acc: 0.489289 \tTrue Acc: 0.490424\n",
      "Epoch: 203 \tTraining Loss: 1.152903 \tValidation Loss: 1.176504 \tValidation Acc: 0.485035 \tTrue Acc: 0.485216\n",
      "Epoch: 204 \tTraining Loss: 1.152755 \tValidation Loss: 1.167439 \tValidation Acc: 0.493243 \tTrue Acc: 0.492319\n",
      "Epoch: 205 \tTraining Loss: 1.152440 \tValidation Loss: 1.167561 \tValidation Acc: 0.493443 \tTrue Acc: 0.492517\n",
      "Epoch: 206 \tTraining Loss: 1.152271 \tValidation Loss: 1.171080 \tValidation Acc: 0.489389 \tTrue Acc: 0.490523\n",
      "Epoch: 207 \tTraining Loss: 1.151744 \tValidation Loss: 1.167256 \tValidation Acc: 0.491792 \tTrue Acc: 0.491891\n",
      "Epoch: 208 \tTraining Loss: 1.151364 \tValidation Loss: 1.166884 \tValidation Acc: 0.493544 \tTrue Acc: 0.492616\n",
      "Epoch: 209 \tTraining Loss: 1.151184 \tValidation Loss: 1.166326 \tValidation Acc: 0.493143 \tTrue Acc: 0.493226\n",
      "Epoch: 210 \tTraining Loss: 1.151107 \tValidation Loss: 1.168005 \tValidation Acc: 0.492893 \tTrue Acc: 0.493984\n",
      "Epoch: 211 \tTraining Loss: 1.150773 \tValidation Loss: 1.167690 \tValidation Acc: 0.491692 \tTrue Acc: 0.489781\n",
      "Epoch: 212 \tTraining Loss: 1.150452 \tValidation Loss: 1.171713 \tValidation Acc: 0.488138 \tTrue Acc: 0.489287\n",
      "Epoch: 213 \tTraining Loss: 1.150279 \tValidation Loss: 1.169524 \tValidation Acc: 0.490490 \tTrue Acc: 0.494627\n",
      "Epoch: 214 \tTraining Loss: 1.149664 \tValidation Loss: 1.165352 \tValidation Acc: 0.494044 \tTrue Acc: 0.495121\n",
      "Epoch: 215 \tTraining Loss: 1.149596 \tValidation Loss: 1.164920 \tValidation Acc: 0.494094 \tTrue Acc: 0.492155\n",
      "Epoch: 216 \tTraining Loss: 1.149137 \tValidation Loss: 1.168617 \tValidation Acc: 0.489740 \tTrue Acc: 0.489864\n",
      "Epoch: 217 \tTraining Loss: 1.149015 \tValidation Loss: 1.167035 \tValidation Acc: 0.493143 \tTrue Acc: 0.494231\n",
      "Epoch: 218 \tTraining Loss: 1.148793 \tValidation Loss: 1.164313 \tValidation Acc: 0.494645 \tTrue Acc: 0.493704\n",
      "Epoch: 219 \tTraining Loss: 1.148352 \tValidation Loss: 1.178705 \tValidation Acc: 0.483133 \tTrue Acc: 0.485347\n",
      "Epoch: 220 \tTraining Loss: 1.148274 \tValidation Loss: 1.164786 \tValidation Acc: 0.493644 \tTrue Acc: 0.495731\n",
      "Epoch: 221 \tTraining Loss: 1.147854 \tValidation Loss: 1.167316 \tValidation Acc: 0.490591 \tTrue Acc: 0.487688\n",
      "Epoch: 222 \tTraining Loss: 1.147782 \tValidation Loss: 1.163788 \tValidation Acc: 0.493794 \tTrue Acc: 0.496885\n",
      "Epoch: 223 \tTraining Loss: 1.147357 \tValidation Loss: 1.171360 \tValidation Acc: 0.489089 \tTrue Acc: 0.491232\n",
      "Epoch: 224 \tTraining Loss: 1.147027 \tValidation Loss: 1.166248 \tValidation Acc: 0.492342 \tTrue Acc: 0.493440\n",
      "Epoch: 225 \tTraining Loss: 1.146760 \tValidation Loss: 1.162933 \tValidation Acc: 0.493944 \tTrue Acc: 0.496028\n",
      "Epoch: 226 \tTraining Loss: 1.146420 \tValidation Loss: 1.162763 \tValidation Acc: 0.495245 \tTrue Acc: 0.493292\n",
      "Epoch: 227 \tTraining Loss: 1.146095 \tValidation Loss: 1.183789 \tValidation Acc: 0.479630 \tTrue Acc: 0.481886\n",
      "Epoch: 228 \tTraining Loss: 1.146101 \tValidation Loss: 1.163816 \tValidation Acc: 0.493343 \tTrue Acc: 0.495434\n",
      "Epoch: 229 \tTraining Loss: 1.145521 \tValidation Loss: 1.170109 \tValidation Acc: 0.488288 \tTrue Acc: 0.491446\n",
      "Epoch: 230 \tTraining Loss: 1.145464 \tValidation Loss: 1.162120 \tValidation Acc: 0.495395 \tTrue Acc: 0.495451\n",
      "Epoch: 231 \tTraining Loss: 1.145247 \tValidation Loss: 1.161684 \tValidation Acc: 0.494394 \tTrue Acc: 0.497478\n",
      "Epoch: 232 \tTraining Loss: 1.144882 \tValidation Loss: 1.167346 \tValidation Acc: 0.489239 \tTrue Acc: 0.488364\n",
      "Epoch: 233 \tTraining Loss: 1.144474 \tValidation Loss: 1.171776 \tValidation Acc: 0.487487 \tTrue Acc: 0.485628\n",
      "Epoch: 234 \tTraining Loss: 1.144451 \tValidation Loss: 1.165197 \tValidation Acc: 0.491842 \tTrue Acc: 0.491940\n",
      "Epoch: 235 \tTraining Loss: 1.144204 \tValidation Loss: 1.164409 \tValidation Acc: 0.492142 \tTrue Acc: 0.494248\n",
      "Epoch: 236 \tTraining Loss: 1.143520 \tValidation Loss: 1.181257 \tValidation Acc: 0.483734 \tTrue Acc: 0.482925\n",
      "Epoch: 237 \tTraining Loss: 1.143375 \tValidation Loss: 1.162460 \tValidation Acc: 0.494394 \tTrue Acc: 0.496473\n",
      "Epoch: 238 \tTraining Loss: 1.143331 \tValidation Loss: 1.165911 \tValidation Acc: 0.490591 \tTrue Acc: 0.491710\n",
      "Epoch: 239 \tTraining Loss: 1.142915 \tValidation Loss: 1.169123 \tValidation Acc: 0.489489 \tTrue Acc: 0.491627\n",
      "Epoch: 240 \tTraining Loss: 1.142507 \tValidation Loss: 1.164807 \tValidation Acc: 0.493293 \tTrue Acc: 0.493374\n",
      "Epoch: 241 \tTraining Loss: 1.142322 \tValidation Loss: 1.159906 \tValidation Acc: 0.495445 \tTrue Acc: 0.495500\n",
      "Epoch: 242 \tTraining Loss: 1.142273 \tValidation Loss: 1.163625 \tValidation Acc: 0.493844 \tTrue Acc: 0.491907\n",
      "Epoch: 243 \tTraining Loss: 1.141850 \tValidation Loss: 1.163361 \tValidation Acc: 0.492242 \tTrue Acc: 0.495352\n",
      "Epoch: 244 \tTraining Loss: 1.141703 \tValidation Loss: 1.162158 \tValidation Acc: 0.492743 \tTrue Acc: 0.493836\n",
      "Epoch: 245 \tTraining Loss: 1.141276 \tValidation Loss: 1.161249 \tValidation Acc: 0.495996 \tTrue Acc: 0.493028\n",
      "Epoch: 246 \tTraining Loss: 1.141175 \tValidation Loss: 1.161237 \tValidation Acc: 0.494394 \tTrue Acc: 0.494462\n",
      "Epoch: 247 \tTraining Loss: 1.140798 \tValidation Loss: 1.161267 \tValidation Acc: 0.494094 \tTrue Acc: 0.495171\n",
      "Epoch: 248 \tTraining Loss: 1.140620 \tValidation Loss: 1.159836 \tValidation Acc: 0.495045 \tTrue Acc: 0.493094\n",
      "Epoch: 249 \tTraining Loss: 1.140753 \tValidation Loss: 1.164149 \tValidation Acc: 0.492142 \tTrue Acc: 0.492237\n",
      "Epoch: 250 \tTraining Loss: 1.140157 \tValidation Loss: 1.164083 \tValidation Acc: 0.492092 \tTrue Acc: 0.491182\n",
      "Epoch: 251 \tTraining Loss: 1.140149 \tValidation Loss: 1.158387 \tValidation Acc: 0.497247 \tTrue Acc: 0.498286\n",
      "Epoch: 252 \tTraining Loss: 1.139582 \tValidation Loss: 1.167270 \tValidation Acc: 0.488338 \tTrue Acc: 0.491495\n",
      "Epoch: 253 \tTraining Loss: 1.139427 \tValidation Loss: 1.161006 \tValidation Acc: 0.494244 \tTrue Acc: 0.491297\n",
      "Epoch: 254 \tTraining Loss: 1.139243 \tValidation Loss: 1.163825 \tValidation Acc: 0.489239 \tTrue Acc: 0.488364\n",
      "Epoch: 255 \tTraining Loss: 1.138776 \tValidation Loss: 1.159878 \tValidation Acc: 0.492543 \tTrue Acc: 0.491627\n",
      "Epoch: 256 \tTraining Loss: 1.138366 \tValidation Loss: 1.158972 \tValidation Acc: 0.496396 \tTrue Acc: 0.497445\n",
      "Epoch: 257 \tTraining Loss: 1.138479 \tValidation Loss: 1.159319 \tValidation Acc: 0.494945 \tTrue Acc: 0.495006\n",
      "Epoch: 258 \tTraining Loss: 1.137916 \tValidation Loss: 1.157088 \tValidation Acc: 0.497848 \tTrue Acc: 0.497874\n",
      "Epoch: 259 \tTraining Loss: 1.137744 \tValidation Loss: 1.162759 \tValidation Acc: 0.491441 \tTrue Acc: 0.493556\n",
      "Epoch: 260 \tTraining Loss: 1.137516 \tValidation Loss: 1.158836 \tValidation Acc: 0.494595 \tTrue Acc: 0.496671\n",
      "Epoch: 261 \tTraining Loss: 1.137333 \tValidation Loss: 1.160057 \tValidation Acc: 0.493243 \tTrue Acc: 0.492319\n",
      "Epoch: 262 \tTraining Loss: 1.137241 \tValidation Loss: 1.156472 \tValidation Acc: 0.497047 \tTrue Acc: 0.496077\n",
      "Epoch: 263 \tTraining Loss: 1.136784 \tValidation Loss: 1.160978 \tValidation Acc: 0.492342 \tTrue Acc: 0.494446\n",
      "Epoch: 264 \tTraining Loss: 1.136699 \tValidation Loss: 1.161007 \tValidation Acc: 0.493844 \tTrue Acc: 0.492913\n",
      "Epoch: 265 \tTraining Loss: 1.136448 \tValidation Loss: 1.161354 \tValidation Acc: 0.493794 \tTrue Acc: 0.492863\n",
      "Epoch: 266 \tTraining Loss: 1.136300 \tValidation Loss: 1.157505 \tValidation Acc: 0.496697 \tTrue Acc: 0.495731\n",
      "Epoch: 267 \tTraining Loss: 1.135719 \tValidation Loss: 1.162664 \tValidation Acc: 0.492192 \tTrue Acc: 0.491281\n",
      "Epoch: 268 \tTraining Loss: 1.135470 \tValidation Loss: 1.175200 \tValidation Acc: 0.485586 \tTrue Acc: 0.487770\n",
      "Epoch: 269 \tTraining Loss: 1.135559 \tValidation Loss: 1.158684 \tValidation Acc: 0.494444 \tTrue Acc: 0.494511\n",
      "Epoch: 270 \tTraining Loss: 1.135292 \tValidation Loss: 1.156050 \tValidation Acc: 0.497397 \tTrue Acc: 0.499440\n",
      "Epoch: 271 \tTraining Loss: 1.134909 \tValidation Loss: 1.161098 \tValidation Acc: 0.493644 \tTrue Acc: 0.492715\n",
      "Epoch: 272 \tTraining Loss: 1.134768 \tValidation Loss: 1.154844 \tValidation Acc: 0.497247 \tTrue Acc: 0.498286\n",
      "Epoch: 273 \tTraining Loss: 1.134394 \tValidation Loss: 1.155985 \tValidation Acc: 0.496947 \tTrue Acc: 0.497989\n",
      "Epoch: 274 \tTraining Loss: 1.133976 \tValidation Loss: 1.157294 \tValidation Acc: 0.496146 \tTrue Acc: 0.496193\n",
      "Epoch: 275 \tTraining Loss: 1.133866 \tValidation Loss: 1.163876 \tValidation Acc: 0.491742 \tTrue Acc: 0.493852\n",
      "Epoch: 276 \tTraining Loss: 1.133779 \tValidation Loss: 1.163672 \tValidation Acc: 0.490190 \tTrue Acc: 0.490309\n",
      "Epoch: 277 \tTraining Loss: 1.133405 \tValidation Loss: 1.161927 \tValidation Acc: 0.491241 \tTrue Acc: 0.493358\n",
      "Epoch: 278 \tTraining Loss: 1.133084 \tValidation Loss: 1.154298 \tValidation Acc: 0.497247 \tTrue Acc: 0.495270\n",
      "Epoch: 279 \tTraining Loss: 1.133267 \tValidation Loss: 1.153676 \tValidation Acc: 0.497748 \tTrue Acc: 0.497775\n",
      "Epoch: 280 \tTraining Loss: 1.132899 \tValidation Loss: 1.153490 \tValidation Acc: 0.498148 \tTrue Acc: 0.499176\n",
      "Epoch: 281 \tTraining Loss: 1.132419 \tValidation Loss: 1.153446 \tValidation Acc: 0.496847 \tTrue Acc: 0.499901\n",
      "Epoch: 282 \tTraining Loss: 1.132244 \tValidation Loss: 1.157368 \tValidation Acc: 0.494044 \tTrue Acc: 0.492105\n",
      "Epoch: 283 \tTraining Loss: 1.132213 \tValidation Loss: 1.166114 \tValidation Acc: 0.488589 \tTrue Acc: 0.488726\n",
      "Epoch: 284 \tTraining Loss: 1.131737 \tValidation Loss: 1.164385 \tValidation Acc: 0.490991 \tTrue Acc: 0.491100\n",
      "Epoch: 285 \tTraining Loss: 1.131796 \tValidation Loss: 1.156624 \tValidation Acc: 0.494094 \tTrue Acc: 0.494165\n",
      "Epoch: 286 \tTraining Loss: 1.131505 \tValidation Loss: 1.158091 \tValidation Acc: 0.492593 \tTrue Acc: 0.491677\n",
      "Epoch: 287 \tTraining Loss: 1.131192 \tValidation Loss: 1.165175 \tValidation Acc: 0.491592 \tTrue Acc: 0.489682\n",
      "Epoch: 288 \tTraining Loss: 1.131078 \tValidation Loss: 1.156507 \tValidation Acc: 0.495145 \tTrue Acc: 0.496209\n",
      "Epoch: 289 \tTraining Loss: 1.130650 \tValidation Loss: 1.153070 \tValidation Acc: 0.496096 \tTrue Acc: 0.494132\n",
      "Epoch: 290 \tTraining Loss: 1.130379 \tValidation Loss: 1.155838 \tValidation Acc: 0.495295 \tTrue Acc: 0.496357\n",
      "Epoch: 291 \tTraining Loss: 1.130181 \tValidation Loss: 1.168842 \tValidation Acc: 0.489540 \tTrue Acc: 0.488660\n",
      "Epoch: 292 \tTraining Loss: 1.130121 \tValidation Loss: 1.154178 \tValidation Acc: 0.496997 \tTrue Acc: 0.499044\n",
      "Epoch: 293 \tTraining Loss: 1.129735 \tValidation Loss: 1.153795 \tValidation Acc: 0.495546 \tTrue Acc: 0.492583\n",
      "Epoch: 294 \tTraining Loss: 1.129566 \tValidation Loss: 1.155301 \tValidation Acc: 0.495746 \tTrue Acc: 0.497808\n",
      "Epoch: 295 \tTraining Loss: 1.129335 \tValidation Loss: 1.160734 \tValidation Acc: 0.492492 \tTrue Acc: 0.489567\n",
      "Epoch: 296 \tTraining Loss: 1.129243 \tValidation Loss: 1.152756 \tValidation Acc: 0.497497 \tTrue Acc: 0.500544\n",
      "Epoch: 297 \tTraining Loss: 1.129066 \tValidation Loss: 1.154854 \tValidation Acc: 0.494494 \tTrue Acc: 0.495566\n",
      "Epoch: 298 \tTraining Loss: 1.128599 \tValidation Loss: 1.153248 \tValidation Acc: 0.496697 \tTrue Acc: 0.497742\n",
      "Epoch: 299 \tTraining Loss: 1.128366 \tValidation Loss: 1.157718 \tValidation Acc: 0.494044 \tTrue Acc: 0.495121\n",
      "Epoch: 300 \tTraining Loss: 1.128151 \tValidation Loss: 1.156803 \tValidation Acc: 0.493844 \tTrue Acc: 0.491907\n",
      "Epoch: 301 \tTraining Loss: 1.128007 \tValidation Loss: 1.153919 \tValidation Acc: 0.496346 \tTrue Acc: 0.494380\n",
      "Epoch: 302 \tTraining Loss: 1.127701 \tValidation Loss: 1.151270 \tValidation Acc: 0.498048 \tTrue Acc: 0.498072\n",
      "Epoch: 303 \tTraining Loss: 1.127310 \tValidation Loss: 1.156838 \tValidation Acc: 0.492893 \tTrue Acc: 0.491973\n",
      "Epoch: 304 \tTraining Loss: 1.127288 \tValidation Loss: 1.151674 \tValidation Acc: 0.496496 \tTrue Acc: 0.495533\n",
      "Epoch: 305 \tTraining Loss: 1.126955 \tValidation Loss: 1.160604 \tValidation Acc: 0.493694 \tTrue Acc: 0.492764\n",
      "Epoch: 306 \tTraining Loss: 1.126717 \tValidation Loss: 1.155768 \tValidation Acc: 0.496296 \tTrue Acc: 0.496341\n",
      "Epoch: 307 \tTraining Loss: 1.126720 \tValidation Loss: 1.151040 \tValidation Acc: 0.497197 \tTrue Acc: 0.495220\n",
      "Epoch: 308 \tTraining Loss: 1.126317 \tValidation Loss: 1.155106 \tValidation Acc: 0.494895 \tTrue Acc: 0.494956\n",
      "Epoch: 309 \tTraining Loss: 1.126260 \tValidation Loss: 1.160459 \tValidation Acc: 0.491191 \tTrue Acc: 0.490292\n",
      "Epoch: 310 \tTraining Loss: 1.125725 \tValidation Loss: 1.163483 \tValidation Acc: 0.490190 \tTrue Acc: 0.490309\n",
      "Epoch: 311 \tTraining Loss: 1.125970 \tValidation Loss: 1.158040 \tValidation Acc: 0.493744 \tTrue Acc: 0.492814\n",
      "Epoch: 312 \tTraining Loss: 1.125637 \tValidation Loss: 1.150793 \tValidation Acc: 0.496647 \tTrue Acc: 0.494676\n",
      "Epoch: 313 \tTraining Loss: 1.125150 \tValidation Loss: 1.156344 \tValidation Acc: 0.494044 \tTrue Acc: 0.491100\n",
      "Epoch: 314 \tTraining Loss: 1.124987 \tValidation Loss: 1.153963 \tValidation Acc: 0.495646 \tTrue Acc: 0.494693\n",
      "Epoch: 315 \tTraining Loss: 1.125178 \tValidation Loss: 1.158644 \tValidation Acc: 0.491942 \tTrue Acc: 0.490028\n",
      "Epoch: 316 \tTraining Loss: 1.124703 \tValidation Loss: 1.148854 \tValidation Acc: 0.497147 \tTrue Acc: 0.497182\n",
      "Epoch: 317 \tTraining Loss: 1.124216 \tValidation Loss: 1.162143 \tValidation Acc: 0.490340 \tTrue Acc: 0.490457\n",
      "Epoch: 318 \tTraining Loss: 1.124168 \tValidation Loss: 1.151735 \tValidation Acc: 0.496496 \tTrue Acc: 0.493523\n",
      "Epoch: 319 \tTraining Loss: 1.124030 \tValidation Loss: 1.149947 \tValidation Acc: 0.496346 \tTrue Acc: 0.496390\n",
      "Epoch: 320 \tTraining Loss: 1.124022 \tValidation Loss: 1.171007 \tValidation Acc: 0.484034 \tTrue Acc: 0.486237\n",
      "Epoch: 321 \tTraining Loss: 1.123452 \tValidation Loss: 1.148724 \tValidation Acc: 0.498749 \tTrue Acc: 0.499769\n",
      "Epoch: 322 \tTraining Loss: 1.123146 \tValidation Loss: 1.152918 \tValidation Acc: 0.495796 \tTrue Acc: 0.495847\n",
      "Epoch: 323 \tTraining Loss: 1.123277 \tValidation Loss: 1.160867 \tValidation Acc: 0.492292 \tTrue Acc: 0.492385\n",
      "Epoch: 324 \tTraining Loss: 1.123078 \tValidation Loss: 1.148902 \tValidation Acc: 0.498148 \tTrue Acc: 0.498170\n",
      "Epoch: 325 \tTraining Loss: 1.122573 \tValidation Loss: 1.154047 \tValidation Acc: 0.493243 \tTrue Acc: 0.493325\n",
      "Epoch: 326 \tTraining Loss: 1.122435 \tValidation Loss: 1.150479 \tValidation Acc: 0.497598 \tTrue Acc: 0.498632\n",
      "Epoch: 327 \tTraining Loss: 1.121979 \tValidation Loss: 1.148638 \tValidation Acc: 0.498949 \tTrue Acc: 0.500972\n",
      "Epoch: 328 \tTraining Loss: 1.121601 \tValidation Loss: 1.153810 \tValidation Acc: 0.494745 \tTrue Acc: 0.496819\n",
      "Epoch: 329 \tTraining Loss: 1.121863 \tValidation Loss: 1.150507 \tValidation Acc: 0.495095 \tTrue Acc: 0.495154\n",
      "Epoch: 330 \tTraining Loss: 1.121592 \tValidation Loss: 1.152608 \tValidation Acc: 0.494595 \tTrue Acc: 0.493654\n",
      "Epoch: 331 \tTraining Loss: 1.121286 \tValidation Loss: 1.150720 \tValidation Acc: 0.496046 \tTrue Acc: 0.493078\n",
      "Epoch: 332 \tTraining Loss: 1.121287 \tValidation Loss: 1.150694 \tValidation Acc: 0.496747 \tTrue Acc: 0.496786\n",
      "Epoch: 333 \tTraining Loss: 1.120907 \tValidation Loss: 1.147356 \tValidation Acc: 0.499550 \tTrue Acc: 0.500560\n",
      "Epoch: 334 \tTraining Loss: 1.120631 \tValidation Loss: 1.148465 \tValidation Acc: 0.497097 \tTrue Acc: 0.500148\n",
      "Epoch: 335 \tTraining Loss: 1.120156 \tValidation Loss: 1.157787 \tValidation Acc: 0.492743 \tTrue Acc: 0.491825\n",
      "Epoch: 336 \tTraining Loss: 1.120236 \tValidation Loss: 1.155324 \tValidation Acc: 0.492693 \tTrue Acc: 0.490770\n",
      "Epoch: 337 \tTraining Loss: 1.119917 \tValidation Loss: 1.149501 \tValidation Acc: 0.496547 \tTrue Acc: 0.497594\n",
      "Epoch: 338 \tTraining Loss: 1.119841 \tValidation Loss: 1.159746 \tValidation Acc: 0.493093 \tTrue Acc: 0.497198\n",
      "Epoch: 339 \tTraining Loss: 1.119606 \tValidation Loss: 1.148129 \tValidation Acc: 0.497548 \tTrue Acc: 0.497577\n",
      "Epoch: 340 \tTraining Loss: 1.119251 \tValidation Loss: 1.148470 \tValidation Acc: 0.497648 \tTrue Acc: 0.499687\n",
      "Epoch: 341 \tTraining Loss: 1.119318 \tValidation Loss: 1.162428 \tValidation Acc: 0.490841 \tTrue Acc: 0.487935\n",
      "Epoch: 342 \tTraining Loss: 1.119267 \tValidation Loss: 1.146826 \tValidation Acc: 0.499149 \tTrue Acc: 0.498154\n",
      "Epoch: 343 \tTraining Loss: 1.118884 \tValidation Loss: 1.151721 \tValidation Acc: 0.495646 \tTrue Acc: 0.497709\n",
      "Epoch: 344 \tTraining Loss: 1.118509 \tValidation Loss: 1.146611 \tValidation Acc: 0.499800 \tTrue Acc: 0.501813\n",
      "Epoch: 345 \tTraining Loss: 1.118471 \tValidation Loss: 1.149686 \tValidation Acc: 0.495345 \tTrue Acc: 0.495402\n",
      "Epoch: 346 \tTraining Loss: 1.118129 \tValidation Loss: 1.162390 \tValidation Acc: 0.491141 \tTrue Acc: 0.490243\n",
      "Epoch: 347 \tTraining Loss: 1.117715 \tValidation Loss: 1.147413 \tValidation Acc: 0.497347 \tTrue Acc: 0.498385\n",
      "Epoch: 348 \tTraining Loss: 1.117879 \tValidation Loss: 1.149559 \tValidation Acc: 0.495646 \tTrue Acc: 0.496704\n",
      "Epoch: 349 \tTraining Loss: 1.117497 \tValidation Loss: 1.153480 \tValidation Acc: 0.495546 \tTrue Acc: 0.497610\n",
      "Epoch: 350 \tTraining Loss: 1.117501 \tValidation Loss: 1.156543 \tValidation Acc: 0.492793 \tTrue Acc: 0.492880\n",
      "Epoch: 351 \tTraining Loss: 1.117291 \tValidation Loss: 1.151052 \tValidation Acc: 0.495045 \tTrue Acc: 0.497116\n",
      "Epoch: 352 \tTraining Loss: 1.117114 \tValidation Loss: 1.146830 \tValidation Acc: 0.500501 \tTrue Acc: 0.501500\n",
      "Epoch: 353 \tTraining Loss: 1.116735 \tValidation Loss: 1.148709 \tValidation Acc: 0.496747 \tTrue Acc: 0.494775\n",
      "Epoch: 354 \tTraining Loss: 1.116239 \tValidation Loss: 1.149006 \tValidation Acc: 0.497447 \tTrue Acc: 0.498484\n",
      "Epoch: 355 \tTraining Loss: 1.116442 \tValidation Loss: 1.149977 \tValidation Acc: 0.495746 \tTrue Acc: 0.496802\n",
      "Epoch: 356 \tTraining Loss: 1.116105 \tValidation Loss: 1.146634 \tValidation Acc: 0.497648 \tTrue Acc: 0.495665\n",
      "Epoch: 357 \tTraining Loss: 1.115695 \tValidation Loss: 1.151941 \tValidation Acc: 0.495445 \tTrue Acc: 0.494495\n",
      "Epoch: 358 \tTraining Loss: 1.115708 \tValidation Loss: 1.144891 \tValidation Acc: 0.501502 \tTrue Acc: 0.504500\n",
      "Epoch: 359 \tTraining Loss: 1.115548 \tValidation Loss: 1.150452 \tValidation Acc: 0.494645 \tTrue Acc: 0.496720\n",
      "Epoch: 360 \tTraining Loss: 1.115464 \tValidation Loss: 1.146541 \tValidation Acc: 0.498498 \tTrue Acc: 0.498517\n",
      "Epoch: 361 \tTraining Loss: 1.115104 \tValidation Loss: 1.156368 \tValidation Acc: 0.491491 \tTrue Acc: 0.489583\n",
      "Epoch: 362 \tTraining Loss: 1.115060 \tValidation Loss: 1.146514 \tValidation Acc: 0.498899 \tTrue Acc: 0.497907\n",
      "Epoch: 363 \tTraining Loss: 1.114910 \tValidation Loss: 1.149909 \tValidation Acc: 0.496947 \tTrue Acc: 0.497989\n",
      "Epoch: 364 \tTraining Loss: 1.114459 \tValidation Loss: 1.143572 \tValidation Acc: 0.501051 \tTrue Acc: 0.503049\n",
      "Epoch: 365 \tTraining Loss: 1.114604 \tValidation Loss: 1.146986 \tValidation Acc: 0.499099 \tTrue Acc: 0.501121\n",
      "Epoch: 366 \tTraining Loss: 1.114043 \tValidation Loss: 1.154562 \tValidation Acc: 0.493794 \tTrue Acc: 0.492863\n",
      "Epoch: 367 \tTraining Loss: 1.114167 \tValidation Loss: 1.152432 \tValidation Acc: 0.493293 \tTrue Acc: 0.492369\n",
      "Epoch: 368 \tTraining Loss: 1.113757 \tValidation Loss: 1.146635 \tValidation Acc: 0.497748 \tTrue Acc: 0.499786\n",
      "Epoch: 369 \tTraining Loss: 1.113314 \tValidation Loss: 1.149499 \tValidation Acc: 0.497097 \tTrue Acc: 0.495121\n",
      "Epoch: 370 \tTraining Loss: 1.113374 \tValidation Loss: 1.143935 \tValidation Acc: 0.500951 \tTrue Acc: 0.499934\n",
      "Epoch: 371 \tTraining Loss: 1.112869 \tValidation Loss: 1.143212 \tValidation Acc: 0.501802 \tTrue Acc: 0.498764\n",
      "Epoch: 372 \tTraining Loss: 1.112714 \tValidation Loss: 1.143546 \tValidation Acc: 0.499099 \tTrue Acc: 0.501121\n",
      "Epoch: 373 \tTraining Loss: 1.112589 \tValidation Loss: 1.147586 \tValidation Acc: 0.498649 \tTrue Acc: 0.496654\n",
      "Epoch: 374 \tTraining Loss: 1.112577 \tValidation Loss: 1.143653 \tValidation Acc: 0.499750 \tTrue Acc: 0.497742\n",
      "Epoch: 375 \tTraining Loss: 1.112556 \tValidation Loss: 1.146267 \tValidation Acc: 0.499850 \tTrue Acc: 0.498846\n",
      "Epoch: 376 \tTraining Loss: 1.112109 \tValidation Loss: 1.145339 \tValidation Acc: 0.499900 \tTrue Acc: 0.499901\n",
      "Epoch: 377 \tTraining Loss: 1.112181 \tValidation Loss: 1.151423 \tValidation Acc: 0.492743 \tTrue Acc: 0.495847\n",
      "Epoch: 378 \tTraining Loss: 1.112025 \tValidation Loss: 1.166912 \tValidation Acc: 0.490490 \tTrue Acc: 0.491611\n",
      "Epoch: 379 \tTraining Loss: 1.111519 \tValidation Loss: 1.142247 \tValidation Acc: 0.501051 \tTrue Acc: 0.501038\n",
      "Epoch: 380 \tTraining Loss: 1.111342 \tValidation Loss: 1.149745 \tValidation Acc: 0.496597 \tTrue Acc: 0.496638\n",
      "Epoch: 381 \tTraining Loss: 1.111244 \tValidation Loss: 1.151750 \tValidation Acc: 0.495896 \tTrue Acc: 0.494940\n",
      "Epoch: 382 \tTraining Loss: 1.110883 \tValidation Loss: 1.146826 \tValidation Acc: 0.498999 \tTrue Acc: 0.499011\n",
      "Epoch: 383 \tTraining Loss: 1.110860 \tValidation Loss: 1.171706 \tValidation Acc: 0.486436 \tTrue Acc: 0.487605\n",
      "Epoch: 384 \tTraining Loss: 1.110272 \tValidation Loss: 1.146594 \tValidation Acc: 0.496246 \tTrue Acc: 0.497297\n",
      "Epoch: 385 \tTraining Loss: 1.110334 \tValidation Loss: 1.153334 \tValidation Acc: 0.494094 \tTrue Acc: 0.493160\n",
      "Epoch: 386 \tTraining Loss: 1.110317 \tValidation Loss: 1.149962 \tValidation Acc: 0.497648 \tTrue Acc: 0.497676\n",
      "Epoch: 387 \tTraining Loss: 1.110235 \tValidation Loss: 1.146246 \tValidation Acc: 0.498899 \tTrue Acc: 0.496901\n",
      "Epoch: 388 \tTraining Loss: 1.109363 \tValidation Loss: 1.141647 \tValidation Acc: 0.501602 \tTrue Acc: 0.502588\n",
      "Epoch: 389 \tTraining Loss: 1.109723 \tValidation Loss: 1.146316 \tValidation Acc: 0.500100 \tTrue Acc: 0.501104\n",
      "Epoch: 390 \tTraining Loss: 1.109336 \tValidation Loss: 1.142160 \tValidation Acc: 0.498749 \tTrue Acc: 0.497758\n",
      "Epoch: 391 \tTraining Loss: 1.109185 \tValidation Loss: 1.153159 \tValidation Acc: 0.494444 \tTrue Acc: 0.493506\n",
      "Epoch: 392 \tTraining Loss: 1.108844 \tValidation Loss: 1.143792 \tValidation Acc: 0.499950 \tTrue Acc: 0.498945\n",
      "Epoch: 393 \tTraining Loss: 1.108926 \tValidation Loss: 1.157672 \tValidation Acc: 0.493844 \tTrue Acc: 0.493918\n",
      "Epoch: 394 \tTraining Loss: 1.108888 \tValidation Loss: 1.143625 \tValidation Acc: 0.499900 \tTrue Acc: 0.499901\n",
      "Epoch: 395 \tTraining Loss: 1.108770 \tValidation Loss: 1.142525 \tValidation Acc: 0.502302 \tTrue Acc: 0.503280\n",
      "Epoch: 396 \tTraining Loss: 1.108272 \tValidation Loss: 1.150048 \tValidation Acc: 0.496196 \tTrue Acc: 0.498253\n",
      "Epoch: 397 \tTraining Loss: 1.108112 \tValidation Loss: 1.143945 \tValidation Acc: 0.497447 \tTrue Acc: 0.495467\n",
      "Epoch: 398 \tTraining Loss: 1.107854 \tValidation Loss: 1.143220 \tValidation Acc: 0.497898 \tTrue Acc: 0.495912\n",
      "Epoch: 399 \tTraining Loss: 1.107601 \tValidation Loss: 1.154300 \tValidation Acc: 0.494294 \tTrue Acc: 0.493358\n",
      "Epoch: 400 \tTraining Loss: 1.107273 \tValidation Loss: 1.150631 \tValidation Acc: 0.497147 \tTrue Acc: 0.498187\n",
      "Epoch: 401 \tTraining Loss: 1.107512 \tValidation Loss: 1.158786 \tValidation Acc: 0.490691 \tTrue Acc: 0.491808\n",
      "Epoch: 402 \tTraining Loss: 1.106804 \tValidation Loss: 1.144747 \tValidation Acc: 0.497147 \tTrue Acc: 0.499192\n",
      "Epoch: 403 \tTraining Loss: 1.107142 \tValidation Loss: 1.160105 \tValidation Acc: 0.492342 \tTrue Acc: 0.494446\n",
      "Epoch: 404 \tTraining Loss: 1.106365 \tValidation Loss: 1.141239 \tValidation Acc: 0.500100 \tTrue Acc: 0.500099\n",
      "Epoch: 405 \tTraining Loss: 1.106430 \tValidation Loss: 1.140930 \tValidation Acc: 0.499800 \tTrue Acc: 0.498797\n",
      "Epoch: 406 \tTraining Loss: 1.106388 \tValidation Loss: 1.157146 \tValidation Acc: 0.495045 \tTrue Acc: 0.496110\n",
      "Epoch: 407 \tTraining Loss: 1.106273 \tValidation Loss: 1.142784 \tValidation Acc: 0.500050 \tTrue Acc: 0.498039\n",
      "Epoch: 408 \tTraining Loss: 1.106123 \tValidation Loss: 1.180265 \tValidation Acc: 0.480330 \tTrue Acc: 0.480568\n",
      "Epoch: 409 \tTraining Loss: 1.105461 \tValidation Loss: 1.145128 \tValidation Acc: 0.498248 \tTrue Acc: 0.496259\n",
      "Epoch: 410 \tTraining Loss: 1.105687 \tValidation Loss: 1.140670 \tValidation Acc: 0.501401 \tTrue Acc: 0.499374\n",
      "Epoch: 411 \tTraining Loss: 1.105573 \tValidation Loss: 1.142472 \tValidation Acc: 0.499850 \tTrue Acc: 0.502868\n",
      "Epoch: 412 \tTraining Loss: 1.104878 \tValidation Loss: 1.139550 \tValidation Acc: 0.502603 \tTrue Acc: 0.505587\n",
      "Epoch: 413 \tTraining Loss: 1.104846 \tValidation Loss: 1.142398 \tValidation Acc: 0.500450 \tTrue Acc: 0.501450\n",
      "Epoch: 414 \tTraining Loss: 1.104493 \tValidation Loss: 1.146502 \tValidation Acc: 0.497598 \tTrue Acc: 0.498632\n",
      "Epoch: 415 \tTraining Loss: 1.104753 \tValidation Loss: 1.140784 \tValidation Acc: 0.501802 \tTrue Acc: 0.500775\n",
      "Epoch: 416 \tTraining Loss: 1.104200 \tValidation Loss: 1.155205 \tValidation Acc: 0.493043 \tTrue Acc: 0.494132\n",
      "Epoch: 417 \tTraining Loss: 1.104484 \tValidation Loss: 1.148751 \tValidation Acc: 0.496797 \tTrue Acc: 0.497841\n",
      "Epoch: 418 \tTraining Loss: 1.104295 \tValidation Loss: 1.142089 \tValidation Acc: 0.501051 \tTrue Acc: 0.500033\n",
      "Epoch: 419 \tTraining Loss: 1.103998 \tValidation Loss: 1.151485 \tValidation Acc: 0.495746 \tTrue Acc: 0.494792\n",
      "Epoch: 420 \tTraining Loss: 1.103748 \tValidation Loss: 1.140017 \tValidation Acc: 0.503253 \tTrue Acc: 0.504219\n",
      "Epoch: 421 \tTraining Loss: 1.103289 \tValidation Loss: 1.141317 \tValidation Acc: 0.502452 \tTrue Acc: 0.502423\n",
      "Epoch: 422 \tTraining Loss: 1.103069 \tValidation Loss: 1.138929 \tValidation Acc: 0.502853 \tTrue Acc: 0.502818\n",
      "Epoch: 423 \tTraining Loss: 1.102984 \tValidation Loss: 1.143774 \tValidation Acc: 0.497798 \tTrue Acc: 0.496819\n",
      "Epoch: 424 \tTraining Loss: 1.102748 \tValidation Loss: 1.152698 \tValidation Acc: 0.496346 \tTrue Acc: 0.496390\n",
      "Epoch: 425 \tTraining Loss: 1.102676 \tValidation Loss: 1.144312 \tValidation Acc: 0.500200 \tTrue Acc: 0.498187\n",
      "Epoch: 426 \tTraining Loss: 1.102603 \tValidation Loss: 1.148960 \tValidation Acc: 0.496446 \tTrue Acc: 0.494479\n",
      "Epoch: 427 \tTraining Loss: 1.102041 \tValidation Loss: 1.140202 \tValidation Acc: 0.499449 \tTrue Acc: 0.501467\n",
      "Epoch: 428 \tTraining Loss: 1.102303 \tValidation Loss: 1.143401 \tValidation Acc: 0.501852 \tTrue Acc: 0.500824\n",
      "Epoch: 429 \tTraining Loss: 1.101902 \tValidation Loss: 1.142674 \tValidation Acc: 0.500200 \tTrue Acc: 0.502209\n",
      "Epoch: 430 \tTraining Loss: 1.101265 \tValidation Loss: 1.152194 \tValidation Acc: 0.496697 \tTrue Acc: 0.495731\n",
      "Epoch: 431 \tTraining Loss: 1.101620 \tValidation Loss: 1.154815 \tValidation Acc: 0.496296 \tTrue Acc: 0.494330\n",
      "Epoch: 432 \tTraining Loss: 1.101444 \tValidation Loss: 1.141422 \tValidation Acc: 0.503303 \tTrue Acc: 0.504269\n",
      "Epoch: 433 \tTraining Loss: 1.101338 \tValidation Loss: 1.143951 \tValidation Acc: 0.500100 \tTrue Acc: 0.502110\n",
      "Epoch: 434 \tTraining Loss: 1.101090 \tValidation Loss: 1.154136 \tValidation Acc: 0.493243 \tTrue Acc: 0.493325\n",
      "Epoch: 435 \tTraining Loss: 1.100758 \tValidation Loss: 1.140180 \tValidation Acc: 0.501652 \tTrue Acc: 0.501632\n",
      "Epoch: 436 \tTraining Loss: 1.100620 \tValidation Loss: 1.146555 \tValidation Acc: 0.498799 \tTrue Acc: 0.499819\n",
      "Epoch: 437 \tTraining Loss: 1.100281 \tValidation Loss: 1.148060 \tValidation Acc: 0.498248 \tTrue Acc: 0.499275\n",
      "Epoch: 438 \tTraining Loss: 1.100621 \tValidation Loss: 1.155517 \tValidation Acc: 0.493694 \tTrue Acc: 0.495781\n",
      "Epoch: 439 \tTraining Loss: 1.100876 \tValidation Loss: 1.183902 \tValidation Acc: 0.482182 \tTrue Acc: 0.482397\n",
      "Epoch: 440 \tTraining Loss: 1.099958 \tValidation Loss: 1.138104 \tValidation Acc: 0.505305 \tTrue Acc: 0.506247\n",
      "Epoch: 441 \tTraining Loss: 1.099492 \tValidation Loss: 1.167758 \tValidation Acc: 0.488238 \tTrue Acc: 0.488380\n",
      "Epoch: 442 \tTraining Loss: 1.099767 \tValidation Loss: 1.144585 \tValidation Acc: 0.497347 \tTrue Acc: 0.499390\n",
      "Epoch: 443 \tTraining Loss: 1.099822 \tValidation Loss: 1.142408 \tValidation Acc: 0.499449 \tTrue Acc: 0.498451\n",
      "Epoch: 444 \tTraining Loss: 1.098968 \tValidation Loss: 1.137964 \tValidation Acc: 0.502703 \tTrue Acc: 0.502670\n",
      "Epoch: 445 \tTraining Loss: 1.098790 \tValidation Loss: 1.145262 \tValidation Acc: 0.498348 \tTrue Acc: 0.501384\n",
      "Epoch: 446 \tTraining Loss: 1.099061 \tValidation Loss: 1.138699 \tValidation Acc: 0.503003 \tTrue Acc: 0.502967\n",
      "Epoch: 447 \tTraining Loss: 1.098679 \tValidation Loss: 1.141525 \tValidation Acc: 0.498949 \tTrue Acc: 0.498962\n",
      "Epoch: 448 \tTraining Loss: 1.098548 \tValidation Loss: 1.177667 \tValidation Acc: 0.480781 \tTrue Acc: 0.481013\n",
      "Epoch: 449 \tTraining Loss: 1.097923 \tValidation Loss: 1.157070 \tValidation Acc: 0.494895 \tTrue Acc: 0.496967\n",
      "Epoch: 450 \tTraining Loss: 1.098572 \tValidation Loss: 1.143966 \tValidation Acc: 0.501251 \tTrue Acc: 0.499225\n",
      "Epoch: 451 \tTraining Loss: 1.098015 \tValidation Loss: 1.150063 \tValidation Acc: 0.496997 \tTrue Acc: 0.501055\n",
      "Epoch: 452 \tTraining Loss: 1.097768 \tValidation Loss: 1.137138 \tValidation Acc: 0.504805 \tTrue Acc: 0.506758\n",
      "Epoch: 453 \tTraining Loss: 1.097754 \tValidation Loss: 1.162752 \tValidation Acc: 0.493694 \tTrue Acc: 0.493770\n",
      "Epoch: 454 \tTraining Loss: 1.097468 \tValidation Loss: 1.147799 \tValidation Acc: 0.498398 \tTrue Acc: 0.496407\n",
      "Epoch: 455 \tTraining Loss: 1.096936 \tValidation Loss: 1.145968 \tValidation Acc: 0.497998 \tTrue Acc: 0.498022\n",
      "Epoch: 456 \tTraining Loss: 1.097054 \tValidation Loss: 1.161902 \tValidation Acc: 0.493443 \tTrue Acc: 0.493523\n",
      "Epoch: 457 \tTraining Loss: 1.096741 \tValidation Loss: 1.138315 \tValidation Acc: 0.503003 \tTrue Acc: 0.504978\n",
      "Epoch: 458 \tTraining Loss: 1.096196 \tValidation Loss: 1.209524 \tValidation Acc: 0.470320 \tTrue Acc: 0.470678\n",
      "Epoch: 459 \tTraining Loss: 1.096876 \tValidation Loss: 1.141995 \tValidation Acc: 0.502002 \tTrue Acc: 0.498962\n",
      "Epoch: 460 \tTraining Loss: 1.096494 \tValidation Loss: 1.137245 \tValidation Acc: 0.505205 \tTrue Acc: 0.506148\n",
      "Epoch: 461 \tTraining Loss: 1.096069 \tValidation Loss: 1.141284 \tValidation Acc: 0.504505 \tTrue Acc: 0.503445\n",
      "Epoch: 462 \tTraining Loss: 1.095595 \tValidation Loss: 1.146694 \tValidation Acc: 0.499049 \tTrue Acc: 0.499061\n",
      "Epoch: 463 \tTraining Loss: 1.095524 \tValidation Loss: 1.148911 \tValidation Acc: 0.497898 \tTrue Acc: 0.498929\n",
      "Epoch: 464 \tTraining Loss: 1.095618 \tValidation Loss: 1.138978 \tValidation Acc: 0.499900 \tTrue Acc: 0.500907\n",
      "Epoch: 465 \tTraining Loss: 1.094947 \tValidation Loss: 1.137640 \tValidation Acc: 0.503253 \tTrue Acc: 0.505225\n",
      "Epoch: 466 \tTraining Loss: 1.095466 \tValidation Loss: 1.157949 \tValidation Acc: 0.495495 \tTrue Acc: 0.493539\n",
      "Epoch: 467 \tTraining Loss: 1.094960 \tValidation Loss: 1.145725 \tValidation Acc: 0.499299 \tTrue Acc: 0.500313\n",
      "Epoch: 468 \tTraining Loss: 1.094648 \tValidation Loss: 1.145988 \tValidation Acc: 0.496496 \tTrue Acc: 0.497544\n",
      "Epoch: 469 \tTraining Loss: 1.094543 \tValidation Loss: 1.150204 \tValidation Acc: 0.498899 \tTrue Acc: 0.499918\n",
      "Epoch: 470 \tTraining Loss: 1.094357 \tValidation Loss: 1.145395 \tValidation Acc: 0.498098 \tTrue Acc: 0.499126\n",
      "Epoch: 471 \tTraining Loss: 1.094613 \tValidation Loss: 1.148981 \tValidation Acc: 0.497548 \tTrue Acc: 0.497577\n",
      "Epoch: 472 \tTraining Loss: 1.094642 \tValidation Loss: 1.142938 \tValidation Acc: 0.497998 \tTrue Acc: 0.496011\n",
      "Epoch: 473 \tTraining Loss: 1.094272 \tValidation Loss: 1.138617 \tValidation Acc: 0.500501 \tTrue Acc: 0.497478\n",
      "Epoch: 474 \tTraining Loss: 1.093932 \tValidation Loss: 1.139460 \tValidation Acc: 0.503103 \tTrue Acc: 0.503066\n",
      "Epoch: 475 \tTraining Loss: 1.093350 \tValidation Loss: 1.142551 \tValidation Acc: 0.499249 \tTrue Acc: 0.501269\n",
      "Epoch: 476 \tTraining Loss: 1.093730 \tValidation Loss: 1.137897 \tValidation Acc: 0.503804 \tTrue Acc: 0.502753\n",
      "Epoch: 477 \tTraining Loss: 1.093291 \tValidation Loss: 1.157608 \tValidation Acc: 0.495045 \tTrue Acc: 0.497116\n",
      "Epoch: 478 \tTraining Loss: 1.093239 \tValidation Loss: 1.147062 \tValidation Acc: 0.497347 \tTrue Acc: 0.496374\n",
      "Epoch: 479 \tTraining Loss: 1.093122 \tValidation Loss: 1.137568 \tValidation Acc: 0.505205 \tTrue Acc: 0.503132\n",
      "Epoch: 480 \tTraining Loss: 1.092807 \tValidation Loss: 1.171366 \tValidation Acc: 0.482633 \tTrue Acc: 0.480831\n",
      "Epoch: 481 \tTraining Loss: 1.092639 \tValidation Loss: 1.143476 \tValidation Acc: 0.501201 \tTrue Acc: 0.499176\n",
      "Epoch: 482 \tTraining Loss: 1.092576 \tValidation Loss: 1.139327 \tValidation Acc: 0.503353 \tTrue Acc: 0.503313\n",
      "Epoch: 483 \tTraining Loss: 1.093057 \tValidation Loss: 1.141343 \tValidation Acc: 0.501251 \tTrue Acc: 0.502242\n",
      "Epoch: 484 \tTraining Loss: 1.092549 \tValidation Loss: 1.214199 \tValidation Acc: 0.469920 \tTrue Acc: 0.472294\n",
      "Epoch: 485 \tTraining Loss: 1.092224 \tValidation Loss: 1.136196 \tValidation Acc: 0.504254 \tTrue Acc: 0.504203\n",
      "Epoch: 486 \tTraining Loss: 1.091909 \tValidation Loss: 1.146042 \tValidation Acc: 0.501351 \tTrue Acc: 0.501335\n",
      "Epoch: 487 \tTraining Loss: 1.091392 \tValidation Loss: 1.147367 \tValidation Acc: 0.498498 \tTrue Acc: 0.497511\n",
      "Epoch: 488 \tTraining Loss: 1.091326 \tValidation Loss: 1.222880 \tValidation Acc: 0.465516 \tTrue Acc: 0.462915\n",
      "Epoch: 489 \tTraining Loss: 1.091912 \tValidation Loss: 1.159346 \tValidation Acc: 0.495495 \tTrue Acc: 0.497561\n",
      "Epoch: 490 \tTraining Loss: 1.091190 \tValidation Loss: 1.150130 \tValidation Acc: 0.497097 \tTrue Acc: 0.495121\n",
      "Epoch: 491 \tTraining Loss: 1.090871 \tValidation Loss: 1.139302 \tValidation Acc: 0.503353 \tTrue Acc: 0.504318\n",
      "Epoch: 492 \tTraining Loss: 1.090367 \tValidation Loss: 1.135761 \tValidation Acc: 0.503604 \tTrue Acc: 0.500544\n",
      "Epoch: 493 \tTraining Loss: 1.090589 \tValidation Loss: 1.183349 \tValidation Acc: 0.481932 \tTrue Acc: 0.484161\n",
      "Epoch: 494 \tTraining Loss: 1.089995 \tValidation Loss: 1.141323 \tValidation Acc: 0.501602 \tTrue Acc: 0.500577\n",
      "Epoch: 495 \tTraining Loss: 1.090392 \tValidation Loss: 1.139097 \tValidation Acc: 0.501301 \tTrue Acc: 0.504302\n",
      "Epoch: 496 \tTraining Loss: 1.089965 \tValidation Loss: 1.136576 \tValidation Acc: 0.504905 \tTrue Acc: 0.506857\n",
      "Epoch: 497 \tTraining Loss: 1.089880 \tValidation Loss: 1.136984 \tValidation Acc: 0.505305 \tTrue Acc: 0.506247\n",
      "Epoch: 498 \tTraining Loss: 1.089414 \tValidation Loss: 1.137809 \tValidation Acc: 0.503904 \tTrue Acc: 0.504862\n",
      "Epoch: 499 \tTraining Loss: 1.089466 \tValidation Loss: 1.154298 \tValidation Acc: 0.495395 \tTrue Acc: 0.494446\n",
      "Epoch: 500 \tTraining Loss: 1.089021 \tValidation Loss: 1.169914 \tValidation Acc: 0.490090 \tTrue Acc: 0.487193\n"
     ]
    }
   ],
   "source": [
    "test_batch_size=256 # using 256 for test batch size for now to use it as a validation set\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# Also, I am gathering two accuracies as I was unsure about my calculation of accuracy\n",
    "# The true accuracy is just named that way to differentiate between the first one and second one. Both are correct\n",
    "# Validation accuracy is gathering \n",
    "\n",
    "# for accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# training params\n",
    "epochs = 500\n",
    "\n",
    "# initialize device\n",
    "# device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "# model.cuda()\n",
    "\n",
    "# train for some number of epochs 150\n",
    "for e in range(epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "  \n",
    "  model.train()\n",
    "  # batch loop\n",
    "  for inputs, labels in train_loader:\n",
    "    # inputs=inputs.cuda()\n",
    "    # labels=labels.cuda()\n",
    "    # labels = labels.type(torch.LongTensor)   # casting to long\n",
    "    # inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # zero accumulated gradients\n",
    "    optimizer.zero_grad()\n",
    "    # get the output from the model\n",
    "    output = model(inputs)\n",
    "    # calculate the loss and perform backprop\n",
    "    loss = loss_fn(output, labels)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*inputs.size(0)\n",
    "\n",
    "  model.eval() # prep model for evaluation\n",
    "  num_correct = 0 # initialize num_correct for correct predictions\n",
    "  epoch_acc=0\n",
    "  count=0\n",
    "  for inputs, labels in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(inputs)\n",
    "    # calculate the loss\n",
    "    loss = loss_fn(output, labels)\n",
    "    # update running validation loss \n",
    "    valid_loss += loss.item()*inputs.size(0)\n",
    "    # convert output probabilities to predicted class (1-5)\n",
    "    # pred = torch.argmax(output, dim=1, keepdim=True)\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    # pred = torch.round(pred)  # rounds to the nearest integer\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    epoch_acc += accuracy_score(pred, labels)\n",
    "    count += 1\n",
    "\n",
    "  # print training/validation statistics \n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader.dataset)\n",
    "  valid_loss = valid_loss/len(test_loader.dataset)\n",
    "\n",
    "  # accuracy over all test data\n",
    "  valid_acc = num_correct/len(test_loader.dataset)\n",
    "  true_acc = epoch_acc/count\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Acc: {:.6f} \\tTrue Acc: {:.6f}'.format(\n",
    "      e+1, \n",
    "      train_loss,\n",
    "      valid_loss,\n",
    "      valid_acc,\n",
    "      true_acc\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9N7DSwi-VsZ"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'fnn_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CaMP775A-nRw",
    "outputId": "755fc275-eeff-4fcb-ee95-49efdf33a76e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('fnn_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmWebSqxC4eu",
    "outputId": "ce5e1323-ae78-4da4-eaec-e32369eb7600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.170\n",
      "Test accuracy: 0.490\n"
     ]
    }
   ],
   "source": [
    "# changing test_batch_size to 1 to go through all the data and record accuracy over it all\n",
    "test_batch_size=1\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Checking for accuracy\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "    # inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    output = model(inputs)\n",
    "\n",
    "    # calculate loss\n",
    "    test_loss = loss_fn(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    # convert output probabilities to predicted class (0-4)\n",
    "    pred = torch.argmax(output, dim=1)  # rounds to the nearest integer\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22_8h0QqXdCL"
   },
   "source": [
    "## Accuracy value for testing split: 0.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4iskdGqZMI2"
   },
   "outputs": [],
   "source": [
    "# To lower RAM usage so that the session doesn't crash\n",
    "Xw2v_mean=0\n",
    "Yw2v_mean=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6qV0Mfr3byf"
   },
   "source": [
    "## b) Concatenate first 10 words of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9CXXNQOmxrM"
   },
   "outputs": [],
   "source": [
    "# In case session crashed\n",
    "# uncomment below lines and run this cell\n",
    "# if not ignore cell\n",
    "\n",
    "# # Loading pretrained “word2vec-google-news-300” Word2Vec model\n",
    "# import gensim.downloader as api\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import copy\n",
    "# dfc = pd.read_csv(\"cleanedDataset.csv\")\n",
    "# dfc = dfc.loc[:, ~dfc.columns.str.match('Unnamed')] # Remove \"Unnamed\" index column which forms in saved csv files\n",
    "\n",
    "# # Remove null values in case formed\n",
    "# dfc=dfc.dropna()\n",
    "# dfc=dfc.reset_index(drop=True)\n",
    "\n",
    "# # dfc['review']=dfc['review'].astype(str)\n",
    "# # Dividing into X and Y numpy arrays based on reviews and labels\n",
    "# X = dfc['review'].to_numpy()\n",
    "# Y = dfc['rating'].to_numpy()\n",
    "\n",
    "# # Converting review which is a string to a list of tokenized words for training our own word2vec model later\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# for i in range(len(X)):\n",
    "#     X[i] = [t for t in word_tokenize(X[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IesiO_gW38Ke",
    "outputId": "710663ea-9d10-477f-e83d-0eded7bee5b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Concatenating first 10 words of a review\n",
    "Xw2v_con = copy.deepcopy(X)\n",
    "words_in_wv = set(wv.wv.vocab.keys())\n",
    "for i in range(len(Xw2v_con)):\n",
    "  data = Xw2v_con[i]\n",
    "  tmp = np.array([wv[w] for w in Xw2v_con[i][:10] if w in words_in_wv])\n",
    "  if(len(tmp)==0):\n",
    "    tmp = np.zeros((1,300))\n",
    "  data = np.concatenate(tmp, axis=0)\n",
    "\n",
    "  if(len(data)<3000):\n",
    "    data = np.concatenate([data, np.zeros(3000-len(data))])\n",
    "  Xw2v_con[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vu2KJ_4O38ga",
    "outputId": "954563c6-5704-4ac1-f6c8-48a6ab426845"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xw2v_con[[y.size != 3000 for y in Xw2v_con]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Po3CiHt38j5",
    "outputId": "d6a3e073-b0a4-4eec-e36c-018d48685a6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99959,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xw2v_con.shape # same (99959,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6MZ-Qn6T5PM"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(Xw2v_con, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMhKLcLbUBru"
   },
   "outputs": [],
   "source": [
    "# converting to nparray of list as it was an np array of objects with datatype 'object' which needs to be float\n",
    "def arrayToList(my_array):\n",
    "  return np.array(my_array.tolist())\n",
    "\n",
    "train_x = arrayToList(train_x)\n",
    "test_x = arrayToList(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_r1f0SyfUTSB"
   },
   "outputs": [],
   "source": [
    "# reducing labels from 1-5 to 0-4\n",
    "test_y = test_y - 1\n",
    "train_y = train_y - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3ZFTWljVDBP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#create Tensor Dataset\n",
    "# train_data=TensorDataset(torch.cuda.FloatTensor(train_x), torch.cuda.LongTensor(train_y))\n",
    "# test_data=TensorDataset(torch.cuda.FloatTensor(test_x), torch.cuda.LongTensor(test_y))\n",
    "\n",
    "train_data=TensorDataset(torch.FloatTensor(train_x), torch.LongTensor(train_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(test_x), torch.LongTensor(test_y))\n",
    "\n",
    "#dataloader\n",
    "train_batch_size=256\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6Xjp3DWUSLp",
    "outputId": "62b2c478-9349-454c-bd75-54f8226e1eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# MLP class\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # number of hidden nodes in each layer\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (300 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 5)\n",
    "        self.fc3 = nn.Linear(hidden_2, 5)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        # initialize weights\n",
    "        # self.weight_init()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        # x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        # x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def weight_init(self):\n",
    "        # Initialize weights\n",
    "        nn.init.zeros_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc3.weight)\n",
    "        nn.init.ones_(self.fc1.bias)\n",
    "        nn.init.ones_(self.fc2.bias)\n",
    "        nn.init.ones_(self.fc3.bias)\n",
    "        \n",
    "\n",
    "model = MLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMdPP_asVmvq",
    "outputId": "9f66a6cb-bc56-4c67-c00f-d13a596a7ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.615914 \tValidation Loss: 1.612286 \tValidation Acc: 0.195928 \tTrue Acc: 0.195593\n",
      "Epoch: 2 \tTraining Loss: 1.610020 \tValidation Loss: 1.609150 \tValidation Acc: 0.195928 \tTrue Acc: 0.196549\n",
      "Epoch: 3 \tTraining Loss: 1.608314 \tValidation Loss: 1.607956 \tValidation Acc: 0.209834 \tTrue Acc: 0.209817\n",
      "Epoch: 4 \tTraining Loss: 1.607452 \tValidation Loss: 1.607097 \tValidation Acc: 0.233093 \tTrue Acc: 0.232809\n",
      "Epoch: 5 \tTraining Loss: 1.606637 \tValidation Loss: 1.606200 \tValidation Acc: 0.247399 \tTrue Acc: 0.246473\n",
      "Epoch: 6 \tTraining Loss: 1.605682 \tValidation Loss: 1.605131 \tValidation Acc: 0.265906 \tTrue Acc: 0.265724\n",
      "Epoch: 7 \tTraining Loss: 1.604486 \tValidation Loss: 1.603783 \tValidation Acc: 0.283964 \tTrue Acc: 0.283574\n",
      "Epoch: 8 \tTraining Loss: 1.602952 \tValidation Loss: 1.602044 \tValidation Acc: 0.288916 \tTrue Acc: 0.287513\n",
      "Epoch: 9 \tTraining Loss: 1.600944 \tValidation Loss: 1.599732 \tValidation Acc: 0.298119 \tTrue Acc: 0.297567\n",
      "Epoch: 10 \tTraining Loss: 1.598276 \tValidation Loss: 1.596647 \tValidation Acc: 0.307173 \tTrue Acc: 0.306517\n",
      "Epoch: 11 \tTraining Loss: 1.594669 \tValidation Loss: 1.592376 \tValidation Acc: 0.313375 \tTrue Acc: 0.311692\n",
      "Epoch: 12 \tTraining Loss: 1.589643 \tValidation Loss: 1.586344 \tValidation Acc: 0.322979 \tTrue Acc: 0.322620\n",
      "Epoch: 13 \tTraining Loss: 1.582397 \tValidation Loss: 1.577548 \tValidation Acc: 0.327181 \tTrue Acc: 0.327729\n",
      "Epoch: 14 \tTraining Loss: 1.571788 \tValidation Loss: 1.564805 \tValidation Acc: 0.336134 \tTrue Acc: 0.337536\n",
      "Epoch: 15 \tTraining Loss: 1.556427 \tValidation Loss: 1.546807 \tValidation Acc: 0.342837 \tTrue Acc: 0.343684\n",
      "Epoch: 16 \tTraining Loss: 1.535211 \tValidation Loss: 1.522540 \tValidation Acc: 0.348239 \tTrue Acc: 0.347112\n",
      "Epoch: 17 \tTraining Loss: 1.508268 \tValidation Loss: 1.493739 \tValidation Acc: 0.353541 \tTrue Acc: 0.352832\n",
      "Epoch: 18 \tTraining Loss: 1.478341 \tValidation Loss: 1.464311 \tValidation Acc: 0.362445 \tTrue Acc: 0.363545\n",
      "Epoch: 19 \tTraining Loss: 1.449060 \tValidation Loss: 1.437666 \tValidation Acc: 0.373549 \tTrue Acc: 0.371654\n",
      "Epoch: 20 \tTraining Loss: 1.423501 \tValidation Loss: 1.415419 \tValidation Acc: 0.381603 \tTrue Acc: 0.382483\n",
      "Epoch: 21 \tTraining Loss: 1.402811 \tValidation Loss: 1.398292 \tValidation Acc: 0.390656 \tTrue Acc: 0.390477\n",
      "Epoch: 22 \tTraining Loss: 1.386621 \tValidation Loss: 1.385489 \tValidation Acc: 0.393507 \tTrue Acc: 0.393295\n",
      "Epoch: 23 \tTraining Loss: 1.373739 \tValidation Loss: 1.374353 \tValidation Acc: 0.399560 \tTrue Acc: 0.398322\n",
      "Epoch: 24 \tTraining Loss: 1.363385 \tValidation Loss: 1.365520 \tValidation Acc: 0.401361 \tTrue Acc: 0.402970\n",
      "Epoch: 25 \tTraining Loss: 1.354755 \tValidation Loss: 1.358967 \tValidation Acc: 0.403912 \tTrue Acc: 0.404058\n",
      "Epoch: 26 \tTraining Loss: 1.347469 \tValidation Loss: 1.353857 \tValidation Acc: 0.406463 \tTrue Acc: 0.406102\n",
      "Epoch: 27 \tTraining Loss: 1.341441 \tValidation Loss: 1.349360 \tValidation Acc: 0.408263 \tTrue Acc: 0.407882\n",
      "Epoch: 28 \tTraining Loss: 1.335871 \tValidation Loss: 1.345329 \tValidation Acc: 0.412315 \tTrue Acc: 0.411887\n",
      "Epoch: 29 \tTraining Loss: 1.331161 \tValidation Loss: 1.342740 \tValidation Acc: 0.411064 \tTrue Acc: 0.410651\n",
      "Epoch: 30 \tTraining Loss: 1.326945 \tValidation Loss: 1.339503 \tValidation Acc: 0.411965 \tTrue Acc: 0.410585\n",
      "Epoch: 31 \tTraining Loss: 1.323072 \tValidation Loss: 1.337329 \tValidation Acc: 0.414666 \tTrue Acc: 0.414689\n",
      "Epoch: 32 \tTraining Loss: 1.319409 \tValidation Loss: 1.335153 \tValidation Acc: 0.413916 \tTrue Acc: 0.413947\n",
      "Epoch: 33 \tTraining Loss: 1.316115 \tValidation Loss: 1.333767 \tValidation Acc: 0.416417 \tTrue Acc: 0.417375\n",
      "Epoch: 34 \tTraining Loss: 1.313101 \tValidation Loss: 1.332402 \tValidation Acc: 0.416317 \tTrue Acc: 0.415365\n",
      "Epoch: 35 \tTraining Loss: 1.310157 \tValidation Loss: 1.331376 \tValidation Acc: 0.416467 \tTrue Acc: 0.415035\n",
      "Epoch: 36 \tTraining Loss: 1.307496 \tValidation Loss: 1.329224 \tValidation Acc: 0.418567 \tTrue Acc: 0.418068\n",
      "Epoch: 37 \tTraining Loss: 1.304991 \tValidation Loss: 1.328592 \tValidation Acc: 0.418367 \tTrue Acc: 0.417392\n",
      "Epoch: 38 \tTraining Loss: 1.302693 \tValidation Loss: 1.327892 \tValidation Acc: 0.416216 \tTrue Acc: 0.415744\n",
      "Epoch: 39 \tTraining Loss: 1.300254 \tValidation Loss: 1.327017 \tValidation Acc: 0.416967 \tTrue Acc: 0.416007\n",
      "Epoch: 40 \tTraining Loss: 1.298307 \tValidation Loss: 1.326856 \tValidation Acc: 0.418467 \tTrue Acc: 0.418447\n",
      "Epoch: 41 \tTraining Loss: 1.295993 \tValidation Loss: 1.324487 \tValidation Acc: 0.419618 \tTrue Acc: 0.421018\n",
      "Epoch: 42 \tTraining Loss: 1.294065 \tValidation Loss: 1.324178 \tValidation Acc: 0.419768 \tTrue Acc: 0.418776\n",
      "Epoch: 43 \tTraining Loss: 1.292061 \tValidation Loss: 1.323367 \tValidation Acc: 0.419268 \tTrue Acc: 0.420672\n",
      "Epoch: 44 \tTraining Loss: 1.290149 \tValidation Loss: 1.322875 \tValidation Acc: 0.420318 \tTrue Acc: 0.420276\n",
      "Epoch: 45 \tTraining Loss: 1.288209 \tValidation Loss: 1.323016 \tValidation Acc: 0.419218 \tTrue Acc: 0.419188\n",
      "Epoch: 46 \tTraining Loss: 1.286467 \tValidation Loss: 1.323689 \tValidation Acc: 0.421118 \tTrue Acc: 0.420589\n",
      "Epoch: 47 \tTraining Loss: 1.284791 \tValidation Loss: 1.321070 \tValidation Acc: 0.421168 \tTrue Acc: 0.422073\n",
      "Epoch: 48 \tTraining Loss: 1.283022 \tValidation Loss: 1.324189 \tValidation Acc: 0.421719 \tTrue Acc: 0.421661\n",
      "Epoch: 49 \tTraining Loss: 1.281404 \tValidation Loss: 1.322748 \tValidation Acc: 0.422319 \tTrue Acc: 0.424644\n",
      "Epoch: 50 \tTraining Loss: 1.279839 \tValidation Loss: 1.324085 \tValidation Acc: 0.422019 \tTrue Acc: 0.421001\n",
      "Epoch: 51 \tTraining Loss: 1.278279 \tValidation Loss: 1.321995 \tValidation Acc: 0.419868 \tTrue Acc: 0.419353\n",
      "Epoch: 52 \tTraining Loss: 1.276409 \tValidation Loss: 1.319754 \tValidation Acc: 0.422519 \tTrue Acc: 0.423408\n",
      "Epoch: 53 \tTraining Loss: 1.275045 \tValidation Loss: 1.320678 \tValidation Acc: 0.421669 \tTrue Acc: 0.421611\n",
      "Epoch: 54 \tTraining Loss: 1.273379 \tValidation Loss: 1.320825 \tValidation Acc: 0.423770 \tTrue Acc: 0.425600\n",
      "Epoch: 55 \tTraining Loss: 1.271789 \tValidation Loss: 1.319076 \tValidation Acc: 0.420768 \tTrue Acc: 0.421199\n",
      "Epoch: 56 \tTraining Loss: 1.270486 \tValidation Loss: 1.319687 \tValidation Acc: 0.418918 \tTrue Acc: 0.420326\n",
      "Epoch: 57 \tTraining Loss: 1.269169 \tValidation Loss: 1.318343 \tValidation Acc: 0.422269 \tTrue Acc: 0.424117\n",
      "Epoch: 58 \tTraining Loss: 1.267430 \tValidation Loss: 1.320681 \tValidation Acc: 0.421369 \tTrue Acc: 0.420837\n",
      "Epoch: 59 \tTraining Loss: 1.265954 \tValidation Loss: 1.321916 \tValidation Acc: 0.419618 \tTrue Acc: 0.419584\n",
      "Epoch: 60 \tTraining Loss: 1.264491 \tValidation Loss: 1.318312 \tValidation Acc: 0.421469 \tTrue Acc: 0.422369\n"
     ]
    }
   ],
   "source": [
    "test_batch_size=256\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# for accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# training params\n",
    "epochs = 60\n",
    "\n",
    "# initialize device\n",
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# model.cuda()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "  \n",
    "  model.train()\n",
    "  # batch loop\n",
    "  for inputs, labels in train_loader:\n",
    "    # inputs=inputs.cuda()\n",
    "    # labels=labels.cuda()\n",
    "    # labels = labels.type(torch.LongTensor)   # casting to long\n",
    "    # inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # zero accumulated gradients\n",
    "    optimizer.zero_grad()\n",
    "    # get the output from the model\n",
    "    output = model(inputs)\n",
    "    # calculate the loss and perform backprop\n",
    "    loss = loss_fn(output, labels)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*inputs.size(0)\n",
    "\n",
    "  model.eval() # prep model for evaluation\n",
    "  num_correct = 0 # initialize num_correct for correct predictions\n",
    "  epoch_acc=0\n",
    "  count=0\n",
    "  for inputs, labels in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(inputs)\n",
    "    # calculate the loss\n",
    "    loss = loss_fn(output, labels)\n",
    "    # update running validation loss \n",
    "    valid_loss += loss.item()*inputs.size(0)\n",
    "    # print(inputs)\n",
    "    # print(output, labels)\n",
    "    # print(output,labels)\n",
    "    # print(output.dtype,labels.dtype)\n",
    "    # convert output probabilities to predicted class (1-5)\n",
    "    # pred = torch.argmax(output, dim=1, keepdim=True)\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    # pred = torch.round(pred)  # rounds to the nearest integer\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    epoch_acc += accuracy_score(pred, labels)\n",
    "    count += 1\n",
    "\n",
    "  # print training/validation statistics \n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader.dataset)\n",
    "  valid_loss = valid_loss/len(test_loader.dataset)\n",
    "\n",
    "  # accuracy over all test data\n",
    "  valid_acc = num_correct/len(test_loader.dataset)\n",
    "  true_acc = epoch_acc/count\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Acc: {:.6f} \\tTrue Acc: {:.6f}'.format(\n",
    "      e+1, \n",
    "      train_loss,\n",
    "      valid_loss,\n",
    "      valid_acc,\n",
    "      true_acc\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_GZbOBiVq4H"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'fnn_b_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHFKHF-tV3c7",
    "outputId": "908b7a1d-6dee-4107-81bd-f8754a0cd6e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.318\n",
      "Test accuracy: 0.421\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('fnn_b_model.pt'))\n",
    "test_batch_size=1\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Checking for accuracy\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "    # inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    output = model(inputs)\n",
    "\n",
    "    # calculate loss\n",
    "    test_loss = loss_fn(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    # print(inputs)\n",
    "    # print(output, labels)\n",
    "    # convert output probabilities to predicted class (0-4)\n",
    "    pred = torch.argmax(output, dim=1)  # rounds to the nearest integer\n",
    "    # print(pred)\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Itm00W4tZAZ1"
   },
   "source": [
    "## Accuracy for testing split: 0.421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QjN55q9iuND"
   },
   "source": [
    "# Question\n",
    "What do you conclude by comparing accuracy values you obtain with those obtained in the “Simple Models” section?\n",
    "# Answer\n",
    "Based on the results of the simple models, I got accuracy values of:<br> TF-IDF: 0.42 (Perceptron) and 0.50 (SVM)\n",
    "<br>W2V: 0.33 (Perceptron) and 0.48 (SVM)\n",
    "<br>FNN model a) 0.49 and b) 0.42\n",
    "<br>I will conclude that the W2V feature usage improved with the FNN model a) and in FNN model b) the concatenated W2V features of the first 10 words do not seem to connect well to the labels. As not all reviews show their sentiment in the first 10 words and some reviews have less than 10 words (which were concatenated with zero value vectors). Due to these I think the model b) didn't run as well as a). Also, Neural Network models are less sensitive with respect to hyperparameters, and training data preparation is more straightforward and systematic. Nowadays, with more computing power FNN perform way better than traditional SVM and Perceptron models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtzQEjOShr3E"
   },
   "outputs": [],
   "source": [
    "# to lower ram usage so that the session doesn't crash\n",
    "Xw2v_con=0\n",
    "data_iter=0\n",
    "train_x=0\n",
    "test_x=0\n",
    "train_y=0\n",
    "test_y=0\n",
    "test_loader=0\n",
    "train_loader=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-GPcxXEToMB"
   },
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_OB0K1Epb8B"
   },
   "source": [
    "## a) RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea86m98Rl_EA"
   },
   "outputs": [],
   "source": [
    "# In case session crashed\n",
    "# uncomment below lines and run this cell\n",
    "\n",
    "# Loading pretrained “word2vec-google-news-300” Word2Vec model\n",
    "# import gensim.downloader as api\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import copy\n",
    "# dfc = pd.read_csv(\"cleanedDataset.csv\")\n",
    "# dfc = dfc.loc[:, ~dfc.columns.str.match('Unnamed')] # Remove \"Unnamed\" index column which forms in saved csv files\n",
    "\n",
    "# # Remove null values in case formed\n",
    "# dfc=dfc.dropna()\n",
    "# dfc=dfc.reset_index(drop=True)\n",
    "\n",
    "# # dfc['review']=dfc['review'].astype(str)\n",
    "# # Dividing into X and Y numpy arrays based on reviews and labels\n",
    "# X = dfc['review'].to_numpy()\n",
    "# Y = dfc['rating'].to_numpy()\n",
    "\n",
    "# # Converting review which is a string to a list of tokenized words for training our own word2vec model later\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# for i in range(len(X)):\n",
    "#     X[i] = [t for t in word_tokenize(X[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjrc_sPLj4D0",
    "outputId": "8abd4df6-6454-4e22-acb6-b9a3dd45f991",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 46.9 KiB for an array with shape (20, 300) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m   review \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m300\u001b[39m))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(temp)\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m20\u001b[39m:\n\u001b[1;32m---> 15\u001b[0m   review \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m   review \u001b[38;5;241m=\u001b[39m temp\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 46.9 KiB for an array with shape (20, 300) and data type float64"
     ]
    }
   ],
   "source": [
    "# Keeping first 20 words in the review as separate arrays\n",
    "Xw2v_RNN = copy.deepcopy(X)\n",
    "# words_in_wv = set(wv.wv.vocab.keys())\n",
    "for i in range(len(Xw2v_RNN)):\n",
    "  review = Xw2v_RNN[i]\n",
    "\n",
    "  if(len(review)<20):\n",
    "    temp = np.array([wv[w] for w in review if w in wv])\n",
    "  else:\n",
    "    temp = np.array([wv[w] for w in review[:20] if w in wv])\n",
    "    \n",
    "  if len(temp) == 0:\n",
    "    review = np.zeros((20,300))\n",
    "  elif len(temp)<20:\n",
    "    review = np.concatenate([temp, np.zeros((20-len(temp), 300))])\n",
    "  else:\n",
    "    review = temp\n",
    "  Xw2v_RNN[i] = review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-jRWaAipPMM",
    "outputId": "b29700f5-d8ad-4ba6-ee85-4709c3124e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99959\n"
     ]
    }
   ],
   "source": [
    "print(len(Xw2v_RNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYpnp8s47iWy"
   },
   "outputs": [],
   "source": [
    "# To lower RAM usage this one is needed as Xw2v_RNN stores a lot of data\n",
    "wv=0\n",
    "X=0\n",
    "set_words=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yba0MChM18Mj"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(Xw2v_RNN, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4akJYq-3-Ttq"
   },
   "outputs": [],
   "source": [
    "# To lower RAM usage as Xw2v_RNN stores a lot of data\n",
    "Xw2v_RNN=0\n",
    "Y=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdhPqSMI2Fvq"
   },
   "outputs": [],
   "source": [
    "# converting form array of array to array of list object\n",
    "def arrayToList(my_array):\n",
    "  return np.array(my_array.tolist())\n",
    "\n",
    "train_x = arrayToList(train_x)\n",
    "test_x = arrayToList(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unGFJJh522TV"
   },
   "outputs": [],
   "source": [
    "# reducing labels from 1-5 to 0-4\n",
    "test_y = test_y - 1\n",
    "train_y = train_y - 1\n",
    "# run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1FsrkS01W2_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#create Tensor Dataset\n",
    "# train_data=TensorDataset(torch.cuda.FloatTensor(train_x), torch.cuda.LongTensor(train_y))\n",
    "# test_data=TensorDataset(torch.cuda.FloatTensor(test_x), torch.cuda.LongTensor(test_y))\n",
    "\n",
    "train_data=TensorDataset(torch.FloatTensor(train_x), torch.LongTensor(train_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(test_x), torch.LongTensor(test_y))\n",
    "\n",
    "#dataloader\n",
    "train_batch_size=256\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85vywoOjC4KF",
    "outputId": "81c99893-105a-43f4-e53e-3e476ed49cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelRNN(\n",
      "  (rnn): RNN(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class modelRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modelRNN, self).__init__()\n",
    "\n",
    "        # self.hidden_size = hidden_size\n",
    "\n",
    "        # self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.rnn = nn.RNN(300, 20, batch_first = True, nonlinearity='relu')\n",
    "        \n",
    "        self.fc = nn.Linear(20,5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.view(-1,20,300)\n",
    "        output, hidden = self.rnn(output)\n",
    "        output=self.fc(output[:,-1,:])\n",
    "                \n",
    "        # hidden = self.i2h(combined)\n",
    "        # output = self.i2o(combined)\n",
    "        # output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "model = modelRNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqrnnsXUTnAp",
    "outputId": "858c1d2c-0565-4e03-892e-1189c0d6c046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.520409 \tValidation Loss: 1.374635 \tValidation Acc: 0.365346 \tTrue Acc: 0.367369\n",
      "Epoch: 2 \tTraining Loss: 1.327186 \tValidation Loss: 1.310352 \tValidation Acc: 0.410064 \tTrue Acc: 0.409184\n",
      "Epoch: 3 \tTraining Loss: 1.296136 \tValidation Loss: 1.291235 \tValidation Acc: 0.417517 \tTrue Acc: 0.416073\n",
      "Epoch: 4 \tTraining Loss: 1.278782 \tValidation Loss: 1.285204 \tValidation Acc: 0.421869 \tTrue Acc: 0.422287\n",
      "Epoch: 5 \tTraining Loss: 1.263897 \tValidation Loss: 1.267683 \tValidation Acc: 0.432473 \tTrue Acc: 0.434204\n",
      "Epoch: 6 \tTraining Loss: 1.254745 \tValidation Loss: 1.261889 \tValidation Acc: 0.438575 \tTrue Acc: 0.436412\n",
      "Epoch: 7 \tTraining Loss: 1.242926 \tValidation Loss: 1.251841 \tValidation Acc: 0.441327 \tTrue Acc: 0.441044\n",
      "Epoch: 8 \tTraining Loss: 1.233409 \tValidation Loss: 1.242190 \tValidation Acc: 0.445078 \tTrue Acc: 0.444752\n",
      "Epoch: 9 \tTraining Loss: 1.218899 \tValidation Loss: 1.244525 \tValidation Acc: 0.444628 \tTrue Acc: 0.444785\n",
      "Epoch: 10 \tTraining Loss: 1.210522 \tValidation Loss: 1.222393 \tValidation Acc: 0.455582 \tTrue Acc: 0.455614\n",
      "Epoch: 11 \tTraining Loss: 1.202089 \tValidation Loss: 1.220119 \tValidation Acc: 0.458133 \tTrue Acc: 0.459092\n",
      "Epoch: 12 \tTraining Loss: 1.193743 \tValidation Loss: 1.207883 \tValidation Acc: 0.468888 \tTrue Acc: 0.467333\n",
      "Epoch: 13 \tTraining Loss: 1.190483 \tValidation Loss: 1.201257 \tValidation Acc: 0.466236 \tTrue Acc: 0.464712\n",
      "Epoch: 14 \tTraining Loss: 1.186237 \tValidation Loss: 1.208740 \tValidation Acc: 0.464136 \tTrue Acc: 0.464547\n",
      "Epoch: 15 \tTraining Loss: 1.182153 \tValidation Loss: 1.203852 \tValidation Acc: 0.466637 \tTrue Acc: 0.465107\n",
      "Epoch: 16 \tTraining Loss: 1.177770 \tValidation Loss: 1.204391 \tValidation Acc: 0.468788 \tTrue Acc: 0.470580\n",
      "Epoch: 17 \tTraining Loss: 1.176401 \tValidation Loss: 1.197908 \tValidation Acc: 0.467737 \tTrue Acc: 0.468585\n",
      "Epoch: 18 \tTraining Loss: 1.174367 \tValidation Loss: 1.196157 \tValidation Acc: 0.469388 \tTrue Acc: 0.469739\n",
      "Epoch: 19 \tTraining Loss: 1.168787 \tValidation Loss: 1.224757 \tValidation Acc: 0.457183 \tTrue Acc: 0.455284\n",
      "Epoch: 20 \tTraining Loss: 1.167068 \tValidation Loss: 1.216939 \tValidation Acc: 0.462435 \tTrue Acc: 0.461432\n",
      "Epoch: 21 \tTraining Loss: 1.167302 \tValidation Loss: 1.188922 \tValidation Acc: 0.477341 \tTrue Acc: 0.478079\n",
      "Epoch: 22 \tTraining Loss: 1.163489 \tValidation Loss: 1.187193 \tValidation Acc: 0.475690 \tTrue Acc: 0.475491\n",
      "Epoch: 23 \tTraining Loss: 1.159175 \tValidation Loss: 1.197193 \tValidation Acc: 0.473840 \tTrue Acc: 0.476530\n",
      "Epoch: 24 \tTraining Loss: 1.156808 \tValidation Loss: 1.187506 \tValidation Acc: 0.473739 \tTrue Acc: 0.473085\n",
      "Epoch: 25 \tTraining Loss: 1.158092 \tValidation Loss: 1.187172 \tValidation Acc: 0.474090 \tTrue Acc: 0.476777\n",
      "Epoch: 26 \tTraining Loss: 1.152939 \tValidation Loss: 1.183202 \tValidation Acc: 0.477941 \tTrue Acc: 0.478194\n",
      "Epoch: 27 \tTraining Loss: 1.153067 \tValidation Loss: 1.186684 \tValidation Acc: 0.478241 \tTrue Acc: 0.475623\n",
      "Epoch: 28 \tTraining Loss: 1.151991 \tValidation Loss: 1.187440 \tValidation Acc: 0.472339 \tTrue Acc: 0.472656\n",
      "Epoch: 29 \tTraining Loss: 1.150592 \tValidation Loss: 1.192793 \tValidation Acc: 0.471839 \tTrue Acc: 0.473118\n",
      "Epoch: 30 \tTraining Loss: 1.148596 \tValidation Loss: 1.181340 \tValidation Acc: 0.473039 \tTrue Acc: 0.472871\n",
      "Epoch: 31 \tTraining Loss: 1.146782 \tValidation Loss: 1.181132 \tValidation Acc: 0.476240 \tTrue Acc: 0.476991\n",
      "Epoch: 32 \tTraining Loss: 1.145357 \tValidation Loss: 1.182354 \tValidation Acc: 0.477691 \tTrue Acc: 0.477469\n",
      "Epoch: 33 \tTraining Loss: 1.142559 \tValidation Loss: 1.179886 \tValidation Acc: 0.478691 \tTrue Acc: 0.478458\n",
      "Epoch: 34 \tTraining Loss: 1.144799 \tValidation Loss: 1.183193 \tValidation Acc: 0.477941 \tTrue Acc: 0.477238\n",
      "Epoch: 35 \tTraining Loss: 1.145732 \tValidation Loss: 1.184279 \tValidation Acc: 0.478391 \tTrue Acc: 0.478639\n",
      "Epoch: 36 \tTraining Loss: 1.140653 \tValidation Loss: 1.192799 \tValidation Acc: 0.470088 \tTrue Acc: 0.471387\n",
      "Epoch: 37 \tTraining Loss: 1.139743 \tValidation Loss: 1.195611 \tValidation Acc: 0.470338 \tTrue Acc: 0.470200\n",
      "Epoch: 38 \tTraining Loss: 1.140521 \tValidation Loss: 1.188995 \tValidation Acc: 0.469338 \tTrue Acc: 0.470167\n",
      "Epoch: 39 \tTraining Loss: 1.138235 \tValidation Loss: 1.177827 \tValidation Acc: 0.478541 \tTrue Acc: 0.477354\n",
      "Epoch: 40 \tTraining Loss: 1.135770 \tValidation Loss: 1.183954 \tValidation Acc: 0.474840 \tTrue Acc: 0.473217\n",
      "Epoch: 41 \tTraining Loss: 1.137562 \tValidation Loss: 1.179009 \tValidation Acc: 0.477691 \tTrue Acc: 0.477947\n",
      "Epoch: 42 \tTraining Loss: 1.135611 \tValidation Loss: 1.182314 \tValidation Acc: 0.477141 \tTrue Acc: 0.478359\n",
      "Epoch: 43 \tTraining Loss: 1.133445 \tValidation Loss: 1.180019 \tValidation Acc: 0.480842 \tTrue Acc: 0.482496\n",
      "Epoch: 44 \tTraining Loss: 1.134711 \tValidation Loss: 1.192364 \tValidation Acc: 0.474790 \tTrue Acc: 0.475079\n",
      "Epoch: 45 \tTraining Loss: 1.132560 \tValidation Loss: 1.179560 \tValidation Acc: 0.477391 \tTrue Acc: 0.477172\n",
      "Epoch: 46 \tTraining Loss: 1.131557 \tValidation Loss: 1.184698 \tValidation Acc: 0.479492 \tTrue Acc: 0.480205\n",
      "Epoch: 47 \tTraining Loss: 1.131764 \tValidation Loss: 1.177154 \tValidation Acc: 0.480942 \tTrue Acc: 0.481639\n",
      "Epoch: 48 \tTraining Loss: 1.132424 \tValidation Loss: 1.176155 \tValidation Acc: 0.479692 \tTrue Acc: 0.479447\n",
      "Epoch: 49 \tTraining Loss: 1.128676 \tValidation Loss: 1.182031 \tValidation Acc: 0.480542 \tTrue Acc: 0.480765\n",
      "Epoch: 50 \tTraining Loss: 1.129839 \tValidation Loss: 1.189581 \tValidation Acc: 0.477141 \tTrue Acc: 0.475969\n",
      "Epoch: 51 \tTraining Loss: 1.129213 \tValidation Loss: 1.178053 \tValidation Acc: 0.479742 \tTrue Acc: 0.479496\n",
      "Epoch: 52 \tTraining Loss: 1.129527 \tValidation Loss: 1.186880 \tValidation Acc: 0.475940 \tTrue Acc: 0.475260\n",
      "Epoch: 53 \tTraining Loss: 1.126020 \tValidation Loss: 1.175934 \tValidation Acc: 0.481593 \tTrue Acc: 0.482282\n",
      "Epoch: 54 \tTraining Loss: 1.125871 \tValidation Loss: 1.176113 \tValidation Acc: 0.482493 \tTrue Acc: 0.482694\n",
      "Epoch: 55 \tTraining Loss: 1.123880 \tValidation Loss: 1.174304 \tValidation Acc: 0.478641 \tTrue Acc: 0.477931\n",
      "Epoch: 56 \tTraining Loss: 1.124386 \tValidation Loss: 1.176484 \tValidation Acc: 0.477741 \tTrue Acc: 0.477040\n",
      "Epoch: 57 \tTraining Loss: 1.123545 \tValidation Loss: 1.196997 \tValidation Acc: 0.477941 \tTrue Acc: 0.475804\n",
      "Epoch: 58 \tTraining Loss: 1.125199 \tValidation Loss: 1.175152 \tValidation Acc: 0.481393 \tTrue Acc: 0.482084\n",
      "Epoch: 59 \tTraining Loss: 1.124212 \tValidation Loss: 1.180442 \tValidation Acc: 0.479092 \tTrue Acc: 0.478376\n",
      "Epoch: 60 \tTraining Loss: 1.121319 \tValidation Loss: 1.179581 \tValidation Acc: 0.476641 \tTrue Acc: 0.476431\n",
      "Epoch: 61 \tTraining Loss: 1.120669 \tValidation Loss: 1.185288 \tValidation Acc: 0.478792 \tTrue Acc: 0.480469\n",
      "Epoch: 62 \tTraining Loss: 1.122047 \tValidation Loss: 1.177669 \tValidation Acc: 0.481743 \tTrue Acc: 0.483864\n",
      "Epoch: 63 \tTraining Loss: 1.121459 \tValidation Loss: 1.187135 \tValidation Acc: 0.474540 \tTrue Acc: 0.474354\n",
      "Epoch: 64 \tTraining Loss: 1.120740 \tValidation Loss: 1.182638 \tValidation Acc: 0.475440 \tTrue Acc: 0.474288\n",
      "Epoch: 65 \tTraining Loss: 1.118695 \tValidation Loss: 1.176784 \tValidation Acc: 0.478992 \tTrue Acc: 0.479711\n",
      "Epoch: 66 \tTraining Loss: 1.118241 \tValidation Loss: 1.176601 \tValidation Acc: 0.482593 \tTrue Acc: 0.481837\n",
      "Epoch: 67 \tTraining Loss: 1.118970 \tValidation Loss: 1.181563 \tValidation Acc: 0.481343 \tTrue Acc: 0.479645\n",
      "Epoch: 68 \tTraining Loss: 1.118572 \tValidation Loss: 1.189025 \tValidation Acc: 0.474290 \tTrue Acc: 0.476019\n",
      "Epoch: 69 \tTraining Loss: 1.116970 \tValidation Loss: 1.176422 \tValidation Acc: 0.482343 \tTrue Acc: 0.482545\n",
      "Epoch: 70 \tTraining Loss: 1.115981 \tValidation Loss: 1.180109 \tValidation Acc: 0.476841 \tTrue Acc: 0.475194\n",
      "Epoch: 71 \tTraining Loss: 1.117571 \tValidation Loss: 1.177710 \tValidation Acc: 0.480492 \tTrue Acc: 0.479282\n",
      "Epoch: 72 \tTraining Loss: 1.116669 \tValidation Loss: 1.183548 \tValidation Acc: 0.474390 \tTrue Acc: 0.474206\n",
      "Epoch: 73 \tTraining Loss: 1.113274 \tValidation Loss: 1.181725 \tValidation Acc: 0.481843 \tTrue Acc: 0.480617\n",
      "Epoch: 74 \tTraining Loss: 1.114079 \tValidation Loss: 1.186839 \tValidation Acc: 0.475390 \tTrue Acc: 0.474717\n",
      "Epoch: 75 \tTraining Loss: 1.113815 \tValidation Loss: 1.183931 \tValidation Acc: 0.480992 \tTrue Acc: 0.483122\n",
      "Epoch: 76 \tTraining Loss: 1.113567 \tValidation Loss: 1.183156 \tValidation Acc: 0.476391 \tTrue Acc: 0.478573\n",
      "Epoch: 77 \tTraining Loss: 1.113393 \tValidation Loss: 1.180865 \tValidation Acc: 0.481242 \tTrue Acc: 0.481458\n",
      "Epoch: 78 \tTraining Loss: 1.114932 \tValidation Loss: 1.180213 \tValidation Acc: 0.482693 \tTrue Acc: 0.482892\n",
      "Epoch: 79 \tTraining Loss: 1.114264 \tValidation Loss: 1.191046 \tValidation Acc: 0.478291 \tTrue Acc: 0.477106\n",
      "Epoch: 80 \tTraining Loss: 1.111553 \tValidation Loss: 1.181798 \tValidation Acc: 0.479942 \tTrue Acc: 0.479216\n",
      "Epoch: 81 \tTraining Loss: 1.111976 \tValidation Loss: 1.185360 \tValidation Acc: 0.478892 \tTrue Acc: 0.478656\n",
      "Epoch: 82 \tTraining Loss: 1.110891 \tValidation Loss: 1.179883 \tValidation Acc: 0.479892 \tTrue Acc: 0.480123\n",
      "Epoch: 83 \tTraining Loss: 1.111682 \tValidation Loss: 1.197279 \tValidation Acc: 0.471238 \tTrue Acc: 0.468701\n",
      "Epoch: 84 \tTraining Loss: 1.110215 \tValidation Loss: 1.180013 \tValidation Acc: 0.481643 \tTrue Acc: 0.481375\n",
      "Epoch: 85 \tTraining Loss: 1.110022 \tValidation Loss: 1.190347 \tValidation Acc: 0.477341 \tTrue Acc: 0.475689\n",
      "Epoch: 86 \tTraining Loss: 1.109565 \tValidation Loss: 1.206991 \tValidation Acc: 0.474140 \tTrue Acc: 0.474436\n",
      "Epoch: 87 \tTraining Loss: 1.109996 \tValidation Loss: 1.192901 \tValidation Acc: 0.472839 \tTrue Acc: 0.474585\n",
      "Epoch: 88 \tTraining Loss: 1.111181 \tValidation Loss: 1.182090 \tValidation Acc: 0.479092 \tTrue Acc: 0.476942\n",
      "Epoch: 89 \tTraining Loss: 1.108613 \tValidation Loss: 1.187443 \tValidation Acc: 0.478942 \tTrue Acc: 0.478705\n",
      "Epoch: 90 \tTraining Loss: 1.107324 \tValidation Loss: 1.195749 \tValidation Acc: 0.476841 \tTrue Acc: 0.476628\n",
      "Epoch: 91 \tTraining Loss: 1.109681 \tValidation Loss: 1.180605 \tValidation Acc: 0.480042 \tTrue Acc: 0.480749\n",
      "Epoch: 92 \tTraining Loss: 1.108220 \tValidation Loss: 1.190114 \tValidation Acc: 0.476741 \tTrue Acc: 0.476530\n",
      "Epoch: 93 \tTraining Loss: 1.107958 \tValidation Loss: 1.187026 \tValidation Acc: 0.478191 \tTrue Acc: 0.477008\n",
      "Epoch: 94 \tTraining Loss: 1.106683 \tValidation Loss: 1.188439 \tValidation Acc: 0.480392 \tTrue Acc: 0.478705\n",
      "Epoch: 95 \tTraining Loss: 1.106019 \tValidation Loss: 1.190140 \tValidation Acc: 0.477091 \tTrue Acc: 0.477832\n",
      "Epoch: 96 \tTraining Loss: 1.106991 \tValidation Loss: 1.188839 \tValidation Acc: 0.481843 \tTrue Acc: 0.482051\n",
      "Epoch: 97 \tTraining Loss: 1.109421 \tValidation Loss: 1.179004 \tValidation Acc: 0.478291 \tTrue Acc: 0.479496\n",
      "Epoch: 98 \tTraining Loss: 1.105668 \tValidation Loss: 1.176042 \tValidation Acc: 0.481242 \tTrue Acc: 0.480980\n",
      "Epoch: 99 \tTraining Loss: 1.104733 \tValidation Loss: 1.182502 \tValidation Acc: 0.478591 \tTrue Acc: 0.478837\n",
      "Epoch: 100 \tTraining Loss: 1.103868 \tValidation Loss: 1.196816 \tValidation Acc: 0.476541 \tTrue Acc: 0.476810\n",
      "Epoch: 101 \tTraining Loss: 1.105689 \tValidation Loss: 1.190646 \tValidation Acc: 0.478241 \tTrue Acc: 0.478013\n",
      "Epoch: 102 \tTraining Loss: 1.105653 \tValidation Loss: 1.193147 \tValidation Acc: 0.479242 \tTrue Acc: 0.479480\n",
      "Epoch: 103 \tTraining Loss: 1.105459 \tValidation Loss: 1.198474 \tValidation Acc: 0.475090 \tTrue Acc: 0.474898\n",
      "Epoch: 104 \tTraining Loss: 1.102910 \tValidation Loss: 1.193735 \tValidation Acc: 0.483743 \tTrue Acc: 0.483452\n",
      "Epoch: 105 \tTraining Loss: 1.103541 \tValidation Loss: 1.184458 \tValidation Acc: 0.483343 \tTrue Acc: 0.483056\n",
      "Epoch: 106 \tTraining Loss: 1.104951 \tValidation Loss: 1.183655 \tValidation Acc: 0.478541 \tTrue Acc: 0.478788\n",
      "Epoch: 107 \tTraining Loss: 1.104522 \tValidation Loss: 1.178918 \tValidation Acc: 0.483193 \tTrue Acc: 0.484342\n",
      "Epoch: 108 \tTraining Loss: 1.102251 \tValidation Loss: 1.185303 \tValidation Acc: 0.478641 \tTrue Acc: 0.478886\n",
      "Epoch: 109 \tTraining Loss: 1.104466 \tValidation Loss: 1.182828 \tValidation Acc: 0.478041 \tTrue Acc: 0.477337\n",
      "Epoch: 110 \tTraining Loss: 1.100210 \tValidation Loss: 1.194009 \tValidation Acc: 0.479892 \tTrue Acc: 0.480601\n",
      "Epoch: 111 \tTraining Loss: 1.103782 \tValidation Loss: 1.210612 \tValidation Acc: 0.467137 \tTrue Acc: 0.467992\n",
      "Epoch: 112 \tTraining Loss: 1.104019 \tValidation Loss: 1.183500 \tValidation Acc: 0.482443 \tTrue Acc: 0.482644\n",
      "Epoch: 113 \tTraining Loss: 1.102137 \tValidation Loss: 1.181553 \tValidation Acc: 0.483043 \tTrue Acc: 0.483238\n",
      "Epoch: 114 \tTraining Loss: 1.102653 \tValidation Loss: 1.183126 \tValidation Acc: 0.480742 \tTrue Acc: 0.480485\n",
      "Epoch: 115 \tTraining Loss: 1.099447 \tValidation Loss: 1.189022 \tValidation Acc: 0.481192 \tTrue Acc: 0.482842\n",
      "Epoch: 116 \tTraining Loss: 1.102697 \tValidation Loss: 1.198321 \tValidation Acc: 0.476541 \tTrue Acc: 0.474898\n",
      "Epoch: 117 \tTraining Loss: 1.101720 \tValidation Loss: 1.190944 \tValidation Acc: 0.483693 \tTrue Acc: 0.484359\n",
      "Epoch: 118 \tTraining Loss: 1.099795 \tValidation Loss: 1.190314 \tValidation Acc: 0.478591 \tTrue Acc: 0.479793\n",
      "Epoch: 119 \tTraining Loss: 1.100525 \tValidation Loss: 1.190889 \tValidation Acc: 0.482293 \tTrue Acc: 0.484408\n",
      "Epoch: 120 \tTraining Loss: 1.100011 \tValidation Loss: 1.192227 \tValidation Acc: 0.481393 \tTrue Acc: 0.482562\n"
     ]
    }
   ],
   "source": [
    "test_batch_size=256\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# for accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training params\n",
    "epochs = 120\n",
    "\n",
    "# initialize device\n",
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# model.cuda()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "  \n",
    "  model.train()\n",
    "  # batch loop\n",
    "  for inputs, labels in train_loader:\n",
    "    # inputs=inputs.cuda()\n",
    "    # labels=labels.cuda()\n",
    "    # labels = labels.type(torch.LongTensor)   # casting to long\n",
    "    # inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # zero accumulated gradients\n",
    "    optimizer.zero_grad()\n",
    "    # get the output from the model\n",
    "    output = model(inputs)\n",
    "    # calculate the loss and perform backprop\n",
    "    loss = loss_fn(output, labels)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*inputs.size(0)\n",
    "\n",
    "  model.eval() # prep model for evaluation\n",
    "  num_correct = 0 # initialize num_correct for correct predictions\n",
    "  epoch_acc=0\n",
    "  count=0\n",
    "  for inputs, labels in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(inputs)\n",
    "    # calculate the loss\n",
    "    loss = loss_fn(output, labels)\n",
    "    # update running validation loss \n",
    "    valid_loss += loss.item()*inputs.size(0)\n",
    "    # print(inputs)\n",
    "    # print(output, labels)\n",
    "    # print(output,labels)\n",
    "    # print(output.dtype,labels.dtype)\n",
    "    # convert output probabilities to predicted class (1-5)\n",
    "    # pred = torch.argmax(output, dim=1, keepdim=True)\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    # pred = torch.round(pred)  # rounds to the nearest integer\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    epoch_acc += accuracy_score(pred, labels)\n",
    "    count += 1\n",
    "\n",
    "  # print training/validation statistics \n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader.dataset)\n",
    "  valid_loss = valid_loss/len(test_loader.dataset)\n",
    "\n",
    "  # accuracy over all test data\n",
    "  valid_acc = num_correct/len(test_loader.dataset)\n",
    "  true_acc = epoch_acc/count\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Acc: {:.6f} \\tTrue Acc: {:.6f}'.format(\n",
    "      e+1, \n",
    "      train_loss,\n",
    "      valid_loss,\n",
    "      valid_acc,\n",
    "      true_acc\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNjv5JjAlGoy"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'rnn_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2CdwFcV7NSd",
    "outputId": "6d6725c9-80fc-467e-e04b-83df564b4cf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.192\n",
      "Test accuracy: 0.481\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('rnn_model.pt'))\n",
    "test_batch_size=1\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Checking for accuracy\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "    # inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    output = model(inputs)\n",
    "\n",
    "    # calculate loss\n",
    "    test_loss = loss_fn(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    # print(inputs)\n",
    "    # print(output, labels)\n",
    "    # convert output probabilities to predicted class (0-4)\n",
    "    pred = torch.argmax(output, dim=1)  # rounds to the nearest integer\n",
    "    # print(pred)\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H07OHRCFnyjX"
   },
   "source": [
    "## Accuracy for RNN: 0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr6zpm2tvfbX"
   },
   "source": [
    "# Question\n",
    "What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models?\n",
    "# Answer\n",
    "For FNN model a) accuracy = 0.49 and for FNN model b) accuracy = 0.42. Now for RNN we got accuracy 0.48. So we can conclude that RNN is performing with similar accuracy as compared to FNN. If we could use more than 20 words in the review we could achieve higher accuracy for RNN. Also, if we could use a different value to fill vector values instead of just using zeros to fill up reviews with less than 20 words.\n",
    "we could achieve higher accuracy for RNN.<br>\n",
    "FNN model doesn't seem to obtain the context of the data, long term dependencies of some words and also the information on order of the words is lost. RNN seems to handle this well especially with the historical weight it uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlVF8rzyphIj"
   },
   "source": [
    "## b) **GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aVAOIw7nscs"
   },
   "outputs": [],
   "source": [
    "# In case session crashed\n",
    "# uncomment below lines and run this cell\n",
    "\n",
    "# # Loading pretrained “word2vec-google-news-300” Word2Vec model\n",
    "# import gensim.downloader as api\n",
    "# wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "# dfc = pd.read_csv(\"cleanedDataset.csv\")\n",
    "# dfc = dfc.loc[:, ~dfc.columns.str.match('Unnamed')] # Remove \"Unnamed\" index column which forms in saved csv files\n",
    "\n",
    "# # Remove null values in case formed\n",
    "# dfc=dfc.dropna()\n",
    "# dfc=dfc.reset_index(drop=True)\n",
    "\n",
    "# # dfc['review']=dfc['review'].astype(str)\n",
    "# # Dividing into X and Y numpy arrays based on reviews and labels\n",
    "# X = dfc['review'].to_numpy()\n",
    "# Y = dfc['rating'].to_numpy()\n",
    "\n",
    "# # Converting review which is a string to a list of tokenized words for training our own word2vec model later\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# for i in range(len(X)):\n",
    "#     X[i] = [t for t in word_tokenize(X[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UdA-kG-rpjVY",
    "outputId": "14747e88-35a1-4548-93b3-763c1b5f58c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelGRU(\n",
      "  (gru): GRU(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class modelGRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modelGRU, self).__init__()\n",
    "\n",
    "        # self.hidden_size = hidden_size\n",
    "\n",
    "        # self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.gru = nn.GRU(300, 20, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(20,5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.view(-1,20,300)\n",
    "        output, hidden = self.gru(output)\n",
    "        output=self.fc(output[:,-1,:])\n",
    "                \n",
    "        # hidden = self.i2h(combined)\n",
    "        # output = self.i2o(combined)\n",
    "        # output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "model = modelGRU()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuYr_XJrBbgu",
    "outputId": "2f4d6404-7742-4122-f269-786f61f68d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.435480 \tValidation Loss: 1.304984 \tValidation Acc: 0.412915 \tTrue Acc: 0.414392\n",
      "Epoch: 2 \tTraining Loss: 1.258649 \tValidation Loss: 1.226030 \tValidation Acc: 0.457833 \tTrue Acc: 0.458795\n",
      "Epoch: 3 \tTraining Loss: 1.206383 \tValidation Loss: 1.195828 \tValidation Acc: 0.473689 \tTrue Acc: 0.473513\n",
      "Epoch: 4 \tTraining Loss: 1.183951 \tValidation Loss: 1.188490 \tValidation Acc: 0.475090 \tTrue Acc: 0.476810\n",
      "Epoch: 5 \tTraining Loss: 1.169610 \tValidation Loss: 1.175286 \tValidation Acc: 0.480592 \tTrue Acc: 0.480337\n",
      "Epoch: 6 \tTraining Loss: 1.157736 \tValidation Loss: 1.168972 \tValidation Acc: 0.485294 \tTrue Acc: 0.485941\n",
      "Epoch: 7 \tTraining Loss: 1.148287 \tValidation Loss: 1.158876 \tValidation Acc: 0.487195 \tTrue Acc: 0.487342\n",
      "Epoch: 8 \tTraining Loss: 1.140887 \tValidation Loss: 1.159673 \tValidation Acc: 0.489046 \tTrue Acc: 0.488693\n",
      "Epoch: 9 \tTraining Loss: 1.133778 \tValidation Loss: 1.154216 \tValidation Acc: 0.491397 \tTrue Acc: 0.492451\n",
      "Epoch: 10 \tTraining Loss: 1.128104 \tValidation Loss: 1.152526 \tValidation Acc: 0.489646 \tTrue Acc: 0.491677\n",
      "Epoch: 11 \tTraining Loss: 1.122525 \tValidation Loss: 1.145485 \tValidation Acc: 0.493347 \tTrue Acc: 0.493424\n",
      "Epoch: 12 \tTraining Loss: 1.117413 \tValidation Loss: 1.147623 \tValidation Acc: 0.492397 \tTrue Acc: 0.492484\n",
      "Epoch: 13 \tTraining Loss: 1.112598 \tValidation Loss: 1.146149 \tValidation Acc: 0.492797 \tTrue Acc: 0.494792\n",
      "Epoch: 14 \tTraining Loss: 1.108719 \tValidation Loss: 1.141988 \tValidation Acc: 0.497599 \tTrue Acc: 0.498105\n",
      "Epoch: 15 \tTraining Loss: 1.104907 \tValidation Loss: 1.139981 \tValidation Acc: 0.497599 \tTrue Acc: 0.497627\n",
      "Epoch: 16 \tTraining Loss: 1.100909 \tValidation Loss: 1.137984 \tValidation Acc: 0.497049 \tTrue Acc: 0.497083\n",
      "Epoch: 17 \tTraining Loss: 1.098715 \tValidation Loss: 1.137686 \tValidation Acc: 0.499500 \tTrue Acc: 0.499984\n",
      "Epoch: 18 \tTraining Loss: 1.094139 \tValidation Loss: 1.137141 \tValidation Acc: 0.497449 \tTrue Acc: 0.497000\n",
      "Epoch: 19 \tTraining Loss: 1.090873 \tValidation Loss: 1.138753 \tValidation Acc: 0.496949 \tTrue Acc: 0.496984\n",
      "Epoch: 20 \tTraining Loss: 1.088722 \tValidation Loss: 1.140056 \tValidation Acc: 0.497649 \tTrue Acc: 0.497676\n"
     ]
    }
   ],
   "source": [
    "# batch size  back to 256 from 1 done in RNN\n",
    "test_batch_size=256\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# for accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training params\n",
    "epochs = 20\n",
    "\n",
    "# initialize device\n",
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# model.cuda()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "  \n",
    "  model.train()\n",
    "  # batch loop\n",
    "  for inputs, labels in train_loader:\n",
    "    # inputs=inputs.cuda()\n",
    "    # labels=labels.cuda()\n",
    "    # labels = labels.type(torch.LongTensor)   # casting to long\n",
    "    # inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # zero accumulated gradients\n",
    "    optimizer.zero_grad()\n",
    "    # get the output from the model\n",
    "    output = model(inputs)\n",
    "    # calculate the loss and perform backprop\n",
    "    loss = loss_fn(output, labels)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*inputs.size(0)\n",
    "\n",
    "  model.eval() # prep model for evaluation\n",
    "  num_correct = 0 # initialize num_correct for correct predictions\n",
    "  epoch_acc=0\n",
    "  count=0\n",
    "  for inputs, labels in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(inputs)\n",
    "    # calculate the loss\n",
    "    loss = loss_fn(output, labels)\n",
    "    # update running validation loss \n",
    "    valid_loss += loss.item()*inputs.size(0)\n",
    "    # print(inputs)\n",
    "    # print(output, labels)\n",
    "    # print(output,labels)\n",
    "    # print(output.dtype,labels.dtype)\n",
    "    # convert output probabilities to predicted class (1-5)\n",
    "    # pred = torch.argmax(output, dim=1, keepdim=True)\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    # pred = torch.round(pred)  # rounds to the nearest integer\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    epoch_acc += accuracy_score(pred, labels)\n",
    "    count += 1\n",
    "\n",
    "  # print training/validation statistics \n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader.dataset)\n",
    "  valid_loss = valid_loss/len(test_loader.dataset)\n",
    "\n",
    "  # accuracy over all test data\n",
    "  valid_acc = num_correct/len(test_loader.dataset)\n",
    "  true_acc = epoch_acc/count\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tValidation Acc: {:.6f} \\tTrue Acc: {:.6f}'.format(\n",
    "      e+1, \n",
    "      train_loss,\n",
    "      valid_loss,\n",
    "      valid_acc,\n",
    "      true_acc\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48Ek2ZKHBbps"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'gru_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_QVB3aEBncg",
    "outputId": "66269694-4148-4a00-8404-f99c406e8e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.140\n",
      "Test accuracy: 0.498\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('gru_model.pt'))\n",
    "test_batch_size=1\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Checking for accuracy\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "    # inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    output = model(inputs)\n",
    "\n",
    "    # calculate loss\n",
    "    test_loss = loss_fn(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    # print(inputs)\n",
    "    # print(output, labels)\n",
    "    # convert output probabilities to predicted class (0-4)\n",
    "    pred = torch.argmax(output, dim=1)  # rounds to the nearest integer\n",
    "    # print(pred)\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    # correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMHodVz-n4o6"
   },
   "source": [
    "## Accuracy for GRU: 0.498"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0zUZKZ0vPZ9"
   },
   "source": [
    "# Question\n",
    "What do you conclude by comparing accuracy values you obtain with those obtained using simple RNN?\n",
    "# Answer\n",
    "For RNN we got accuracy of 0.48 and for GRU we got accuracy of 0.498. So we can conclude that GRU improved in accuracy by a very tiny amount. Conceptually, RNN faces vanishing/exploding gradients problem, which could affect the way it learns and ultimately lowering its accuracy. Whereas, Gated RNN can learn long term dependencies and can control how much information they pass on. And its usage of tanh function solves the vanishing/exploding gradient problems."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
