{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d65dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim.downloader as api\n",
    "import gensim.models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gc\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3eb19",
   "metadata": {},
   "source": [
    "# Task 1: Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5176f682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n",
    "df = df[['star_rating', 'review_body']]\n",
    "class_one = df[(df['star_rating']==1) | (df['star_rating']==2)]\n",
    "class_two = df[df['star_rating']==3]\n",
    "class_three = df[(df['star_rating']==4) | (df['star_rating']==5)]\n",
    "\n",
    "class_one.loc[:, \"label\"] =1\n",
    "class_two.loc[:, \"label\"] =2\n",
    "class_three.loc[:, \"label\"] =3\n",
    "\n",
    "class_one = class_one.sample(n=20000, random_state=100)\n",
    "class_two = class_two.sample(n=20000, random_state=100)\n",
    "class_three = class_three.sample(n=20000, random_state=100)\n",
    "dataset = pd.concat([class_one, class_two, class_three])\n",
    "\n",
    "dataset.reset_index(drop=True)\n",
    "train = dataset.sample(frac=0.8, random_state=100)\n",
    "test = dataset.drop(train.index)\n",
    "\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)\n",
    "\n",
    "train.to_csv('train.csv', index = False)\n",
    "test.to_csv('test.csv', index = False)\n",
    "\n",
    "del globals()['class_one'], globals()['class_two'], globals()['class_three'], globals()['dataset'], globals()['df']\n",
    "train = test = df = dataset = [[99999, 99999]]\n",
    "del df, train, test, dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084dd4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pretrained Word2Vec model:\n",
    "pretrained_w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71968073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check semantic similarities of the generated vectors:\n",
      "[('queen', 0.7118193507194519)]\n",
      "Excellent ~ Outstanding: 0.55674857\n",
      "time ~ schedule: 0.26993576\n"
     ]
    }
   ],
   "source": [
    "print('Check semantic similarities of the generated vectors:')\n",
    "print(pretrained_w2v.most_similar(positive=['king', 'woman'], negative=['man'], topn = 1))\n",
    "print('Excellent ~ Outstanding:', pretrained_w2v.similarity('excellent', 'outstanding'))\n",
    "print('time ~ schedule:', pretrained_w2v.similarity('time', 'schedule'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40149bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d5b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert all reviews to lower case\n",
    "train['review_body'] = train['review_body'].str.lower()\n",
    "test['review_body'] = test['review_body'].str.lower()\n",
    "\n",
    "'''\n",
    "URL Remover code\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "test['review_body'] = test['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "def html_tag_remover(review):\n",
    "    soup = BeautifulSoup(review, 'html.parser')\n",
    "    review = soup.get_text()\n",
    "    return review  \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: html_tag_remover(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: html_tag_remover(review))\n",
    "\n",
    "'''\n",
    "remove non-alphabetical characters\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n",
    "\n",
    "'''\n",
    "remove extra spaces\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub(' +', ' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub(' +', ' ', review))\n",
    "\n",
    "'''\n",
    "perform contractions on the reviews\n",
    "'''\n",
    "def expand_contractions(review):\n",
    "    review = contractions.fix(review)\n",
    "    return review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: expand_contractions(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: expand_contractions(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7713746",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "remove the stop words AND perform lemmatization\n",
    "\n",
    "'''\n",
    "avg_len_before_prepro = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    stop_words_english = set(stopwords.words('english'))\n",
    "    review_word_tokens = word_tokenize(review)\n",
    "    filtered_review = [word for word in review_word_tokens if not word in stop_words_english]\n",
    "    return filtered_review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: remove_stopwords(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: remove_stopwords(review))\n",
    "\n",
    "def review_lemmatize(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_review = [lemmatizer.lemmatize(word) for word in review]\n",
    "    return ' '.join(lemmatized_review)    \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "\n",
    "avg_len_after_prepro = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0836a8",
   "metadata": {},
   "source": [
    "# Task 2: Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38dd8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Sentences = [sentence.split(' ') for sentence in train['review_body'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e685c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Word2Vec\n",
    "custom_model = gensim.models.Word2Vec(all_Sentences, vector_size = 300, min_count=9, window=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31afd952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check semantic similarities of the generated vectors:\n",
      "('touted', 0.7799292802810669)\n",
      "Excellent ~ Outstanding: 0.70792377\n",
      "time ~ schedule: 0.17524442\n"
     ]
    }
   ],
   "source": [
    "print('Check semantic similarities of the generated vectors:')\n",
    "print(custom_model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn = 1)[0])\n",
    "print('Excellent ~ Outstanding:', custom_model.wv.similarity('excellent', 'outstanding'))\n",
    "print('time ~ schedule:', custom_model.wv.similarity('time', 'schedule'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e8889e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_Sentences, custom_model\n",
    "gc.collect()\n",
    "all_Sentences = [1]\n",
    "custom_model = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67a25c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = dir()\n",
    "# a = sorted(a, key = lambda x: -getsizeof(x))\n",
    "# a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821ed51",
   "metadata": {},
   "source": [
    "# Task 3: Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fc2a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word2Vec vectors\n",
    "# all_sentence_vector = pretrained_w2v\n",
    "# del pretrained_w2v\n",
    "# gc.collect()\n",
    "# pretrained_w2v = [1]\n",
    "def average_vectors(review, label):\n",
    "    temp_review = review.split(' ')\n",
    "    review_vector = np.array([pretrained_w2v[word] for word in temp_review if word in pretrained_w2v])\n",
    "    if len(review_vector) >=1:\n",
    "#         review_vector = []\n",
    "#         for word in words:\n",
    "#             review_vector.append(pretrained_w2v[word])\n",
    "        return review_vector, label\n",
    "\n",
    "def average_vectors_concat(review, label):\n",
    "    temp_review = review.split(' ')\n",
    "    review_vector = np.array([pretrained_w2v[word] for word in temp_review[:10] if word in pretrained_w2v])\n",
    "        \n",
    "#     review_vector = []\n",
    "#     for word in words:\n",
    "#         review_vector.append(pretrained_w2v[word])\n",
    "#     review_vector = np.array(review_vector)\n",
    "    \n",
    "    # can be the case where the words in the review are not found in the W2V vocabulary\n",
    "    if len(review_vector)==0:\n",
    "        review_vector = np.zeros((1, 300))\n",
    "    review_vector = np.concatenate(review_vector, axis=0)\n",
    "    \n",
    "    # In the case where the total dim of the feature vector is <3000 add the padding with zeros\n",
    "    if len(review_vector)<3000:\n",
    "        review_vector = np.concatenate([review_vector, np.zeros(3000-len(data))])\n",
    "    return review_vector/10, label\n",
    "    \n",
    "    \n",
    "def featurization(dataset, concat = False):\n",
    "    features = []\n",
    "    y_labels = []\n",
    "    concat = concat\n",
    "    \n",
    "    for review, label in zip(dataset['review_body'], dataset['label']):\n",
    "        try:\n",
    "            if not concat:\n",
    "                x, y = average_vectors(review, label)\n",
    "                features.append(np.mean(x, axis=0))\n",
    "            else:\n",
    "                x, y = average_vectors_concat(review, label)\n",
    "                features.append(x)\n",
    "                \n",
    "            y_labels.append(y)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    return features, y_labels\n",
    "\n",
    "# Vectors without concatenation\n",
    "w2v_pretrain_train_x, w2v_pretrain_train_y = featurization(train)\n",
    "w2v_pretrain_test_x, w2v_pretrain_test_y = featurization(test)\n",
    "\n",
    "# Vectors with concatenation\n",
    "w2v_pretrain_train_concat_x, w2v_pretrain_train_concat_y = featurization(train, True)\n",
    "w2v_pretrain_test_concat_x, w2v_pretrain_test_concat_y = featurization(test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5239dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Feature Extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 0.001)\n",
    "tfidf_X_train = tfidf_vectorizer.fit_transform(list(train['review_body']))\n",
    "tfidf_X_train = pd.DataFrame(tfidf_X_train.toarray())\n",
    "\n",
    "tfidf_X_test = tfidf_vectorizer.transform(list(test['review_body']))\n",
    "tfidf_X_test = pd.DataFrame(tfidf_X_test.toarray())\n",
    "\n",
    "tfidf_Y_train = train['label']\n",
    "tfidf_Y_test = test['label']\n",
    "\n",
    "tfidf_Y_train = tfidf_Y_train.astype('int')\n",
    "tfidf_Y_test = tfidf_Y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bebea69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Perceptron Model on Average Word2Vec Features\n",
    "perceptr_w2v = Perceptron(random_state = 100, eta0=0.1)\n",
    "perceptr_w2v.fit(w2v_pretrain_train_x, w2v_pretrain_train_y)\n",
    "Y_pred_w2v_test = perceptr_w2v.predict(w2v_pretrain_test_x)\n",
    "\n",
    "# Training Perceptron Model on TF-IDF Features\n",
    "perceptr_tfidf = Perceptron(random_state = 100, eta0=0.1)\n",
    "perceptr_tfidf.fit(tfidf_X_train, tfidf_Y_train)\n",
    "Y_pred_tfidf_test = perceptr_tfidf.predict(tfidf_X_test)\n",
    "\n",
    "target_names = ['class 1', 'class 2', 'class 3']\n",
    "report_w2v = classification_report(w2v_pretrain_test_y, Y_pred_w2v_test, target_names=target_names, output_dict=True)\n",
    "report_tfidf = classification_report(tfidf_Y_test, Y_pred_tfidf_test, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7515f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy values PERCEPTRON for w2v and tfidf features:\n",
      "0.5805374728759807 0.6170833333333333\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy values PERCEPTRON for w2v and tfidf features:')\n",
    "print(report_w2v['accuracy'], report_tfidf['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a35497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training SVM Model on Average Word2Vec Features\n",
    "svm_w2v = LinearSVC(random_state=100, max_iter=1000)\n",
    "svm_w2v.fit(w2v_pretrain_train_x, w2v_pretrain_train_y)\n",
    "Y_pred_w2v_svm_test = svm_w2v.predict(w2v_pretrain_test_x)\n",
    "\n",
    "# Training SVM Model on TFIDF Features\n",
    "svm_tfidf = LinearSVC(random_state=100, max_iter=1000)\n",
    "svm_tfidf.fit(tfidf_X_train, tfidf_Y_train)\n",
    "Y_pred_tfidf_svm_test = svm_tfidf.predict(tfidf_X_test)\n",
    "\n",
    "report_svm_w2v = classification_report(w2v_pretrain_test_y, Y_pred_w2v_svm_test, target_names=target_names, output_dict=True)\n",
    "report_svm_tfidf = classification_report(tfidf_Y_test, Y_pred_tfidf_svm_test, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66f59b74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy values SVM for w2v and tfidf features:\n",
      "0.627691537305959 0.6685\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy values SVM for w2v and tfidf features:')\n",
    "print(report_svm_w2v['accuracy'], report_svm_tfidf['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8a1d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "del globals()['pretrained_w2v']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c042fdd",
   "metadata": {},
   "source": [
    "# Task 4: Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f23f3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e243849",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e99310ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n",
      "MLP_concat(\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 300):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)  \n",
    "        \n",
    "#         self.sig = nn.Sigmoid()\n",
    "#         self.soft = nn.Softmax(dim = 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class MLP_concat(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 3000):\n",
    "        super(MLP_concat, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)  \n",
    "        \n",
    "#         self.sig = nn.Sigmoid()\n",
    "#         self.soft = nn.Softmax(dim = 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "model_concat = MLP_concat()\n",
    "model = model\n",
    "model_concat = model_concat\n",
    "print(model)\n",
    "print(model_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17d6f7",
   "metadata": {},
   "source": [
    "-- Task 4(a) using the average Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95bf3f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.098787 \tValidation Loss: 1.096163 \tEpoch Accuracy: 0.402020\n"
     ]
    }
   ],
   "source": [
    "train_data=TensorDataset(torch.FloatTensor(w2v_pretrain_train_x), torch.LongTensor(w2v_pretrain_train_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(w2v_pretrain_test_x), torch.LongTensor(w2v_pretrain_test_y))\n",
    "\n",
    "# Data Loader\n",
    "train_batch_size=16\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_batch_size=16\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 1\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, (target-1))\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, (target-1))\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == (target-1)).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bea9c9",
   "metadata": {},
   "source": [
    "-- Test Dataset Accuracy classwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cac3daa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.08      0.14      4085\n",
      "           1       0.35      0.75      0.48      3979\n",
      "           2       0.51      0.39      0.44      3918\n",
      "\n",
      "    accuracy                           0.40     11982\n",
      "   macro avg       0.48      0.40      0.35     11982\n",
      "weighted avg       0.48      0.40      0.35     11982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test_batch_size= len(w2v_pretrain_test_x)\n",
    "# test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "model.eval() # prep model for evaluation\n",
    "main_tar = []\n",
    "predss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, (target-1))\n",
    "        # update running validation loss \n",
    "        #valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        #correct += (ypred == target-1).float().sum()\n",
    "        for i in np.array(target-1):\n",
    "            main_tar.append(i)\n",
    "        for j in np.array(ypred):\n",
    "            predss.append(j)\n",
    "        \n",
    "#         mat = confusion_matrix((target-1),ypred)\n",
    "#         ans = mat.diagonal()/mat.sum(axis=1)\n",
    "        \n",
    "# print(\"Accuracy Values for each class\")\n",
    "\n",
    "# for i,acc in enumerate(ans):\n",
    "#     print(f\"Class {i+1} : {acc: .6f}\")\n",
    "print(classification_report(main_tar, predss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12cf86",
   "metadata": {},
   "source": [
    "Task 4(b) 10 word vectors concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce0fb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # del globals()['w2v_pretrain_test_x']\n",
    "# del w2v_pretrain_train_x, w2v_pretrain_train_y, w2v_pretrain_test_x, w2v_pretrain_test_y\n",
    "\n",
    "# gc.collect()\n",
    "# # del df\n",
    "# df = [1]\n",
    "# model = [1]\n",
    "# model_concat = [1]\n",
    "# train_data = [1]\n",
    "# test_data = [1]\n",
    "# w2v_pretrain_train_x = [1]\n",
    "# w2v_pretrain_train_y = [1]\n",
    "# w2v_pretrain_test_x = [1]\n",
    "# w2v_pretrain_test_y = [1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "538558ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.978633 \tValidation Loss: 0.925617 \tEpoch Accuracy: 0.551729\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data=TensorDataset(torch.FloatTensor(w2v_pretrain_train_concat_x), torch.LongTensor(w2v_pretrain_train_concat_y))\n",
    "\n",
    "test_data=TensorDataset(torch.FloatTensor(w2v_pretrain_test_concat_x), torch.LongTensor(w2v_pretrain_test_concat_y))\n",
    "\n",
    "# Data Loader\n",
    "train_batch_size=16\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_batch_size=16\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.Adam(model_concat.parameters(), lr=0.002)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 1\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model_concat.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model_concat.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16d917b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Values for each class\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.61      0.58      2600\n",
      "           1       0.51      0.43      0.47      2602\n",
      "           2       0.59      0.61      0.60      2434\n",
      "\n",
      "    accuracy                           0.55      7636\n",
      "   macro avg       0.55      0.55      0.55      7636\n",
      "weighted avg       0.55      0.55      0.55      7636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test_batch_size= len(w2v_pretrain_test_concat_x)\n",
    "# test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "model_concat.eval() # prep model for evaluation\n",
    "main_tar = []\n",
    "predss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        #valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        #correct += (ypred == target-1).float().sum()\n",
    "        for i in np.array(target-1):\n",
    "            main_tar.append(i)\n",
    "        for j in np.array(ypred):\n",
    "            predss.append(j)\n",
    "#         mat = confusion_matrix(target-1,ypred)\n",
    "#         ans = mat.diagonal()/mat.sum(axis=1)\n",
    "        \n",
    "print(\"Accuracy Values for each class\")\n",
    "\n",
    "# for i,acc in enumerate(ans):\n",
    "#     print(f\"Class {i+1} : {acc: .6f}\")\n",
    "print(classification_report(main_tar, predss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe698dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del globals()['tfidf_X_train'], globals()['tfidf_X_test'], globals()['tfidf_Y_train'], globals()['tfidf_Y_test']\n",
    "\n",
    "del globals()['w2v_pretrain_train_x'], globals()['w2v_pretrain_train_y']\n",
    "del globals()['w2v_pretrain_test_x'], globals()['w2v_pretrain_test_y']\n",
    "\n",
    "del globals()['w2v_pretrain_train_concat_x'], globals()['w2v_pretrain_train_concat_y']\n",
    "del globals()['w2v_pretrain_test_concat_x'], globals()['w2v_pretrain_test_concat_y']\n",
    "\n",
    "del globals()['model'], globals()['model_concat'], globals()['train_data'], globals()['test_data']\n",
    "del globals()['Y_pred_w2v_test'], globals()['Y_pred_tfidf_test'], globals()['Y_pred_w2v_svm_test'], globals()['Y_pred_tfidf_svm_test']\n",
    "\n",
    "del globals()['train_loader'], globals()['test_loader']\n",
    "del globals()['main_tar'], globals()['predss']\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a9aa9",
   "metadata": {},
   "source": [
    "# Task 5 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bcf18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained Word2Vec model:\n",
    "pretrained_w2v = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8373e8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_13376\\3504712409.py\", line 43, in <cell line: 43>\n",
      "    w2v_pretrain_test_x, w2v_pretrain_test_y = featurization_rnn(test)\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_13376\\3504712409.py\", line 33, in featurization_rnn\n",
      "    x = average_vectors_rnn(review)\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Temp\\ipykernel_13376\\3504712409.py\", line 23, in average_vectors_rnn\n",
      "    review_vector = np.concatenate([review_vector, np.zeros((20-len(review_vector), 300))])\n",
      "  File \"<__array_function__ internals>\", line 200, in concatenate\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 46.9 KiB for an array with shape (20, 300) and data type float64\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\executing\\executing.py\", line 317, in executing\n",
      "    args = executing_cache[key]\n",
      "KeyError: (<code object run_code at 0x000001C4CBC5EC30, file \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3362>, 1944743963696, 76)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 799, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 854, in get_records\n",
      "    return list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\core.py\", line 565, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\utils.py\", line 84, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\core.py\", line 555, in mapper\n",
      "    return cls(f, options)\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\core.py\", line 520, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\executing\\executing.py\", line 369, in executing\n",
      "    args = find(source=cls.for_frame(frame), retry_cache=True)\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\executing\\executing.py\", line 252, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "  File \"C:\\Users\\Ayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\executing\\executing.py\", line 269, in for_filename\n",
      "    lines = tuple(linecache.getlines(filename, module_globals))\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "limiting the maximum review length to 20 by truncating longer reviews and padding\n",
    "shorter reviews with a null value (0)\n",
    "'''\n",
    "# Average word2Vec vectors\n",
    "def average_vectors_rnn(review):\n",
    "    temp_review = review.split(' ')\n",
    "   \n",
    "        \n",
    "#     review_vector = []\n",
    "#     for word in words:\n",
    "#         review_vector.append()\n",
    "#     review_vector = np.array(review_vector)\n",
    "\n",
    "    review_vector = np.array([pretrained_w2v[word] for word in temp_review[:20] if word in pretrained_w2v])\n",
    "    \n",
    "    # can be the case where the words in the review are not found in the W2V vocabulary\n",
    "    if len(review_vector)==0:\n",
    "        review_vector = np.zeros((20, 300))\n",
    "    \n",
    "    # In the case where the total dim of the feature vector is <20 add the padding with zeros\n",
    "    elif len(review_vector)<20:\n",
    "        review_vector = np.concatenate([review_vector, np.zeros((20-len(review_vector), 300))])\n",
    "        \n",
    "    return review_vector\n",
    "    \n",
    "    \n",
    "def featurization_rnn(dataset):\n",
    "    features = []\n",
    "\n",
    "    for review in dataset['review_body']:\n",
    "#         try:\n",
    "        x = average_vectors_rnn(review)\n",
    "        features.append(x)\n",
    "        \n",
    "#         except:\n",
    "#             pass\n",
    "    return features\n",
    "\n",
    "# Vectors without concatenation\n",
    "w2v_pretrain_train_x = featurization_rnn(train)\n",
    "w2v_pretrain_train_y = train['label']\n",
    "w2v_pretrain_test_x, w2v_pretrain_test_y = featurization_rnn(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dbe6512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# del globals()['w2v_pretrain_train_x'], globals()['w2v_pretrain_train_y']\n",
    "# del globals()['w2v_pretrain_test_x'], globals()['w2v_pretrain_test_y']\n",
    "# del globals()['perceptr_w2v'], globals()['perceptr_tfidf'], globals()['svm_w2v'], globals()['svm_tfidf']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f98cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(modelRNN, self).__init__()\n",
    "\n",
    "        # self.hidden_size = hidden_size\n",
    "\n",
    "        # self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.rnn = nn.RNN(300, 20, batch_first = True, nonlinearity='relu')\n",
    "        \n",
    "        self.fc = nn.Linear(20,5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.view(-1,20,300)\n",
    "        output, hidden = self.rnn(output)\n",
    "        output=self.fc(output[:,-1,:])\n",
    "                \n",
    "        # hidden = self.i2h(combined)\n",
    "        # output = self.i2o(combined)\n",
    "        # output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "model = modelRNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=TensorDataset(torch.FloatTensor(w2v_pretrain_train_x), torch.LongTensor(w2v_pretrain_train_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(w2v_pretrain_test_x), torch.LongTensor(w2v_pretrain_test_y))\n",
    "\n",
    "# Data Loader\n",
    "train_batch_size=16\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_batch_size=16\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.Adam(model_concat.parameters(), lr=0.002)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 1\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model_concat.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model_concat.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
