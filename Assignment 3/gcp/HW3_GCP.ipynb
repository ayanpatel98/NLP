{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5965d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install bs4\n",
    "# ! pip install contractions\n",
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca7dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d65dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/ayanpatel_69/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ayanpatel_69/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/ayanpatel_69/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim.downloader as api\n",
    "import gensim.models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gc\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3eb19",
   "metadata": {},
   "source": [
    "# Task 1: Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5176f682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n",
    "df = df[['star_rating', 'review_body']]\n",
    "\n",
    "# Classwise dataset generation step\n",
    "class_one = df[(df['star_rating']==1) | (df['star_rating']==2)]\n",
    "class_two = df[df['star_rating']==3]\n",
    "class_three = df[(df['star_rating']==4) | (df['star_rating']==5)]\n",
    "\n",
    "class_one.loc[:, \"label\"] =1\n",
    "class_two.loc[:, \"label\"] =2\n",
    "class_three.loc[:, \"label\"] =3\n",
    "\n",
    "class_one = class_one.sample(n=20000, random_state=100)\n",
    "class_two = class_two.sample(n=20000, random_state=100)\n",
    "class_three = class_three.sample(n=20000, random_state=100)\n",
    "df = pd.concat([class_one, class_two, class_three])\n",
    "\n",
    "df.reset_index(drop=True)\n",
    "\n",
    "# Final Train Test 80/20 Split\n",
    "train = df.sample(frac=0.8, random_state=100)\n",
    "test = df.drop(train.index)\n",
    "\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)\n",
    "\n",
    "# clearing some memory\n",
    "del globals()['class_one'], globals()['class_two'], globals()['class_three'], globals()['df']\n",
    "df = dataset = [[99999, 99999]]\n",
    "del df, dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d5b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert all reviews to lower case\n",
    "train['review_body'] = train['review_body'].str.lower()\n",
    "test['review_body'] = test['review_body'].str.lower()\n",
    "\n",
    "'''\n",
    "URL Remover code\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "test['review_body'] = test['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "def html_tag_remover(review):\n",
    "    soup = BeautifulSoup(review, 'html.parser')\n",
    "    review = soup.get_text()\n",
    "    return review  \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: html_tag_remover(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: html_tag_remover(review))\n",
    "\n",
    "'''\n",
    "remove non-alphabetical characters\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n",
    "\n",
    "'''\n",
    "remove extra spaces\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub(' +', ' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub(' +', ' ', review))\n",
    "\n",
    "'''\n",
    "perform contractions on the reviews\n",
    "'''\n",
    "def expand_contractions(review):\n",
    "    review = contractions.fix(review)\n",
    "    return review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: expand_contractions(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: expand_contractions(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7713746",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "remove the stop words AND perform lemmatization\n",
    "\n",
    "'''\n",
    "def remove_stopwords(review):\n",
    "    stop_words_english = set(stopwords.words('english'))\n",
    "    review_word_tokens = word_tokenize(review)\n",
    "    filtered_review = [word for word in review_word_tokens if not word in stop_words_english]\n",
    "    return filtered_review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: remove_stopwords(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: remove_stopwords(review))\n",
    "\n",
    "def review_lemmatize(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_review = [lemmatizer.lemmatize(word) for word in review]\n",
    "    return ' '.join(lemmatized_review)    \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: review_lemmatize(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0836a8",
   "metadata": {},
   "source": [
    "# Task 2: Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41cb643",
   "metadata": {},
   "source": [
    "# Task 2(a) pretrained “word2vec-google-news-300” Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "084dd4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading Pretrained Word2Vec model:\n",
    "pretrained_w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71968073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check semantic similarities of the generated vectors:\n",
      "[('queen', 0.7118193507194519)]\n",
      "Excellent ~ Outstanding: 0.5567486\n",
      "time ~ schedule: 0.26993576\n"
     ]
    }
   ],
   "source": [
    "print('Check semantic similarities of the generated vectors:')\n",
    "print(pretrained_w2v.most_similar(positive=['king', 'woman'], negative=['man'], topn = 1))\n",
    "print('Excellent ~ Outstanding:', pretrained_w2v.similarity('excellent', 'outstanding'))\n",
    "print('time ~ schedule:', pretrained_w2v.similarity('time', 'schedule'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56db2da",
   "metadata": {},
   "source": [
    "# Task 2(b) Word2Vec model using your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38dd8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating list of all the words corresponding to its sentence\n",
    "all_Sentences = [sentence.split(' ') for sentence in train['review_body'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e685c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Word2Vec Setting the embedding size to be 300 and the window size to be 13. \n",
    "custom_model = gensim.models.Word2Vec(all_Sentences, vector_size = 300, min_count=9, window=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31afd952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check semantic similarities of the generated vectors:\n",
      "('ray', 0.768917441368103)\n",
      "Excellent ~ Outstanding: 0.751618\n",
      "time ~ schedule: 0.16850077\n"
     ]
    }
   ],
   "source": [
    "print('Check semantic similarities of the generated vectors:')\n",
    "print(custom_model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn = 1)[0])\n",
    "print('Excellent ~ Outstanding:', custom_model.wv.similarity('excellent', 'outstanding'))\n",
    "print('time ~ schedule:', custom_model.wv.similarity('time', 'schedule'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8889e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing some memory\n",
    "del all_Sentences, custom_model\n",
    "gc.collect()\n",
    "all_Sentences = [1]\n",
    "custom_model = [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99759768",
   "metadata": {},
   "source": [
    "# Question Task 2:\n",
    "What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec\n",
    "models seems to encode semantic similarities between words better?\n",
    "\n",
    "# Answer:\n",
    "The pre-trained Word2vec model seems to encode semantic similarities better than my trained model. Pre-trained model encodes similarities better than my model because it got a lot of information from all the words it was trained on. Pretrained word2vec models that are trained on large, diverse corpus are generally known to perform well in capturing semantic similarities between words. I see that there are many more word keys generated by the pre-trained vort2vec model. This is because it has been trained on a very large dataset for a long time, making it a more accurate model for embedding words from many different words. Word2Vec is trained on the Google News dataset (about 100 billion words). It has several use cases like recommendation engines, knowledge discovery, and what we use it for, text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821ed51",
   "metadata": {},
   "source": [
    "# Task 3: Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fc2a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature INPUT Vectors FOR BOTH TRAIN AND TEST DATA created for both Task 3 and Task 4 WHICH INCLUDES FEATURE VECTORS \n",
    "OF BOTH AVERAGE WORD2VEC VECTORS AND CONCATENATED THE FIRST 10 Word2Vec VECTORS for each review \n",
    "'''\n",
    "# Calculates Average word2Vec vectors\n",
    "def average_vectors(review, label):\n",
    "    temp_review = review.split(' ')\n",
    "    review_vector = np.array([pretrained_w2v[word] for word in temp_review if word in pretrained_w2v])\n",
    "    if len(review_vector) >=1:\n",
    "#         review_vector = []\n",
    "#         for word in words:\n",
    "#             review_vector.append(pretrained_w2v[word])\n",
    "        return review_vector, label\n",
    "\n",
    "# Calculates concatenated first 10 Word2Vec Vectors\n",
    "def average_vectors_concat(review, label):\n",
    "    temp_review = review.split(' ')\n",
    "    words = np.array([word for word in temp_review[:10] if word in pretrained_w2v])\n",
    "    review_vector = []\n",
    "    for word in words:\n",
    "        review_vector.append(pretrained_w2v[word])\n",
    "    \n",
    "    # can be the case where the words in the review are not found in the W2V vocabulary\n",
    "    if len(review_vector)==0:\n",
    "        review_vector = np.zeros((1, 300))\n",
    "    review_vector = np.concatenate(review_vector, axis=0)\n",
    "    \n",
    "    # In the case where the total dim of the feature vector is <3000 add the padding with zeros\n",
    "    if len(review_vector)<3000:\n",
    "        review_vector = np.concatenate([review_vector, np.zeros(3000-len(review_vector))])\n",
    "    return review_vector/10, label    \n",
    "    \n",
    "def featurization(dataset, concat = False):\n",
    "    features = []\n",
    "    y_labels = []\n",
    "    concat = concat\n",
    "    \n",
    "    for review, label in zip(dataset['review_body'], dataset['label']):\n",
    "        try:\n",
    "            if not concat:\n",
    "                x, y = average_vectors(review, label)\n",
    "                features.append(np.mean(x, axis=0))\n",
    "            else:\n",
    "                x, y = average_vectors_concat(review, label)\n",
    "                features.append(x)\n",
    "                \n",
    "            y_labels.append(y)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    return features, y_labels\n",
    "\n",
    "'''\n",
    "Driver code for calculation of Word2Vec Vectors\n",
    "'''\n",
    "# Vectors without concatenation\n",
    "# Average Word2Vec Vectors for train and test data\n",
    "w2v_pretrain_train_x, w2v_pretrain_train_y = featurization(train)\n",
    "w2v_pretrain_test_x, w2v_pretrain_test_y = featurization(test)\n",
    "\n",
    "# Vectors with concatenation\n",
    "# Concatenated first 10 Word2Vec Vectors for train and test data\n",
    "w2v_pretrain_train_concat_x, w2v_pretrain_train_concat_y = featurization(train, True)\n",
    "w2v_pretrain_test_concat_x, w2v_pretrain_test_concat_y = featurization(test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5239dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TF-IDF Feature Extraction for both train and test data\n",
    "'''\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 0.001)\n",
    "\n",
    "# Final TFIDF Features\n",
    "tfidf_X_train = tfidf_vectorizer.fit_transform(list(train['review_body']))\n",
    "tfidf_X_train = pd.DataFrame(tfidf_X_train.toarray())\n",
    "\n",
    "tfidf_X_test = tfidf_vectorizer.transform(list(test['review_body']))\n",
    "tfidf_X_test = pd.DataFrame(tfidf_X_test.toarray())\n",
    "\n",
    "tfidf_Y_train = train['label']\n",
    "tfidf_Y_test = test['label']\n",
    " \n",
    "tfidf_Y_train = tfidf_Y_train.astype('int')\n",
    "tfidf_Y_test = tfidf_Y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bebea69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training Perceptron Model on Average Word2Vec Features\n",
    "'''\n",
    "perceptr_w2v = Perceptron(random_state = 100, eta0=0.1)\n",
    "perceptr_w2v.fit(w2v_pretrain_train_x, w2v_pretrain_train_y)\n",
    "Y_pred_w2v_test = perceptr_w2v.predict(w2v_pretrain_test_x)\n",
    "\n",
    "'''\n",
    "Training Perceptron Model on TF-IDF Features\n",
    "'''\n",
    "perceptr_tfidf = Perceptron(random_state = 100, eta0=0.1)\n",
    "perceptr_tfidf.fit(tfidf_X_train, tfidf_Y_train)\n",
    "Y_pred_tfidf_test = perceptr_tfidf.predict(tfidf_X_test)\n",
    "\n",
    "# Accuracy Calculation\n",
    "target_names = ['class 1', 'class 2', 'class 3']\n",
    "report_w2v = classification_report(w2v_pretrain_test_y, Y_pred_w2v_test, target_names=target_names, output_dict=True)\n",
    "report_tfidf = classification_report(tfidf_Y_test, Y_pred_tfidf_test, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7515f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy values PERCEPTRON for w2v and tfidf features:\n",
      "0.5805374728759807 0.6170833333333333\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy values PERCEPTRON for w2v and tfidf features:')\n",
    "print(report_w2v['accuracy'], report_tfidf['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a35497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training SVM Model on Average Word2Vec Features\n",
    "'''\n",
    "svm_w2v = LinearSVC(random_state=100, max_iter=1000)\n",
    "svm_w2v.fit(w2v_pretrain_train_x, w2v_pretrain_train_y)\n",
    "Y_pred_w2v_svm_test = svm_w2v.predict(w2v_pretrain_test_x)\n",
    "\n",
    "'''\n",
    "Training SVM Model on TFIDF Features\n",
    "'''\n",
    "svm_tfidf = LinearSVC(random_state=100, max_iter=1000)\n",
    "svm_tfidf.fit(tfidf_X_train, tfidf_Y_train)\n",
    "Y_pred_tfidf_svm_test = svm_tfidf.predict(tfidf_X_test)\n",
    "\n",
    "# Accuracy Calculation\n",
    "report_svm_w2v = classification_report(w2v_pretrain_test_y, Y_pred_w2v_svm_test, target_names=target_names, output_dict=True)\n",
    "report_svm_tfidf = classification_report(tfidf_Y_test, Y_pred_tfidf_svm_test, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66f59b74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy values SVM for w2v and tfidf features:\n",
      "0.627691537305959 0.6685\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy values SVM for w2v and tfidf features:')\n",
    "print(report_svm_w2v['accuracy'], report_svm_tfidf['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a626d",
   "metadata": {},
   "source": [
    "# Question Task 3:\n",
    "What do you conclude from comparing performances for the models trained using the two \n",
    "different feature types (TF-IDF and your trained Word2Vec features)?\n",
    "\n",
    "# Answer:\n",
    "In my experiments, I found that TF-IDF features outperformed Word2vec features in both the Perceptron and SVM models. Although the SVM model with Word2vec took a long time to train, possibly due to the time required to determine the margin, overall the SVM model performed better than the Perceptron model. The TF-IDF feature set contained 48,000 features per review, while the Word2vec feature set contained only 300 features obtained by averaging all the words in a review. The reason for the poorer performance of Word2vec may be that averaging the word vector values results in the loss of information connecting the feature to the label, and this data is not suitable for simple models like the Perceptron. Furthermore, TF-IDF is a statistical measure that is specific to the dataset, whereas Word2vec embeddings are based on a pretrained vector that may not be specific to this dataset and contain a large amount of unrelated information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c042fdd",
   "metadata": {},
   "source": [
    "# Task 4: Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f23f3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e243849",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e99310ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n",
      "MLP_concat(\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FNN: network with two hidden layers, each with 100 and 10 nodes\n",
    "'''\n",
    "# For 4(a)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 300):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# For 4(b)\n",
    "class MLP_concat(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 3000):\n",
    "        super(MLP_concat, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "model_concat = MLP_concat()\n",
    "model = model\n",
    "model_concat = model_concat\n",
    "print(model)\n",
    "print(model_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17d6f7",
   "metadata": {},
   "source": [
    "## -- Task 4(a) using the average Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95bf3f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.095245 \tValidation Loss: 1.085394 \tEpoch Accuracy: 0.438241\n",
      "Epoch: 2 \tTraining Loss: 1.039120 \tValidation Loss: 0.984465 \tEpoch Accuracy: 0.529377\n",
      "Epoch: 3 \tTraining Loss: 0.932123 \tValidation Loss: 0.912589 \tEpoch Accuracy: 0.561008\n",
      "Epoch: 4 \tTraining Loss: 0.889675 \tValidation Loss: 0.886198 \tEpoch Accuracy: 0.579369\n",
      "Epoch: 5 \tTraining Loss: 0.866262 \tValidation Loss: 0.868845 \tEpoch Accuracy: 0.598565\n",
      "Epoch: 6 \tTraining Loss: 0.846607 \tValidation Loss: 0.858551 \tEpoch Accuracy: 0.603155\n",
      "Epoch: 7 \tTraining Loss: 0.833461 \tValidation Loss: 0.838942 \tEpoch Accuracy: 0.622601\n",
      "Epoch: 8 \tTraining Loss: 0.824110 \tValidation Loss: 0.836921 \tEpoch Accuracy: 0.620347\n",
      "Epoch: 9 \tTraining Loss: 0.818204 \tValidation Loss: 0.832157 \tEpoch Accuracy: 0.624019\n",
      "Epoch: 10 \tTraining Loss: 0.812506 \tValidation Loss: 0.829871 \tEpoch Accuracy: 0.625438\n",
      "Epoch: 11 \tTraining Loss: 0.808316 \tValidation Loss: 0.823904 \tEpoch Accuracy: 0.629778\n",
      "Epoch: 12 \tTraining Loss: 0.804297 \tValidation Loss: 0.857766 \tEpoch Accuracy: 0.608746\n",
      "Epoch: 13 \tTraining Loss: 0.801441 \tValidation Loss: 0.817335 \tEpoch Accuracy: 0.633283\n",
      "Epoch: 14 \tTraining Loss: 0.798735 \tValidation Loss: 0.823436 \tEpoch Accuracy: 0.629194\n",
      "Epoch: 15 \tTraining Loss: 0.794912 \tValidation Loss: 0.814164 \tEpoch Accuracy: 0.632699\n",
      "Epoch: 16 \tTraining Loss: 0.792207 \tValidation Loss: 0.813080 \tEpoch Accuracy: 0.635453\n",
      "Epoch: 17 \tTraining Loss: 0.789171 \tValidation Loss: 0.815732 \tEpoch Accuracy: 0.633951\n",
      "Epoch: 18 \tTraining Loss: 0.785690 \tValidation Loss: 0.808095 \tEpoch Accuracy: 0.636872\n",
      "Epoch: 19 \tTraining Loss: 0.783736 \tValidation Loss: 0.809173 \tEpoch Accuracy: 0.640210\n",
      "Epoch: 20 \tTraining Loss: 0.781308 \tValidation Loss: 0.846638 \tEpoch Accuracy: 0.616007\n"
     ]
    }
   ],
   "source": [
    "train_data=TensorDataset(torch.FloatTensor(w2v_pretrain_train_x), torch.LongTensor(w2v_pretrain_train_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(w2v_pretrain_test_x), torch.LongTensor(w2v_pretrain_test_y))\n",
    "\n",
    "# Data Loader\n",
    "train_batch_size=16\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_batch_size=16\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, (target-1))\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, (target-1))\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == (target-1)).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bea9c9",
   "metadata": {},
   "source": [
    "## -- Test Dataset Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cac3daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # prep model for evaluation\n",
    "main_tar = []\n",
    "predss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, (target-1))\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        for i in np.array(target-1):\n",
    "            main_tar.append(i)\n",
    "        for j in np.array(ypred):\n",
    "            predss.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1d36355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Value\n",
      "0.6160073443498582\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Value\")\n",
    "report = classification_report(main_tar, predss, digits=6, output_dict=True)\n",
    "print(report['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12cf86",
   "metadata": {},
   "source": [
    "## Task 4(b) 10 word vectors concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "538558ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.941390 \tValidation Loss: 0.911243 \tEpoch Accuracy: 0.561833\n",
      "Epoch: 2 \tTraining Loss: 0.857803 \tValidation Loss: 0.889943 \tEpoch Accuracy: 0.579833\n",
      "Epoch: 3 \tTraining Loss: 0.801910 \tValidation Loss: 0.908848 \tEpoch Accuracy: 0.575667\n",
      "Epoch: 4 \tTraining Loss: 0.720994 \tValidation Loss: 0.957854 \tEpoch Accuracy: 0.569333\n",
      "Epoch: 5 \tTraining Loss: 0.607623 \tValidation Loss: 1.062071 \tEpoch Accuracy: 0.560583\n",
      "Epoch: 6 \tTraining Loss: 0.479041 \tValidation Loss: 1.256614 \tEpoch Accuracy: 0.548333\n",
      "Epoch: 7 \tTraining Loss: 0.363243 \tValidation Loss: 1.564067 \tEpoch Accuracy: 0.535583\n",
      "Epoch: 8 \tTraining Loss: 0.275674 \tValidation Loss: 1.829120 \tEpoch Accuracy: 0.534000\n",
      "Epoch: 9 \tTraining Loss: 0.215885 \tValidation Loss: 2.109466 \tEpoch Accuracy: 0.533000\n",
      "Epoch: 10 \tTraining Loss: 0.182354 \tValidation Loss: 2.507781 \tEpoch Accuracy: 0.529833\n",
      "Epoch: 11 \tTraining Loss: 0.159975 \tValidation Loss: 2.592043 \tEpoch Accuracy: 0.527500\n",
      "Epoch: 12 \tTraining Loss: 0.141952 \tValidation Loss: 2.872877 \tEpoch Accuracy: 0.524500\n",
      "Epoch: 13 \tTraining Loss: 0.132474 \tValidation Loss: 3.010039 \tEpoch Accuracy: 0.524583\n",
      "Epoch: 14 \tTraining Loss: 0.118733 \tValidation Loss: 3.029299 \tEpoch Accuracy: 0.525500\n",
      "Epoch: 15 \tTraining Loss: 0.107735 \tValidation Loss: 3.221778 \tEpoch Accuracy: 0.523667\n",
      "Epoch: 16 \tTraining Loss: 0.103279 \tValidation Loss: 3.461644 \tEpoch Accuracy: 0.525583\n",
      "Epoch: 17 \tTraining Loss: 0.098106 \tValidation Loss: 3.558471 \tEpoch Accuracy: 0.524333\n",
      "Epoch: 18 \tTraining Loss: 0.095432 \tValidation Loss: 3.643724 \tEpoch Accuracy: 0.524167\n",
      "Epoch: 19 \tTraining Loss: 0.097249 \tValidation Loss: 3.791314 \tEpoch Accuracy: 0.528083\n",
      "Epoch: 20 \tTraining Loss: 0.088351 \tValidation Loss: 3.740162 \tEpoch Accuracy: 0.516167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data=TensorDataset(torch.FloatTensor(w2v_pretrain_train_concat_x), torch.LongTensor(w2v_pretrain_train_concat_y))\n",
    "\n",
    "test_data=TensorDataset(torch.FloatTensor(w2v_pretrain_test_concat_x), torch.LongTensor(w2v_pretrain_test_concat_y))\n",
    "\n",
    "# Data Loader\n",
    "train_batch_size=16\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_batch_size=16\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "optimizer = torch.optim.Adam(model_concat.parameters(), lr=0.002)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model_concat.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model_concat.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16d917b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_concat.eval() # prep model for evaluation\n",
    "main_tar = []\n",
    "predss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        ypred = output.argmax(dim = 1)\n",
    "        for i in np.array(target-1):\n",
    "            main_tar.append(i)\n",
    "        for j in np.array(ypred):\n",
    "            predss.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ca2294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Value: After contenating first 10 review vectors\n",
      "0.5161666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Value: After contenating first 10 review vectors\")\n",
    "concate_report = classification_report(main_tar, predss, digits=6, output_dict=True)\n",
    "print(concate_report['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab4065f",
   "metadata": {},
   "source": [
    "# Question Task 4:\n",
    "What do you conclude by comparing accuracy values you obtain with those obtained in the “Simple Models” section?\n",
    "\n",
    "# Answer:\n",
    "Based on the outcomes of the basic models, the accuracy values obtained were as follows:\n",
    "\n",
    "- For W2V: 0.5805 (Perceptron) and 0.6276 (SVM)\n",
    "- For TF-IDF: 0.6170 (Perceptron) and 0.6685 (SVM)\n",
    "- For FNN model a): 0.6160\n",
    "- For FNN model b): 0.5161\n",
    "- In my opinion, the utilization of W2V features showed improvement in FNN model a), whereas in FNN model b), the concatenated W2V features of the first 10 words did not seem to have a strong connection with the labels. This is because not all reviews express their sentiment in the first 10 words, and some reviews have less than 10 words, which were concatenated with zero value vectors. Consequently, the performance of model b) was not as good as that of a). Furthermore, Neural Network models are less sensitive to hyperparameters, and the preparation of training data is more straightforward and systematic. With the advancement of computing power, FNN models have become far more effective than traditional SVM and Perceptron models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe698dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clearing some memory\n",
    "del globals()['tfidf_X_train'], globals()['tfidf_X_test'], globals()['tfidf_Y_train'], globals()['tfidf_Y_test']\n",
    "\n",
    "del globals()['w2v_pretrain_train_x'], globals()['w2v_pretrain_train_y']\n",
    "del globals()['w2v_pretrain_test_x'], globals()['w2v_pretrain_test_y']\n",
    "\n",
    "del globals()['w2v_pretrain_train_concat_x'], globals()['w2v_pretrain_train_concat_y']\n",
    "del globals()['w2v_pretrain_test_concat_x'], globals()['w2v_pretrain_test_concat_y']\n",
    "\n",
    "del globals()['model'], globals()['model_concat'], globals()['train_data'], globals()['test_data']\n",
    "del globals()['Y_pred_w2v_test'], globals()['Y_pred_tfidf_test'], globals()['Y_pred_w2v_svm_test'], globals()['Y_pred_tfidf_svm_test']\n",
    "\n",
    "del globals()['train_loader'], globals()['test_loader']\n",
    "del globals()['main_tar'], globals()['predss']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a9aa9",
   "metadata": {},
   "source": [
    "# Task 5 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8373e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "limiting the maximum review length to 20 by truncating longer reviews and padding\n",
    "shorter reviews with a null value (0)\n",
    "'''\n",
    "# Average word2Vec vectors\n",
    "def average_vectors_rnn(review):\n",
    "    temp_review = review.split(' ')\n",
    "\n",
    "    words = np.array([word for word in temp_review[:20] if word in pretrained_w2v])\n",
    "        \n",
    "    review_vector = []\n",
    "    for word in words:\n",
    "        review_vector.append(pretrained_w2v[word])\n",
    "    review_vector = np.array(review_vector)\n",
    "    \n",
    "    # can be the case where the words in the review are not found in the W2V vocabulary\n",
    "    if len(review_vector)==0:\n",
    "        review_vector = np.zeros((20, 300))\n",
    "    \n",
    "    # In the case where the total dim of the feature vector is <20 add the padding with zeros\n",
    "    elif len(review_vector)<20:\n",
    "        review_vector = np.concatenate([review_vector, np.zeros((20-len(review_vector), 300))])\n",
    "        \n",
    "    return review_vector\n",
    "    \n",
    "def featurization_rnn(dataset):\n",
    "    features = []\n",
    "\n",
    "    for review in dataset['review_body']:\n",
    "        x = average_vectors_rnn(review)\n",
    "        features.append(x)\n",
    "        \n",
    "    return features\n",
    "\n",
    "'''\n",
    "Review Vectors for  first 20 words each\n",
    "'''\n",
    "w2v_pretrain_train_x = featurization_rnn(train)\n",
    "w2v_pretrain_train_y = train['label']\n",
    "w2v_pretrain_test_x = featurization_rnn(test)\n",
    "w2v_pretrain_test_y = test['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c3778",
   "metadata": {},
   "source": [
    "\n",
    "# - Task 5(a): Train a simple RNN for sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d87ba165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_model(\n",
      "  (rnn_layer): RNN(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class rnn_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rnn_model, self).__init__()\n",
    "        \n",
    "        self.rnn_layer = nn.RNN(300, 20, batch_first = True)        \n",
    "        self.fc = nn.Linear(20,3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.view(-1,20,300)\n",
    "        output, hidden = self.rnn_layer(output)\n",
    "        output=self.fc(output[:,-1,:])\n",
    "        return output\n",
    "\n",
    "model = rnn_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce461403",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=TensorDataset(torch.FloatTensor(w2v_pretrain_train_x), torch.LongTensor(w2v_pretrain_train_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(w2v_pretrain_test_x), torch.LongTensor(w2v_pretrain_test_y))\n",
    "\n",
    "# Data Loader\n",
    "train_batch_size=200\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_batch_size=200\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "007b9941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.068041 \tValidation Loss: 0.972443 \tEpoch Accuracy: 0.515667\n",
      "Epoch: 2 \tTraining Loss: 0.946077 \tValidation Loss: 0.931448 \tEpoch Accuracy: 0.541500\n",
      "Epoch: 3 \tTraining Loss: 0.907505 \tValidation Loss: 0.898142 \tEpoch Accuracy: 0.570083\n",
      "Epoch: 4 \tTraining Loss: 0.886334 \tValidation Loss: 0.894273 \tEpoch Accuracy: 0.573083\n",
      "Epoch: 5 \tTraining Loss: 0.873569 \tValidation Loss: 0.891884 \tEpoch Accuracy: 0.570833\n",
      "Epoch: 6 \tTraining Loss: 0.868208 \tValidation Loss: 0.889946 \tEpoch Accuracy: 0.574583\n",
      "Epoch: 7 \tTraining Loss: 0.864372 \tValidation Loss: 0.876857 \tEpoch Accuracy: 0.588250\n",
      "Epoch: 8 \tTraining Loss: 0.857649 \tValidation Loss: 0.876751 \tEpoch Accuracy: 0.583750\n",
      "Epoch: 9 \tTraining Loss: 0.851378 \tValidation Loss: 0.871992 \tEpoch Accuracy: 0.591083\n",
      "Epoch: 10 \tTraining Loss: 0.849071 \tValidation Loss: 0.881663 \tEpoch Accuracy: 0.582083\n",
      "Epoch: 11 \tTraining Loss: 0.844632 \tValidation Loss: 0.865923 \tEpoch Accuracy: 0.598500\n",
      "Epoch: 12 \tTraining Loss: 0.841565 \tValidation Loss: 0.861431 \tEpoch Accuracy: 0.599083\n",
      "Epoch: 13 \tTraining Loss: 0.837749 \tValidation Loss: 0.866031 \tEpoch Accuracy: 0.591917\n",
      "Epoch: 14 \tTraining Loss: 0.836802 \tValidation Loss: 0.857653 \tEpoch Accuracy: 0.607250\n",
      "Epoch: 15 \tTraining Loss: 0.831219 \tValidation Loss: 0.857527 \tEpoch Accuracy: 0.606167\n",
      "Epoch: 16 \tTraining Loss: 0.828503 \tValidation Loss: 0.852041 \tEpoch Accuracy: 0.608917\n",
      "Epoch: 17 \tTraining Loss: 0.823943 \tValidation Loss: 0.854108 \tEpoch Accuracy: 0.617333\n",
      "Epoch: 18 \tTraining Loss: 0.827398 \tValidation Loss: 0.853338 \tEpoch Accuracy: 0.607333\n",
      "Epoch: 19 \tTraining Loss: 0.817418 \tValidation Loss: 0.847530 \tEpoch Accuracy: 0.614417\n",
      "Epoch: 20 \tTraining Loss: 0.814677 \tValidation Loss: 0.859046 \tEpoch Accuracy: 0.603333\n"
     ]
    }
   ],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))\n",
    "    \n",
    "model.eval() # prep model for evaluation\n",
    "main_tar = []\n",
    "predss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        for i in np.array(target-1):\n",
    "            main_tar.append(i)\n",
    "        for j in np.array(ypred):\n",
    "            predss.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7abfdf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Value: RNN\n",
      "0.6033333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Value: RNN\")\n",
    "rnn_report = classification_report(main_tar, predss, digits=6, output_dict=True)\n",
    "print(rnn_report['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41938b71",
   "metadata": {},
   "source": [
    "# Task 5(b): Considering a gated recurrent unit cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b5ce182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru_model(\n",
      "  (gru_layer): GRU(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class gru_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(gru_model, self).__init__()\n",
    "        self.gru_layer = nn.GRU(300, 20, batch_first = True)\n",
    "        self.fc = nn.Linear(20,3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.view(-1,20,300)\n",
    "        output, hidden = self.gru_layer(output)\n",
    "        output=self.fc(output[:,-1,:])\n",
    "        return output\n",
    "\n",
    "model_gru = gru_model()\n",
    "print(model_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1280ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.008304 \tValidation Loss: 0.906597 \tEpoch Accuracy: 0.556667\n",
      "Epoch: 2 \tTraining Loss: 0.871335 \tValidation Loss: 0.866509 \tEpoch Accuracy: 0.584833\n",
      "Epoch: 3 \tTraining Loss: 0.833902 \tValidation Loss: 0.827483 \tEpoch Accuracy: 0.618750\n",
      "Epoch: 4 \tTraining Loss: 0.806703 \tValidation Loss: 0.802922 \tEpoch Accuracy: 0.629500\n",
      "Epoch: 5 \tTraining Loss: 0.785005 \tValidation Loss: 0.804980 \tEpoch Accuracy: 0.636667\n",
      "Epoch: 6 \tTraining Loss: 0.769362 \tValidation Loss: 0.777338 \tEpoch Accuracy: 0.649000\n",
      "Epoch: 7 \tTraining Loss: 0.758515 \tValidation Loss: 0.770910 \tEpoch Accuracy: 0.656667\n",
      "Epoch: 8 \tTraining Loss: 0.749954 \tValidation Loss: 0.770134 \tEpoch Accuracy: 0.654833\n",
      "Epoch: 9 \tTraining Loss: 0.741368 \tValidation Loss: 0.766501 \tEpoch Accuracy: 0.656750\n",
      "Epoch: 10 \tTraining Loss: 0.734971 \tValidation Loss: 0.764203 \tEpoch Accuracy: 0.658583\n",
      "Epoch: 11 \tTraining Loss: 0.727820 \tValidation Loss: 0.765951 \tEpoch Accuracy: 0.652750\n",
      "Epoch: 12 \tTraining Loss: 0.722957 \tValidation Loss: 0.768525 \tEpoch Accuracy: 0.652750\n",
      "Epoch: 13 \tTraining Loss: 0.718677 \tValidation Loss: 0.761530 \tEpoch Accuracy: 0.656333\n",
      "Epoch: 14 \tTraining Loss: 0.713302 \tValidation Loss: 0.769965 \tEpoch Accuracy: 0.652000\n",
      "Epoch: 15 \tTraining Loss: 0.708194 \tValidation Loss: 0.764820 \tEpoch Accuracy: 0.657500\n",
      "Epoch: 16 \tTraining Loss: 0.706288 \tValidation Loss: 0.765403 \tEpoch Accuracy: 0.660750\n",
      "Epoch: 17 \tTraining Loss: 0.700849 \tValidation Loss: 0.765382 \tEpoch Accuracy: 0.655500\n",
      "Epoch: 18 \tTraining Loss: 0.695064 \tValidation Loss: 0.766828 \tEpoch Accuracy: 0.659417\n",
      "Epoch: 19 \tTraining Loss: 0.691395 \tValidation Loss: 0.765902 \tEpoch Accuracy: 0.658417\n",
      "Epoch: 20 \tTraining Loss: 0.688022 \tValidation Loss: 0.769943 \tEpoch Accuracy: 0.657083\n"
     ]
    }
   ],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "optimizer = torch.optim.Adam(model_gru.parameters(), lr=0.001)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model_gru.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_gru(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model_gru.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_gru(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))\n",
    "    \n",
    "model_gru.eval() # prep model for evaluation\n",
    "main_tar = []\n",
    "predss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_gru(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        for i in np.array(target-1):\n",
    "            main_tar.append(i)\n",
    "        for j in np.array(ypred):\n",
    "            predss.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d1a67f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Value: GRU\n",
      "0.6570833333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Value: GRU\")\n",
    "gru_report = classification_report(main_tar, predss, digits=6, output_dict=True)\n",
    "print(gru_report['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae604ef9",
   "metadata": {},
   "source": [
    "# - Task 5(c): Considering a  LSTM unit cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f788042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_model(\n",
      "  (lstm_layer): LSTM(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class lstm_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lstm_model, self).__init__()\n",
    "        self.lstm_layer = nn.LSTM(300, 20, batch_first = True)\n",
    "        self.fc = nn.Linear(20,3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.view(-1,20,300)\n",
    "        output, hidden = self.lstm_layer(output)\n",
    "        output=self.fc(output[:,-1,:])\n",
    "        return output\n",
    "\n",
    "model_lstm = lstm_model()\n",
    "print(model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "826d0f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.018164 \tValidation Loss: 0.918034 \tEpoch Accuracy: 0.549583\n",
      "Epoch: 2 \tTraining Loss: 0.875050 \tValidation Loss: 0.864892 \tEpoch Accuracy: 0.594083\n",
      "Epoch: 3 \tTraining Loss: 0.839194 \tValidation Loss: 0.835017 \tEpoch Accuracy: 0.619000\n",
      "Epoch: 4 \tTraining Loss: 0.814308 \tValidation Loss: 0.813177 \tEpoch Accuracy: 0.631583\n",
      "Epoch: 5 \tTraining Loss: 0.793888 \tValidation Loss: 0.799908 \tEpoch Accuracy: 0.642583\n",
      "Epoch: 6 \tTraining Loss: 0.779068 \tValidation Loss: 0.807967 \tEpoch Accuracy: 0.637917\n",
      "Epoch: 7 \tTraining Loss: 0.767645 \tValidation Loss: 0.794317 \tEpoch Accuracy: 0.641583\n",
      "Epoch: 8 \tTraining Loss: 0.754578 \tValidation Loss: 0.789441 \tEpoch Accuracy: 0.649917\n",
      "Epoch: 9 \tTraining Loss: 0.746755 \tValidation Loss: 0.777910 \tEpoch Accuracy: 0.657167\n",
      "Epoch: 10 \tTraining Loss: 0.737703 \tValidation Loss: 0.784912 \tEpoch Accuracy: 0.654667\n",
      "Epoch: 11 \tTraining Loss: 0.729838 \tValidation Loss: 0.772910 \tEpoch Accuracy: 0.655083\n",
      "Epoch: 12 \tTraining Loss: 0.723032 \tValidation Loss: 0.772639 \tEpoch Accuracy: 0.653833\n",
      "Epoch: 13 \tTraining Loss: 0.715835 \tValidation Loss: 0.771529 \tEpoch Accuracy: 0.654500\n",
      "Epoch: 14 \tTraining Loss: 0.709885 \tValidation Loss: 0.772301 \tEpoch Accuracy: 0.655917\n",
      "Epoch: 15 \tTraining Loss: 0.702764 \tValidation Loss: 0.770811 \tEpoch Accuracy: 0.658583\n",
      "Epoch: 16 \tTraining Loss: 0.697297 \tValidation Loss: 0.773639 \tEpoch Accuracy: 0.658083\n",
      "Epoch: 17 \tTraining Loss: 0.695691 \tValidation Loss: 0.778288 \tEpoch Accuracy: 0.658667\n",
      "Epoch: 18 \tTraining Loss: 0.686277 \tValidation Loss: 0.767325 \tEpoch Accuracy: 0.656583\n",
      "Epoch: 19 \tTraining Loss: 0.683824 \tValidation Loss: 0.770309 \tEpoch Accuracy: 0.657667\n",
      "Epoch: 20 \tTraining Loss: 0.678004 \tValidation Loss: 0.769233 \tEpoch Accuracy: 0.657833\n"
     ]
    }
   ],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model_lstm.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_lstm(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model_lstm.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_lstm(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))\n",
    "    \n",
    "model_lstm.eval() # prep model for evaluation\n",
    "main_tar = []\n",
    "predss = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_lstm(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        for i in np.array(target-1):\n",
    "            main_tar.append(i)\n",
    "        for j in np.array(ypred):\n",
    "            predss.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9577225a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Value: LSTM\n",
      "0.6578333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Value: LSTM\")\n",
    "lstm_report = classification_report(main_tar, predss, digits=6, output_dict=True)\n",
    "print(lstm_report['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3ae7f",
   "metadata": {},
   "source": [
    "# Question Task 5:\n",
    "What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN?\n",
    "\n",
    "# Answer:\n",
    "For Simple RNN, GRU and LSTM we got the respective accuracies of 0.6033, 0.6570 and 0.6578. To summarize, the GRU has shown a slight improvement in accuracy compared to traditional RNNs and LSTM also performs slightly better than GRU. While RNNs may encounter issues with vanishing or exploding gradients, leading to decreased accuracy, Gated RNNs, such as the GRU, have mechanisms that allow them to learn long-term dependencies and regulate the amount of information they pass on. The use of the tanh function in GRUs further helps to address the problem of vanishing and exploding gradients. One reason for why LSTM outperforms GRU is that LSTMs have more gating mechanisms than GRUs. Specifically, LSTMs have three gating mechanisms: input, forget, and output gates. This extra gate, the forget gate, allows the LSTM to selectively forget information from previous time steps, which can be useful for tasks where some of the historical information is no longer relevant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
