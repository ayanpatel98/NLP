{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d65dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim.downloader as api\n",
    "import gensim.models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import gc\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3eb19",
   "metadata": {},
   "source": [
    "# Task 1: Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "084dd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained Word2Vec model:\n",
    "pretrained_w2v = api.load('word2vec-google-news-300')\n",
    "\n",
    "print('Check semantic similarities of the generated vectors:')\n",
    "print(pretrained_w2v.most_similar(positive=['king', 'woman'], negative=['man'], topn = 1)[0])\n",
    "print('Excellent ~ Outstanding:', pretrained_w2v.similarity('excellent', 'outstanding'))\n",
    "print('time ~ schedule:', pretrained_w2v.similarity('time', 'schedule'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5176f682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n",
    "df = df[['star_rating', 'review_body']]\n",
    "class_one = df[(df['star_rating']==1) | (df['star_rating']==2)]\n",
    "class_two = df[df['star_rating']==3]\n",
    "class_three = df[(df['star_rating']==4) | (df['star_rating']==5)]\n",
    "\n",
    "# del df\n",
    "# gc.collect()\n",
    "# df = [1]\n",
    "\n",
    "class_one.loc[:, \"label\"] =1\n",
    "class_two.loc[:, \"label\"] =2\n",
    "class_three.loc[:, \"label\"] =3\n",
    "\n",
    "class_one = class_one.sample(n=20000, random_state=100)\n",
    "class_two = class_two.sample(n=20000, random_state=100)\n",
    "class_three = class_three.sample(n=20000, random_state=100)\n",
    "dataset = pd.concat([class_one, class_two, class_three])\n",
    "# del class_one, class_two, class_three\n",
    "# gc.collect()\n",
    "# class_one = [1]\n",
    "# class_two = [1]\n",
    "# class_three = [1]\n",
    "\n",
    "dataset.reset_index(drop=True)\n",
    "train = dataset.sample(frac=0.8, random_state=100)\n",
    "test = dataset.drop(train.index)\n",
    "# del dataset\n",
    "# gc.collect()\n",
    "# dataset = [1]\n",
    "\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)\n",
    "\n",
    "\n",
    "del globals()['class_one'], globals()['class_two'], globals()['class_three'], globals()['dataset'], globals()['df']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d5b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Covert all reviews to lower case\n",
    "train['review_body'] = train['review_body'].str.lower()\n",
    "test['review_body'] = test['review_body'].str.lower()\n",
    "\n",
    "'''\n",
    "URL Remover code\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "test['review_body'] = test['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "def html_tag_remover(review):\n",
    "    soup = BeautifulSoup(review, 'html.parser')\n",
    "    review = soup.get_text()\n",
    "    return review  \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: html_tag_remover(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: html_tag_remover(review))\n",
    "\n",
    "'''\n",
    "remove non-alphabetical characters\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n",
    "\n",
    "'''\n",
    "remove extra spaces\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub(' +', ' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub(' +', ' ', review))\n",
    "\n",
    "'''\n",
    "perform contractions on the reviews\n",
    "'''\n",
    "def expand_contractions(review):\n",
    "    review = contractions.fix(review)\n",
    "    return review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: expand_contractions(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: expand_contractions(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7713746",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "remove the stop words AND perform lemmatization\n",
    "\n",
    "'''\n",
    "avg_len_before_prepro = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    stop_words_english = set(stopwords.words('english'))\n",
    "    review_word_tokens = word_tokenize(review)\n",
    "    filtered_review = [word for word in review_word_tokens if not word in stop_words_english]\n",
    "    return filtered_review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: remove_stopwords(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: remove_stopwords(review))\n",
    "\n",
    "def review_lemmatize(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_review = [lemmatizer.lemmatize(word) for word in review]\n",
    "    return ' '.join(lemmatized_review)    \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "\n",
    "avg_len_after_prepro = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0836a8",
   "metadata": {},
   "source": [
    "# Task 2: Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38dd8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyCorpus:\n",
    "#     def __init__(self):\n",
    "#         self.sentences = train['review_body']\n",
    "#     def __iter__(self):\n",
    "#         for sentence in self.sentences:\n",
    "#             yield sentence\n",
    "\n",
    "# all_Sentences = MyCorpus()\n",
    "all_Sentences = [sentence.split(' ') for sentence in train['review_body'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4950ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e685c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Word2Vec\n",
    "custom_model = gensim.models.Word2Vec(all_Sentences, vector_size = 300, min_count=9, window=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31afd952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check semantic similarities of the generated vectors:\n",
      "('touted', 0.7282370924949646)\n",
      "Excellent ~ Outstanding: 0.73700804\n",
      "time ~ schedule: 0.1772071\n"
     ]
    }
   ],
   "source": [
    "print('Check semantic similarities of the generated vectors:')\n",
    "print(custom_model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn = 1)[0])\n",
    "print('Excellent ~ Outstanding:', custom_model.wv.similarity('excellent', 'outstanding'))\n",
    "print('time ~ schedule:', custom_model.wv.similarity('time', 'schedule'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e8889e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_Sentences, custom_model\n",
    "gc.collect()\n",
    "all_Sentences = [1]\n",
    "custom_model = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67a25c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contractions',\n",
       " 'tensorflow',\n",
       " 'avg_len_before_prepro',\n",
       " 'classification_report',\n",
       " 'avg_len_after_prepro',\n",
       " 'expand_contractions',\n",
       " 'WordNetLemmatizer',\n",
       " 'torch',\n",
       " 'confusion_matrix',\n",
       " 'html_tag_remover',\n",
       " 'remove_stopwords',\n",
       " 'review_lemmatize',\n",
       " 'TfidfVectorizer',\n",
       " 'accuracy_score',\n",
       " 'nltk',\n",
       " 'pretrained_w2v',\n",
       " 'BeautifulSoup',\n",
       " 'TensorDataset',\n",
       " 'all_Sentences',\n",
       " 'word_tokenize',\n",
       " '__builtins__',\n",
       " 'custom_model',\n",
       " '__builtin__',\n",
       " '__package__',\n",
       " 'get_ipython',\n",
       " 'DataLoader',\n",
       " 'Perceptron',\n",
       " '__loader__',\n",
       " 'transforms',\n",
       " 'LinearSVC',\n",
       " 'getsizeof',\n",
       " 'stopwords',\n",
       " '__name__',\n",
       " '__spec__',\n",
       " 'warnings',\n",
       " 'Dataset',\n",
       " '__doc__',\n",
       " 'gensim',\n",
       " 'optim',\n",
       " 'train',\n",
       " 'utils',\n",
       " '_i10',\n",
       " '_iii',\n",
       " 'exit',\n",
       " 'quit',\n",
       " 'test',\n",
       " 'Out',\n",
       " 'SVC',\n",
       " '___',\n",
       " '_dh',\n",
       " '_i1',\n",
       " '_i2',\n",
       " '_i3',\n",
       " '_i4',\n",
       " '_i5',\n",
       " '_i6',\n",
       " '_i7',\n",
       " '_i8',\n",
       " '_i9',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_oh',\n",
       " 'api',\n",
       " 'In',\n",
       " '_3',\n",
       " '__',\n",
       " '_i',\n",
       " 'gc',\n",
       " 'nn',\n",
       " 'np',\n",
       " 'pd',\n",
       " 're',\n",
       " 'F',\n",
       " '_']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dir()\n",
    "a = sorted(a, key = lambda x: -getsizeof(x))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f51a46e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_three' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m getsizeof(\u001b[43mclass_three\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_three' is not defined"
     ]
    }
   ],
   "source": [
    "# getsizeof(class_three)\n",
    "# del globals()['df']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821ed51",
   "metadata": {},
   "source": [
    "# Task 3: Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fc2a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word2Vec vectors\n",
    "# all_sentence_vector = pretrained_w2v\n",
    "# del pretrained_w2v\n",
    "# gc.collect()\n",
    "# pretrained_w2v = [1]\n",
    "def average_vectors(review, label):\n",
    "    temp_review = review.split(' ')\n",
    "    words = [word for word in temp_review if word in pretrained_w2v]\n",
    "    if len(words) >=1:\n",
    "        review_vector = []\n",
    "        for word in words:\n",
    "            review_vector.append(pretrained_w2v[word])\n",
    "        return review_vector, label\n",
    "\n",
    "def average_vectors_concat(review, label):\n",
    "    temp_review = review.split(' ')\n",
    "    words = [word for word in temp_review[:10] if word in pretrained_w2v]\n",
    "        \n",
    "    review_vector = []\n",
    "    for word in words:\n",
    "        review_vector.append(pretrained_w2v[word])\n",
    "    review_vector = np.array(review_vector)\n",
    "    \n",
    "    # can be the case where the words in the review are not found in the W2V vocabulary\n",
    "    if len(words)==0:\n",
    "        review_vector = np.zeros((1, 300))\n",
    "    data_vector = np.concatenate(review_vector, axis=0)\n",
    "    \n",
    "    # In the case where the total dim of the feature vector is <3000 add the padding with zeros\n",
    "    if len(data_vector)<3000:\n",
    "        data_vector = np.concatenate([data_vector, np.zeros(3000-len(data))])\n",
    "    return data_vector/10, label\n",
    "    \n",
    "    \n",
    "def featurization(dataset, concat = False):\n",
    "    features = []\n",
    "    y_labels = []\n",
    "    concat = concat\n",
    "    \n",
    "    for review, label in zip(dataset['review_body'], dataset['label']):\n",
    "        try:\n",
    "            if not concat:\n",
    "                x, y = average_vectors(review, label)\n",
    "                features.append(np.mean(x, axis=0))\n",
    "            else:\n",
    "                x, y = average_vectors_concat(review, label)\n",
    "                features.append(x)\n",
    "                \n",
    "            y_labels.append(y)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    return features, y_labels\n",
    "\n",
    "# Vectors without concatenation\n",
    "w2v_pretrain_train_x, w2v_pretrain_train_y = featurization(train)\n",
    "w2v_pretrain_test_x, w2v_pretrain_test_y = featurization(test)\n",
    "\n",
    "# Vectors with concatenation\n",
    "w2v_pretrain_train_concat_x, w2v_pretrain_train_concat_y = featurization(train, True)\n",
    "w2v_pretrain_test_concat_x, w2v_pretrain_test_concat_y = featurization(test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2470964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily Delete google vectors to free memory\n",
    "del globals()['pretrained_w2v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5239dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Feature Extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 0.001)\n",
    "tfidf_X_train = tfidf_vectorizer.fit_transform(list(train['review_body']))\n",
    "tfidf_X_train = pd.DataFrame(tfidf_X_train.toarray())\n",
    "\n",
    "tfidf_X_test = tfidf_vectorizer.transform(list(test['review_body']))\n",
    "tfidf_X_test = pd.DataFrame(tfidf_X_test.toarray())\n",
    "\n",
    "tfidf_Y_train = train['label']\n",
    "tfidf_Y_test = test['label']\n",
    "\n",
    "tfidf_Y_train = tfidf_Y_train.astype('int')\n",
    "tfidf_Y_test = tfidf_Y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bebea69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Perceptron Model on Average Word2Vec Features\n",
    "perceptr_w2v = Perceptron(random_state = 100, eta0=0.1)\n",
    "perceptr_w2v.fit(w2v_pretrain_train_x, w2v_pretrain_train_y)\n",
    "Y_pred_w2v_test = perceptr_w2v.predict(w2v_pretrain_test_x)\n",
    "\n",
    "# Training Perceptron Model on TF-IDF Features\n",
    "perceptr_tfidf = Perceptron(random_state = 100, eta0=0.1)\n",
    "perceptr_tfidf.fit(tfidf_X_train, tfidf_Y_train)\n",
    "Y_pred_tfidf_test = perceptr_tfidf.predict(tfidf_X_test)\n",
    "\n",
    "target_names = ['class 1', 'class 2', 'class 3']\n",
    "report_w2v = classification_report(w2v_pretrain_test_y, Y_pred_w2v_test, target_names=target_names, output_dict=True)\n",
    "report_tfidf = classification_report(tfidf_Y_test, Y_pred_tfidf_test, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7515f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy values PERCEPTRON for w2v and tfidf features:\n",
      "0.5805374728759807 0.6170833333333333\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy values PERCEPTRON for w2v and tfidf features:')\n",
    "print(report_w2v['accuracy'], report_tfidf['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a35497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training SVM Model on Average Word2Vec Features\n",
    "svm_w2v = LinearSVC(random_state=100, max_iter=1000)\n",
    "svm_w2v.fit(w2v_pretrain_train_x, w2v_pretrain_train_y)\n",
    "Y_pred_w2v_svm_test = svm_w2v.predict(w2v_pretrain_test_x)\n",
    "\n",
    "# Training SVM Model on TFIDF Features\n",
    "svm_tfidf = LinearSVC(random_state=100, max_iter=1000)\n",
    "svm_tfidf.fit(tfidf_X_train, tfidf_Y_train)\n",
    "Y_pred_tfidf_svm_test = svm_tfidf.predict(tfidf_X_test)\n",
    "\n",
    "report_svm_w2v = classification_report(w2v_pretrain_test_y, Y_pred_w2v_svm_test, target_names=target_names, output_dict=True)\n",
    "report_svm_tfidf = classification_report(tfidf_Y_test, Y_pred_tfidf_svm_test, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66f59b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy values SVM for w2v and tfidf features:\n",
      "0.627691537305959 0.6685\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy values SVM for w2v and tfidf features:')\n",
    "print(report_svm_w2v['accuracy'], report_svm_tfidf['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c042fdd",
   "metadata": {},
   "source": [
    "# Task 4: Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e99310ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "MLP_concat(\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (fc1): Linear(in_features=3000, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 300):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)  \n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "#         self.sig = nn.Sigmoid()\n",
    "#         self.soft = nn.Softmax(dim = 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class MLP_concat(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 3000):\n",
    "        super(MLP_concat, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)  \n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "#         self.sig = nn.Sigmoid()\n",
    "#         self.soft = nn.Softmax(dim = 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "model_concat = MLP_concat()\n",
    "model = model\n",
    "model_concat = model_concat\n",
    "print(model)\n",
    "print(model_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17d6f7",
   "metadata": {},
   "source": [
    "-- Task 4(a) using the average Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95bf3f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.100207 \tValidation Loss: 1.099523 \tEpoch Accuracy: 0.332081\n",
      "Epoch: 2 \tTraining Loss: 1.098991 \tValidation Loss: 1.098897 \tEpoch Accuracy: 0.332081\n",
      "Epoch: 3 \tTraining Loss: 1.098649 \tValidation Loss: 1.098647 \tEpoch Accuracy: 0.331998\n",
      "Epoch: 4 \tTraining Loss: 1.098482 \tValidation Loss: 1.098487 \tEpoch Accuracy: 0.336839\n",
      "Epoch: 5 \tTraining Loss: 1.098361 \tValidation Loss: 1.098363 \tEpoch Accuracy: 0.338424\n",
      "Epoch: 6 \tTraining Loss: 1.098236 \tValidation Loss: 1.098236 \tEpoch Accuracy: 0.346353\n",
      "Epoch: 7 \tTraining Loss: 1.098072 \tValidation Loss: 1.098085 \tEpoch Accuracy: 0.360290\n",
      "Epoch: 8 \tTraining Loss: 1.097884 \tValidation Loss: 1.097904 \tEpoch Accuracy: 0.366299\n",
      "Epoch: 9 \tTraining Loss: 1.097710 \tValidation Loss: 1.097757 \tEpoch Accuracy: 0.371390\n",
      "Epoch: 10 \tTraining Loss: 1.097532 \tValidation Loss: 1.097550 \tEpoch Accuracy: 0.378484\n"
     ]
    }
   ],
   "source": [
    "train_data=TensorDataset(torch.FloatTensor(w2v_pretrain_train_x), torch.LongTensor(w2v_pretrain_train_y))\n",
    "test_data=TensorDataset(torch.FloatTensor(w2v_pretrain_test_x), torch.LongTensor(w2v_pretrain_test_y))\n",
    "\n",
    "# Data Loader\n",
    "train_batch_size=256\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_batch_size=256\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, (target-1))\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, (target-1))\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == (target-1)).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bea9c9",
   "metadata": {},
   "source": [
    "-- Test Dataset Accuracy classwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cac3daa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Values for each class\n",
      "Class 1 :  0.040636\n",
      "Class 2 :  0.340287\n",
      "Class 3 :  0.769525\n"
     ]
    }
   ],
   "source": [
    "test_batch_size= len(w2v_pretrain_test_x)\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "model.eval() # prep model for evaluation\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, (target-1))\n",
    "        # update running validation loss \n",
    "        #valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        #correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "        mat = confusion_matrix((target-1),ypred)\n",
    "        ans = mat.diagonal()/mat.sum(axis=1)\n",
    "        \n",
    "print(\"Accuracy Values for each class\")\n",
    "\n",
    "for i,acc in enumerate(ans):\n",
    "    print(f\"Class {i+1} : {acc: .6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12cf86",
   "metadata": {},
   "source": [
    "Task 4(b) 10 word vectors concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce0fb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del globals()['w2v_pretrain_test_x']\n",
    "# del w2v_pretrain_train_x, w2v_pretrain_train_y, w2v_pretrain_test_x, w2v_pretrain_test_y\n",
    "\n",
    "# gc.collect()\n",
    "# # del df\n",
    "# df = [1]\n",
    "# model = [1]\n",
    "# model_concat = [1]\n",
    "# train_data = [1]\n",
    "# test_data = [1]\n",
    "# w2v_pretrain_train_x = [1]\n",
    "# w2v_pretrain_train_y = [1]\n",
    "# w2v_pretrain_test_x = [1]\n",
    "# w2v_pretrain_test_y = [1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "538558ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.113230 \tValidation Loss: 1.107735 \tEpoch Accuracy: 0.340754\n",
      "Epoch: 2 \tTraining Loss: 1.103868 \tValidation Loss: 1.101986 \tEpoch Accuracy: 0.340754\n",
      "Epoch: 3 \tTraining Loss: 1.100367 \tValidation Loss: 1.099764 \tEpoch Accuracy: 0.340754\n",
      "Epoch: 4 \tTraining Loss: 1.099013 \tValidation Loss: 1.098854 \tEpoch Accuracy: 0.340754\n",
      "Epoch: 5 \tTraining Loss: 1.098474 \tValidation Loss: 1.098459 \tEpoch Accuracy: 0.340754\n",
      "Epoch: 6 \tTraining Loss: 1.098250 \tValidation Loss: 1.098279 \tEpoch Accuracy: 0.340754\n",
      "Epoch: 7 \tTraining Loss: 1.098151 \tValidation Loss: 1.098191 \tEpoch Accuracy: 0.340754\n",
      "Epoch: 8 \tTraining Loss: 1.098103 \tValidation Loss: 1.098153 \tEpoch Accuracy: 0.340754\n",
      "Epoch: 9 \tTraining Loss: 1.098084 \tValidation Loss: 1.098125 \tEpoch Accuracy: 0.340754\n",
      "Epoch: 10 \tTraining Loss: 1.098072 \tValidation Loss: 1.098106 \tEpoch Accuracy: 0.340754\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data=TensorDataset(torch.FloatTensor(w2v_pretrain_train_concat_x), torch.LongTensor(w2v_pretrain_train_concat_y))\n",
    "\n",
    "test_data=TensorDataset(torch.FloatTensor(w2v_pretrain_test_concat_x), torch.LongTensor(w2v_pretrain_test_concat_y))\n",
    "\n",
    "# Data Loader\n",
    "train_batch_size=256\n",
    "train_loader=DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_batch_size=256\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model_concat.parameters(), lr=0.01)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model #\n",
    "    model_concat.train() # prep model for training\n",
    "    for data, target in train_loader: # iterates upto number of batch size\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "          \n",
    "    # validate the model #\n",
    "    model_concat.eval() # prep model for evaluation\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tEpoch Accuracy: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss,\n",
    "        correct/len(test_loader.dataset)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16d917b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Values for each class\n",
      "Class 1 :  0.000000\n",
      "Class 2 :  1.000000\n",
      "Class 3 :  0.000000\n"
     ]
    }
   ],
   "source": [
    "test_batch_size= len(w2v_pretrain_test_concat_x)\n",
    "test_loader=DataLoader(test_data, batch_size=test_batch_size, shuffle=True)\n",
    "model_concat.eval() # prep model for evaluation\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_concat(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target-1)\n",
    "        # update running validation loss \n",
    "        #valid_loss += loss.item()*data.size(0)\n",
    "        ypred = output.argmax(dim = 1)\n",
    "        #correct += (ypred == target-1).float().sum()\n",
    "        \n",
    "        mat = confusion_matrix(target-1,ypred)\n",
    "        ans = mat.diagonal()/mat.sum(axis=1)\n",
    "        \n",
    "print(\"Accuracy Values for each class\")\n",
    "\n",
    "for i,acc in enumerate(ans):\n",
    "    print(f\"Class {i+1} : {acc: .6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe698dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del globals()['tfidf_X_train'], globals()['tfidf_X_test'], globals()['tfidf_Y_train'], globals()['tfidf_Y_test']\n",
    "\n",
    "del globals()['w2v_pretrain_train_x'], globals()['w2v_pretrain_train_y']\n",
    "del globals()['w2v_pretrain_test_x'], globals()['w2v_pretrain_test_y']\n",
    "\n",
    "del globals()['w2v_pretrain_train_concat_x'], globals()['w2v_pretrain_train_concat_y']\n",
    "del globals()['w2v_pretrain_test_concat_x'], globals()['w2v_pretrain_test_concat_y']\n",
    "\n",
    "del globals()['model'], globals()['model_concat'], globals()['train_data'], globals()['test_data']\n",
    "del globals()['Y_pred_w2v_test'], globals()['Y_pred_tfidf_test'], globals()['Y_pred_w2v_svm_test'], globals()['Y_pred_tfidf_svm_test']\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a9aa9",
   "metadata": {},
   "source": [
    "# Task 5 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bcf18de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "del globals()['custom_model']\n",
    "# Pretrained Word2Vec model:\n",
    "pretrained_w2v = api.load('word2vec-google-news-300')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8373e8ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 46.9 KiB for an array with shape (20, 300) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features, y_labels\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Vectors without concatenation\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m w2v_pretrain_train_x, w2v_pretrain_train_y \u001b[38;5;241m=\u001b[39m \u001b[43mfeaturization_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mfeaturization_rnn\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     27\u001b[0m     y_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m review, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_body\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#         try:\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m \u001b[43maverage_vectors_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m         features\u001b[38;5;241m.\u001b[39mappend(x)                \n\u001b[0;32m     33\u001b[0m         y_labels\u001b[38;5;241m.\u001b[39mappend(y)\n",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36maverage_vectors_rnn\u001b[1;34m(review, label)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# In the case where the total dim of the feature vector is <20 add the padding with zeros\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(review_vector)\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m20\u001b[39m:\n\u001b[1;32m---> 21\u001b[0m     review_vector \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreview_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreview_vector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m review_vector\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m20\u001b[39m, label\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 46.9 KiB for an array with shape (20, 300) and data type float64"
     ]
    }
   ],
   "source": [
    "'''\n",
    "limiting the maximum review length to 20 by truncating longer reviews and padding\n",
    "shorter reviews with a null value (0)\n",
    "'''\n",
    "# Average word2Vec vectors\n",
    "def average_vectors_rnn(review, label):\n",
    "    temp_review = review.split(' ')\n",
    "    words = [word for word in temp_review[:20] if word in pretrained_w2v]\n",
    "        \n",
    "    review_vector = []\n",
    "    for word in words:\n",
    "        review_vector.append(pretrained_w2v[word])\n",
    "    review_vector = np.array(review_vector)\n",
    "    \n",
    "    # can be the case where the words in the review are not found in the W2V vocabulary\n",
    "    if len(review_vector)==0:\n",
    "        review_vector = np.zeros((20, 300))\n",
    "    \n",
    "    # In the case where the total dim of the feature vector is <20 add the padding with zeros\n",
    "    if len(review_vector)<20:\n",
    "        review_vector = np.concatenate([review_vector, np.zeros((20-len(review_vector), 300))])\n",
    "    return review_vector/20, label\n",
    "    \n",
    "    \n",
    "def featurization_rnn(dataset):\n",
    "    features = []\n",
    "    y_labels = []\n",
    "    \n",
    "    for review, label in zip(dataset['review_body'], dataset['label']):\n",
    "#         try:\n",
    "        x, y = average_vectors_rnn(review, label)\n",
    "        features.append(x)                \n",
    "        y_labels.append(y)\n",
    "        \n",
    "#         except:\n",
    "#             pass\n",
    "    return features, y_labels\n",
    "\n",
    "# Vectors without concatenation\n",
    "w2v_pretrain_train_x, w2v_pretrain_train_y = featurization_rnn(train)\n",
    "# w2v_pretrain_test_x, w2v_pretrain_test_y = featurization_rnn(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7dbe6512",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'w2v_pretrain_test_x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# del globals()['w2v_pretrain_train_x'], globals()['w2v_pretrain_train_y']\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2v_pretrain_test_x\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2v_pretrain_test_y\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# del globals()['perceptr_w2v'], globals()['perceptr_tfidf'], globals()['svm_w2v'], globals()['svm_tfidf']\u001b[39;00m\n\u001b[0;32m      8\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'w2v_pretrain_test_x'"
     ]
    }
   ],
   "source": [
    "\n",
    "# del globals()['w2v_pretrain_train_x'], globals()['w2v_pretrain_train_y']\n",
    "# del globals()['w2v_pretrain_test_x'], globals()['w2v_pretrain_test_y']\n",
    "# del globals()['perceptr_w2v'], globals()['perceptr_tfidf'], globals()['svm_w2v'], globals()['svm_tfidf']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d4e3042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00522461,  0.00014877, -0.00505371, ..., -0.003125  ,\n",
       "         0.00296631, -0.01054688],\n",
       "       [ 0.00358887,  0.01040039, -0.00142212, ..., -0.00839844,\n",
       "        -0.0010437 , -0.00712891],\n",
       "       [ 0.00576172, -0.00402832, -0.00390625, ..., -0.00339355,\n",
       "         0.00123901, -0.00600586],\n",
       "       ...,\n",
       "       [ 0.00480957, -0.00143433, -0.00541992, ...,  0.0034668 ,\n",
       "         0.00522461, -0.00820313],\n",
       "       [-0.00378418,  0.00168457, -0.00324707, ...,  0.0006012 ,\n",
       "         0.00678711, -0.00456543],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pretrain_test_x[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
