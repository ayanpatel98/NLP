{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6e53bc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim.downloader as api\n",
    "import gensim.models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3eb19",
   "metadata": {},
   "source": [
    "# Task 1: Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5176f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n",
    "df = df[['star_rating', 'review_body']]\n",
    "class_one = df[(df['star_rating']==1) | (df['star_rating']==2)]\n",
    "class_two = df[df['star_rating']==3]\n",
    "class_three = df[(df['star_rating']==4) | (df['star_rating']==5)]\n",
    "\n",
    "class_one.loc[:, \"label\"] =1\n",
    "class_two.loc[:, \"label\"] =2\n",
    "class_three.loc[:, \"label\"] =3\n",
    "\n",
    "class_one = class_one.sample(n=20000, random_state=100)\n",
    "class_two = class_two.sample(n=20000, random_state=100)\n",
    "class_three = class_three.sample(n=20000, random_state=100)\n",
    "dataset = pd.concat([class_one, class_two, class_three])\n",
    "dataset.reset_index(drop=True)\n",
    "train = dataset.sample(frac=0.8, random_state=100)\n",
    "test = dataset.drop(train.index)\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d5b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of the reviews in terms of character length before and after cleaning:  287.3779333333333 , 276.02595\n"
     ]
    }
   ],
   "source": [
    "avg_len_before = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "\n",
    "# Covert all reviews to lower case\n",
    "train['review_body'] = train['review_body'].str.lower()\n",
    "test['review_body'] = test['review_body'].str.lower()\n",
    "\n",
    "'''\n",
    "URL Remover code\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "test['review_body'] = test['review_body'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "def html_tag_remover(review):\n",
    "    soup = BeautifulSoup(review, 'html.parser')\n",
    "    review = soup.get_text()\n",
    "    return review  \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: html_tag_remover(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: html_tag_remover(review))\n",
    "\n",
    "'''\n",
    "remove non-alphabetical characters\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub('[^a-zA-Z]+',' ', review))\n",
    "\n",
    "'''\n",
    "remove extra spaces\n",
    "'''\n",
    "train['review_body'] = train['review_body'].apply(lambda review: re.sub(' +', ' ', review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: re.sub(' +', ' ', review))\n",
    "\n",
    "'''\n",
    "perform contractions on the reviews\n",
    "'''\n",
    "def expand_contractions(review):\n",
    "    review = contractions.fix(review)\n",
    "    return review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: expand_contractions(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: expand_contractions(review))\n",
    "avg_len_after = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7713746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of the reviews in terms of character length before and after preprocessing:  276.02595 , 165.65626666666665\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "remove the stop words AND perform lemmatization\n",
    "\n",
    "'''\n",
    "avg_len_before_prepro = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    stop_words_english = set(stopwords.words('english'))\n",
    "    review_word_tokens = word_tokenize(review)\n",
    "    filtered_review = [word for word in review_word_tokens if not word in stop_words_english]\n",
    "    return filtered_review\n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: remove_stopwords(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: remove_stopwords(review))\n",
    "\n",
    "def review_lemmatize(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_review = [lemmatizer.lemmatize(word) for word in review]\n",
    "    return ' '.join(lemmatized_review)    \n",
    "\n",
    "train['review_body'] = train['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "test['review_body'] = test['review_body'].apply(lambda review: review_lemmatize(review))\n",
    "\n",
    "avg_len_after_prepro = (train['review_body'].str.len().sum() + test['review_body'].str.len().sum())/60000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0836a8",
   "metadata": {},
   "source": [
    "# Task 2: Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38dd8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyCorpus:\n",
    "#     def __init__(self):\n",
    "#         self.sentences = train['review_body']\n",
    "#     def __iter__(self):\n",
    "#         for sentence in self.sentences:\n",
    "#             yield sentence\n",
    "\n",
    "# all_Sentences = MyCorpus()\n",
    "all_Sentences = [sentence.split(' ') for sentence in train['review_body'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4950ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check semantic similarities of the generated vectors:\n",
      "('queen', 0.7118193507194519)\n",
      "Excellent ~ Outstanding: 0.55674857\n",
      "time ~ schedule: 0.26993576\n"
     ]
    }
   ],
   "source": [
    "# Pretrained Word2Vec model:\n",
    "pretrained_w2v = api.load('word2vec-google-news-300')\n",
    "\n",
    "print('Check semantic similarities of the generated vectors:')\n",
    "print(pretrained_w2v.most_similar(positive=['king', 'woman'], negative=['man'], topn = 1)[0])\n",
    "print('Excellent ~ Outstanding:', pretrained_w2v.similarity('excellent', 'outstanding'))\n",
    "print('time ~ schedule:', pretrained_w2v.similarity('time', 'schedule'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e685c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Word2Vec\n",
    "custom_model = gensim.models.Word2Vec(all_Sentences, vector_size = 300, min_count=9, window=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31afd952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check semantic similarities of the generated vectors:\n",
      "('conscious', 0.7422319054603577)\n",
      "Excellent ~ Outstanding: 0.7323696\n",
      "time ~ schedule: 0.18484564\n"
     ]
    }
   ],
   "source": [
    "print('Check semantic similarities of the generated vectors:')\n",
    "print(custom_model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn = 1)[0])\n",
    "print('Excellent ~ Outstanding:', custom_model.wv.similarity('excellent', 'outstanding'))\n",
    "print('time ~ schedule:', custom_model.wv.similarity('time', 'schedule'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6d887",
   "metadata": {},
   "source": [
    "# Task 3: Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9fc2a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word2Vec vectors\n",
    "all_sentence_vector = pretrained_w2v\n",
    "def average_vectors(review, label):\n",
    "    temp_review = review.split(' ')\n",
    "    words = [word for word in temp_review if word in all_sentence_vector]\n",
    "    if len(words) >=1:\n",
    "        review_vector = []\n",
    "        for word in words:\n",
    "            review_vector.append(all_sentence_vector[word])\n",
    "        return review_vector, label\n",
    "\n",
    "def featurization(dataset):\n",
    "    features = []\n",
    "    y_labels = []\n",
    "    \n",
    "    for review, label in zip(dataset['review_body'], dataset['label']):\n",
    "        try:\n",
    "            x, y = average_vectors(review, label)\n",
    "            features.append(np.mean(x, axis=0))\n",
    "            y_labels.append(y)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    return features, y_labels\n",
    "\n",
    "w2v_pretrain_train_x, w2v_pretrain_train_y = featurization(train)\n",
    "w2v_pretrain_test_x, w2v_pretrain_test_y = featurization(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "746a1e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Feature Extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 0.001)\n",
    "tfidf_X_train = tfidf_vectorizer.fit_transform(list(train['review_body']))\n",
    "tfidf_X_train = pd.DataFrame(tfidf_X_train.toarray())\n",
    "\n",
    "tfidf_X_test = tfidf_vectorizer.transform(list(test['review_body']))\n",
    "tfidf_X_test = pd.DataFrame(tfidf_X_test.toarray())\n",
    "\n",
    "tfidf_Y_train = train['label']\n",
    "tfidf_Y_test = test['label']\n",
    "\n",
    "tfidf_Y_train = tfidf_Y_train.astype('int')\n",
    "tfidf_Y_test = tfidf_Y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "185755fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Perceptron Model on Average Word2Vec Features\n",
    "perceptr_w2v = Perceptron(random_state = 100, eta0=0.1)\n",
    "perceptr_w2v.fit(w2v_pretrain_train_x, w2v_pretrain_train_y)\n",
    "Y_pred_w2v_test = perceptr_w2v.predict(w2v_pretrain_test_x)\n",
    "\n",
    "# Training Perceptron Model on TF-IDF Features\n",
    "perceptr_tfidf = Perceptron(random_state = 100, eta0=0.1)\n",
    "perceptr_tfidf.fit(tfidf_X_train, tfidf_Y_train)\n",
    "Y_pred_tfidf_test = perceptr_tfidf.predict(tfidf_X_test)\n",
    "\n",
    "target_names = ['class 1', 'class 2', 'class 3']\n",
    "report_w2v = classification_report(w2v_pretrain_test_y, Y_pred_w2v_test, target_names=target_names, output_dict=True)\n",
    "report_tfidf = classification_report(tfidf_Y_test, Y_pred_tfidf_test, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "acd989cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy values PERCEPTRON for w2v and tfidf features:\n",
      "0.5805374728759807 0.6170833333333333\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy values PERCEPTRON for w2v and tfidf features:')\n",
    "print(report_w2v['accuracy'], report_tfidf['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e20346d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training SVM Model on Average Word2Vec Features\n",
    "svm_w2v = LinearSVC(random_state=100, max_iter=1000)\n",
    "svm_w2v.fit(w2v_pretrain_train_x, w2v_pretrain_train_y)\n",
    "Y_pred_w2v_svm_test = svm_w2v.predict(w2v_pretrain_test_x)\n",
    "\n",
    "# Training SVM Model on TFIDF Features\n",
    "svm_tfidf = LinearSVC(random_state=100, max_iter=1000)\n",
    "svm_tfidf.fit(tfidf_X_train, tfidf_Y_train)\n",
    "Y_pred_tfidf_svm_test = svm_tfidf.predict(tfidf_X_test)\n",
    "\n",
    "report_svm_w2v = classification_report(w2v_pretrain_test_y, Y_pred_w2v_svm_test, target_names=target_names, output_dict=True)\n",
    "report_svm_tfidf = classification_report(tfidf_Y_test, Y_pred_tfidf_svm_test, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74846bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy values SVM for w2v and tfidf features:\n",
      "0.627691537305959 0.6685\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy values SVM for w2v and tfidf features:')\n",
    "print(report_svm_w2v['accuracy'], report_svm_tfidf['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ea5aa4",
   "metadata": {},
   "source": [
    "# Task 4: Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b1cf383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 300):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 10\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)  \n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "#         self.sig = nn.Sigmoid()\n",
    "#         self.soft = nn.Softmax(dim = 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
